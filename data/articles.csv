url,article
https://www.diariodiunanalista.it,"Cosa è l'IA generativa e perché sta ridefinendo i confini del possibile attraverso tecnologie che permettono alle macchine non solo di apprendere ma anche di creare
Concetti e tecniche di Data Science, Machine Learning & Analytics
Ricevi aggiornamenti su nuove pubblicazioni e news via email
Articoli scelti per te
Una guida introduttiva agli algoritmi di clustering: cosa sono, quali sono, perché sono importanti e come valutarli nel contesto dell'analisi dei dati e la data science
Impara come usare la PCA in Python e Sklearn per trasformare un dataset multidimensionale in un numero arbitrario di dimensioni e visualizzare i dati ridotti con Matplotlib
Una guida a servire un modello di machine learning via API con FastAPI, Pydantic e Sklearn
L'apprendimento non supervisionato è una branca fondamentale dell'analisi dei dati che si concentra sulla scoperta di strutture nascoste nei dati senza la presenza di etichette di output.
Il data leakage rappresenta, insieme all'over/underfitting, la causa principale di fallimento di progetti di machine learning che vanno in produzione. Scopri come evitarlo in questo articolo.
Deep Learning e MNIST: Come utilizzare una rete neurale convolutiva per il riconoscimento di immagini
Come approcciare al riconoscimento delle immagini e delle cifre MNIST con una rete neurale convolutiva usando Keras e TensorFlow"
https://www.diariodiunanalista.it/about/,"Un diario virtuale di appunti personali
Ecco cosa è questo sito web. Nasce come una raccolta personale di pensieri e tecniche che ho avuto e incontrato durante gli anni di lavoro.
Metto a disposizione ora questo materiale al lettore, che può usare il contenuto che legge per i suoi progetti personali o di lavoro.
Come io ho trovato ispirazione e guida nei blog di settore, trovo giusto condividere le mie conoscenze per continuare questo ciclo virtuoso.
Una delle sfide più grandi che ho affrontato durante la mia carriera è stata quella di trovare l'orientamento giusto nel mare magnum di informazioni che circondano il machine learning e il data science.
Se ti senti perso, smarrito, ma sei determinato a perseguire la tua carriera nel data science, allora il contenuto di questo sito fa per te.
Allo stesso tempo, penso tu ne possa beneficiare allo stesso modo se cerchi soluzioni veloci a problemi ben definiti.
Soluzioni offerte
Di seguito le attività che offerte dallo staff.
Consulenza
Puoi metterti in contatto con me per una consulenza sul tuo progetto di data science, analytics e machine learning.
Formazione
Le mie consulenze spesso si trasformano in veri e propri corsi, sia per team interni o esterni all'azienda. Mi dedico anche a insegnare corsi relativi al data science in master di formazione professionale.
Affiliazioni
Questo blog utilizza il modello di affiliazione per sostenersi. Troverete risorse che punteranno a negozi online come Amazon. Se comprerete un articolo da me raccomandato io riceverò una commissione dal rivenditore, senza togliere nulla alla vostra esperienza di acquisto.
Se il lettore è interessato a propormi una partnership, è invogliato a contattarmi per discutere dei dettagli.
Link utili
Scopri lo staff
Scopri lo staff di autori e contributori al blog.
Andrea D'Agostino
Fondatore e autoreFollow @theDrewDag
Ciao! Sono Andrea, data scientist che lavora nel settore del marketing e business intelligence con oltre 6 anni di esperienza a lavorare con grossi clienti su progetti di data science.
Copro la posizione di Head of Data Science in DigiTrails, dove mi occupo di definire, organizzare e implementare sistemi di machine learning per la business intelligence.
Sei interessato ad una consulenza in ambito Data Science o Machine Learning?
Sono laureato in neuroscienze cognitive e applico la mia conoscenza nel settore del marketing perché c'è una sovrapposizione utile tra comportamento umano e disponibilità di dati.
Leggi tutti gli articoli di Andrea D'Agostino qui
Marco Speciale
Autore
Studente di Data Science presso l’universita degli studi di Palermo. Appassionato di numeri e analisi dei dati, mi dedico a esplorare il mondo affascinante della Data Science, aprendo nuove prospettive e affrontando sfide con entusiasmo.
Seguitemi in questo viaggio di scoperte e successi nel mondo dei dati!
Leggi tutti gli articoli di Marco Speciale"
https://www.diariodiunanalista.it/account/,"Ti sei iscritto con successo a Diario Di Un Analista | Data Science, Machine Learning & Analytics
Ben tornato! Hai effettuato l’accesso con successo.
Ottimo! Ti sei registrato con successo.
Successo! La tua email è stata aggiornata.
Il tuo link è scaduto
Successo! Controlla la tua casella di posta per il link magico per accedere."
https://www.diariodiunanalista.it/author/andrea-dagostino/,"Cosa è l'IA generativa e perché sta ridefinendo i confini del possibile attraverso tecnologie che permettono alle macchine non solo di apprendere ma anche di creare
Andrea D'Agostino
Data scientist con 6 anni di esperienza nell'applicare tecniche di data science per aiutare i clienti a risolvere problemi nei loro asset e a sfruttare le debolezze dei competitor a loro vantaggio.
Il prompt engineering consiste nello sviluppare e ottimizzare i prompt (cioè le domande che vengono poste al modello da parte dell'umano) al fine di ottenere risposte più precise da parte dei modelli linguistici
Una guida introduttiva ai concetti del machine learning e data science per principianti. Orientati con dei consigli diretti proprio a te che sei novizio e che vuoi diventare un esperto di machine learning
Una guida introduttiva agli algoritmi di clustering: cosa sono, quali sono, perché sono importanti e come valutarli nel contesto dell'analisi dei dati e la data science
Impara come usare la PCA in Python e Sklearn per trasformare un dataset multidimensionale in un numero arbitrario di dimensioni e visualizzare i dati ridotti con Matplotlib
Una guida a servire un modello di machine learning via API con FastAPI, Pydantic e Sklearn
Il data leakage rappresenta, insieme all'over/underfitting, la causa principale di fallimento di progetti di machine learning che vanno in produzione. Scopri come evitarlo in questo articolo.
Una lista di software utili per la gestione di progetti di data science e la raccolta di idee e contenuti
Fai scelte intelligenti per la tua strategia aziendale: quando utilizzare il machine learning e quando optare per soluzioni più semplici. Esplora i vantaggi e le sfide dell'adozione dei modelli di apprendimento automatico per prendere decisioni informate"
https://www.diariodiunanalista.it/author/marco-speciale/,"L'apprendimento non supervisionato è una branca fondamentale dell'analisi dei dati che si concentra sulla scoperta di strutture nascoste nei dati senza la presenza di etichette di output.
Marco Speciale
2 post
Deep Learning e MNIST: Come utilizzare una rete neurale convolutiva per il riconoscimento di immagini
Come approcciare al riconoscimento delle immagini e delle cifre MNIST con una rete neurale convolutiva usando Keras e TensorFlow"
https://www.diariodiunanalista.it/contact/,"Diario Di Un Analista | Data Science, Machine Learning & Analytics — Contatti Contatti Invia messaggio"
https://www.diariodiunanalista.it/glossario/,"Le informazioni in questa pagina sono in continuo aggiornamento.
Accuratezza
L'accuratezza è una metrica di valutazione delle performance di un modello di machine learning. Misura il numero di osservazioni correttamente predette sul numero di osservazioni totali.
La sua formula
\[ accuratezza \ = \frac{numero\_osservazioni\_corrette}{N} \]
Adam
Adam è il nome di uno degli algoritmi di ottimizzazione più usati per l'addestramento dei modelli di deep learning. Viene utilizzato per calcolare i tassi di apprendimento adattivo per ciascun parametro. Oltre ad essere efficiente dal punto di vista computazionale, Adam impatta poco sulla memoria del sistema e di conseguenza funziona bene in molti scenari diversi.
Albero decisionale
Un albero decisionale è un tipo di apprendimento automatico supervisionato utilizzato per fare classificazione o regressione in base a come è stata data risposta a una precedente serie di domande. Il modello è una forma di apprendimento supervisionato, il che significa che il modello viene addestrato e testato su un set di dati che contiene la categorizzazione desiderata.
Algoritmo
Un algoritmo è un insieme di istruzioni che diamo a un computer in modo che possa eseguire un compito. Nel contesto della data science, la parola algoritmo può essere utilizzata per descrivere un modello di machine learning.
Backend
Il backend è tutto il codice e la tecnologia che lavora dietro le quinte per popolare il frontend con informazioni utili. Ciò include database, server, procedure di autenticazione e molto altro.
Backpropagation
In italiano propagazione a ritroso, è un processo di aggiornamento dei pesi e dei bias delle reti neurali. Gli algoritmi di backpropagation funzionano determinando l'error in output e quindi propagandolo nuovamente nella rete, andando a ritroso. I pesi vengono aggiornati per ridurre al minimo l'errore derivante da ciascun neurone. La minimizzazione dell'errore avviene determinando la derivata di ciascun nodo rispetto all'output finale. In base alla pendenza della funzione in un determinato punto e alla funzione di perdita, l'algoritmo di backpropagation è in grado di modificare pesi e bias della rete neurale per contribuire ad un miglioramento delle performance.
Bagging
Il bagging è una tecnica in cui vengono creati più modelli predittivi e loro le previsioni finali vengono determinate combinando le previsioni di tutti i modelli. Il modello Random Forest è un modello che si basa sulla tecnica del bagging.
Bias
In italiano traducibile come imparzialità, pregiudizio, il bias è, nel contesto del machine learning, uno dei parametri regolabili in una rete neurale insieme ai pesi. Attraverso algoritmi di ottimizzazione e di aggiornamento come la backpropagation, una rete neurale raggiunge performance migliori andando a modificare i parametri di bias e peso per approssimare meglio la funzione che minimizza l'errore.
Big Data
Il termine big data si riferisce ad un insieme di tecniche di raccolta e processamento del dato responsabili ad elaborare enormi moli di dati assicurando tempi di esecuzione molto brevi. Si va nei big data quando si parla di mettere insieme dataset che superano il terabyte di grandezza, e di fare ciò ripetutamente per periodi di tempi indefiniti. I problemi che dobbiamo affrontare con i big data sono classificati dalle 4 V: volume, varietà, veridicità e velocità.
Boosting
Il boosting è un processo sequenziale, in cui ogni modello successivo tenta di correggere gli errori del modello precedente. I modelli successivi dipendono dal modello precedente. Alcuni degli algoritmi di potenziamento sono Ada Boost, GBM, LightGBM, CatBoost e altri.
Bootstrapping
Il bootstraping è il processo di divisione del dataset in più sottoinsiemi, con sostituzione. Ogni sottoinsieme ha le stesse dimensioni del set di dati. Questi campioni sono chiamati campioni bootstrap.
Business Intelligence
La business intelligence è un insieme di strategie, applicazioni, dati, tecnologie utilizzate da un'organizzazione per la raccolta dei dati, l'analisi e la generazione di valore per ricavare opportunità di business strategiche.
Classificazione
La classificazione è un problema di apprendimento automatico supervisionato. Quando siamo di fronte ad un problema di classificazione il nostro obiettivo è quello di separare un punto in base alla sua somiglianza (o anche altri fattori) con altri punti.
Cloud Computing
Il Cloud Computing è un termine generico che indica tutto ciò che comporta la fornitura di servizi ospitati su Internet. Questi servizi sono suddivisi in tre categorie principali o tipi di cloud computing: infrastruttura come servizio (IaaS), piattaforma come servizio (PaaS) e software come servizio (SaaS).
Clustering
Il clustering (in italiano raggruppamento) è un metodo di apprendimento non supervisionato utilizzato per scoprire raggruppamenti intrinseci nei dati. Esempi di algoritmi di clustering sono KMeans e clustering gerarchico.
Computer Vision
La Computer Vision è un campo dell'informatica che si occupa di consentire ai computer di visualizzare, elaborare e identificare immagini/video.
Correlazione
La correlazione è una misura statistica che indica quanto due o più variabili cambiano l'una rispetto all'altra. Una correlazione positiva indica la misura in cui tali variabili aumentano o diminuiscono parallelamente; una correlazione negativa indica la misura in cui una variabile aumenta al diminuire dell'altra.
Cross-validazione
La cross-validazione è una tecnica utilizzata per la valutazione di come i risultati di un modello predittivo si generalizzano a un set di dati indipendente da quello di addestramento. La cross-validazione è ampiamente utilizzata in contesti in cui l'obiettivo è la previsione ed è necessario stimare l'errore delle prestazioni di un modello predittivo. Nella cross-validazione K-fold, i dati vengono prima partizionati in k segmenti uguali. Successivamente vengono eseguite k iterazioni di addestramento e convalida in modo tale che all'interno di ogni iterazione una diversa porzione dei dati venga tenuta fuori per la convalida mentre le rimanenti k - 1 porzioni vengono utilizzate per l'apprendimento.
Database
Un database non è altro che uno spazio di archiviazione per i dati, e può essere fisico o virtuale. Utilizziamo principalmente database con un Database Management System (DBMS), come PostgreSQL o MySQL. Un database è tipicamente relazionale o non-relazionale.
Data Warehouse
Una data warehouse (magazzino dati in italiano) è un sistema utilizzato per eseguire analisi dati utilizzando dati provenienti da molte fonti. Sono progettati per consentire agli utenti di effettuare analisi senza permettere loro di connettersi direttamente a dei database, dove i dati possono essere in un formato poco utilizzabile o che richiedono processamento.
Data Science
La Data Science (scienza dei dati) è il campo di studio che combina competenze di dominio, capacità di programmazione e conoscenza della matematica e della statistica per estrarre informazioni significative dai dati. I professionisti della scienza dei dati applicano algoritmi di apprendimento automatico a numeri, testo, immagini, video, audio e altro ancora per produrre sistemi di intelligenza artificiale (AI) per eseguire attività che normalmente richiedono intelligenza umana. A loro volta, questi sistemi generano informazioni che gli analisti e gli utenti aziendali possono tradurre in valore aziendale tangibile.
Data Scientist
Un Data Scientist è un professionista dell'analisi dati che è responsabile della raccolta, dell'analisi e dell'interpretazione dei dati per aiutare a guidare il processo decisionale in un'organizzazione. Il ruolo di data scientist combina elementi di diversi lavori tradizionali e tecnici, tra questi rientrano competenze di matematica, statistica e programmazione. Implica l'uso di tecniche di analisi avanzate, come il machine learning e la modellazione predittiva, insieme all'applicazione di principi scientifici.
Data Analytics
La Data Analytics è la scienza che analizza i dati grezzi per trarre conclusioni su tali informazioni. Molte delle tecniche e dei processi di analisi dei dati vengono automatizzati in processi meccanici con algoritmi che lavorano su dati grezzi per il consumo umano.
Data Analyst
Un Data Analyst (analista di dati) è un professionista che recupera, organizza e analizza le informazioni da varie fonti per aiutare un'organizzazione a raggiungere gli obiettivi aziendali. I Data Analyst utilizzano tecniche statistiche e linguaggi di programmazione per trasformare numeri in informazioni che un'organizzazione può utilizzare per migliorare il flusso di lavoro e i processi aziendali. Un obiettivo importante dell'analisi è distinguere tra quali dati sono importanti e quali dati dovrebbero avere meno peso. In molte organizzazioni, i data analyst sono anche responsabili della qualità dei dati e della preparazione dei report per gli stakeholder interni ed esterni.
Data Engineering
L'attività di Data Engineering (ingegneria dei dati) rende dati grezzi utilizzabili ai team di analisi dati all'interno di un'organizzazione. L'ingegneria dei dati comprende numerose specialità della data science. Oltre a rendere i dati accessibili, i data engineer creano analisi dei dati grezzi per fornire modelli predittivi e mostrare le tendenze a breve e lungo termine. Senza l'ingegneria dei dati, sarebbe impossibile dare un senso alle enormi quantità di dati a disposizione delle aziende.
Data Engineer
I Data Engineer (ingegneri dei dati) sono responsabili del ciclo di vita di un dataset all'interno dell'organizzazione. Essi sono principalmente interessati ad aggregare dati grezzi e trasformarli in formati di dati utili, ordinati e strutturati e renderli disponibili al team di analisi dati per l'elaborazione. Una delle loro responsabilità cardine è creare l'impalcatura per salvare il dato nel cloud trovando le soluzioni di memorizzazione più efficace per il progetto e l'organizzazione.
Data Mining
Il data mining è un insieme di tecniche utili per identificare e raccogliere informazioni da una fonte di dati che possono aiutare a risolvere i problemi di business. Le tecniche e gli strumenti di data mining consentono alle aziende di prevedere le tendenze future e prendere decisioni aziendali più informate.
Il data mining è una parte fondamentale dell'analisi dei dati in generale e una delle discipline fondamentali nella data science, che utilizza tecniche di analisi avanzate per trovare informazioni utili in dataset. A un livello più granulare, il data mining è una fase del processo di scoperta della conoscenza presente nei database.
Data Visualization
La Data Visualization (visualizzazione dei dati) è la rappresentazione grafica di informazioni e dati. Utilizzando elementi visivi come diagrammi, grafici e mappe, gli strumenti di visualizzazione dei dati forniscono un modo accessibile per vedere e comprendere tendenze, valori anomali e modelli nei dati. Inoltre, fornisce un modo eccellente per i dipendenti o gli imprenditori di presentare i dati a un pubblico non tecnico senza confusione.
Deep Learning
Il deep learning, in italiano apprendimento profondo, è un sottoinsieme dell'apprendimento automatico. Il termine descrive principalmente le reti neurali artificiali. Queste reti neurali tentano di simulare il comportamento del cervello umano, anche se lungi dall'eguagliare le sue capacità, permettendogli di ""imparare"" da grandi quantità di dati. Mentre una rete neurale con un singolo strato può ancora fare previsioni approssimative, ulteriori livelli nascosti possono aiutare a ottimizzare e perfezionare la precisione.
Early Stopping
L'Early Stopping (arresto anticipato) è una tecnica nel machine learning per la prevenzione dell'overfitting in un modello predittivo. Se lasciamo che un modello di machine learning come una rete neurale si addestri abbastanza a lungo su un dataset, alla fine questo può apprendere come modellare esattamente questi dati (overfitting). Al contrario, se il modello viene addestrato solo per poco poco tempo o su dati non rappresentativi, il modello potrebbe generalizzare bene ma non avrà un'accuratezza desiderabile (underfitting). L'early stopping termina l'addestramento di un modello nel momento migliore, andando ad evitare sia underfitting che overfitting.
EDA (Exploratory Data Analysis)
L'EDA o analisi esplorativa dei dati è una fase della pipeline di data science in cui l'obiettivo è comprendere la struttura e pattern dei dati attraverso la visualizzazione o l'analisi statistica dei dati.
ETL (Extract, Load, Transform)
ETL (estrazione, caricamento e trasformazione del dato) rappresenta tre processi che, in combinazione, spostano i dati da un database o altre fonti a un repository unificato, in genere un data warehouse. Consente all'analisi dei dati di fornire informazioni aziendali fruibili, preparando efficacemente i dati per i processi di analisi e business intelligence.
Frontend
Il frontend è tutto ciò con cui un utente può vedere e interagire direttamente. Ciò include dashboard di dati, pagine Web e moduli. Un frontend è basato tipicamente su HTML, CSS e Javascript.
Iperparametro
Un iperparametro del modello è una configurazione esterna ad esso il cui valore non può essere stimato dai dati. Modificare un iperparametro modifica di conseguenza il comportamento del modello sui nostri dati e può migliorare o peggiorare le nostre performance.
Modello
Nel machine learning, un modello è una rappresentazione matematica di un fenomeno nella realtà generata da dati. Un modello di apprendimento automatico è l'espressione di un algoritmo che impara da grosse moli di dati per trovare pattern o fare previsioni. Ad esempio, un modello ML per la visione artificiale potrebbe essere in grado di identificare auto e pedoni in un video in tempo reale. mentre uno per l'elaborazione del linguaggio naturale potrebbe tradurre parole e frasi.
Overfitting
La metrica di valutazione del modello rimane uguale o peggiora mentre aumenta nel training set. Il nostro modello impara sempre meglio i dati di training, ma la sua performance non migliora sul test set. Questo porta il modello a non generalizzare bene per dati non appartenenti al training set.
Rasoio di Occam
La spiegazione più semplice è potenzialmente quella corretta. Nel machine learning, quando il nostro modello non obedisce al rasoio di Occam, questo sta probabilmente overfittando."
https://www.diariodiunanalista.it/inizia-qui/,"Stai iniziando il tuo percorso con Python, Machine Learning e Data Analytics?
Questa pagina ti fornirà il percorso ideale in base al contenuto presente in questo blog
Ogni sezione riporterà gli articoli più idonei per il livello e gli obiettivi del lettore.
Ogni sezione sarà inoltre aggiornata ad ogni nuova pubblicazione per integrare tale contenuto nella roadmap.
Livello: Principiante
Se state appena iniziando il vostro percorso e volete orientarvi nel mondo dell'apprendimento automatico e analisi dati
- Impostare un ambiente di sviluppo in Python
- Le migliori risorse per imparare Python online
- Analisi esplorativa dei dati con Python e Pandas
- Cos'è il Machine Learning: come spiego il concetto ad un neofita
- Algoritmi di Machine Learning: Guida introduttiva per comprendere i principi e le applicazioni
- Come strutturare un progetto di machine learning
- Il più grande ostacolo nel machine learning: l’overfitting
- Perché il tuo progetto di machine learning potrebbe fallire
- Data Leakage: cosa è e perché fa fallire i nostri sistemi predittivi
- Come preparare i dati per il machine learning
- Cosa è la cross-validazione nel machine learning
- Valutazione delle performance di un modello di regressione
- Valutazione delle performance di un modello di classificazione binaria
- Come prendere appunti nella Data Science
- Introduzione alle reti neurali - pesi, bias e attivazione
- Introduzione al clustering non supervisionato con K-Means
Livello: Intermedio
Vuoi andare più in profondità con concetti di deep learning, progetti e applicazioni specifiche. Eccoti accontentato 👇
- Servire modelli di machine learning con FastAPI
- Introduzione alla PCA in Python con Sklearn, Pandas e Matplotlib
- Deep Learning e MNIST: Come utilizzare una rete neurale convolutiva per il riconoscimento di immagini
- Costruire il proprio dataset: vantaggi, approccio e strumenti
- Il compromesso Bias-Varianza nel Machine Learning
- Come ripartire i dati per la cross-validazione K-fold
- Migliorare i propri modelli di dati con Pydantic
- Regolarizzazione L1 vs L2 nel Machine Learning: differenze, vantaggi e come applicarle in Python
- Rappresentazioni vettoriali per il Machine Learning
- Come calcolare la similarità tra testi di un sito web con TF-IDF in Python
- Perché avere un grosso numero di feature può peggiorare il tuo modello
- Introduzione a PyTorch: dal training loop alla predizione
- Introduzione a Tensorflow 2.0 - API e modello sequenziale di deep learning
- Classificazione binaria di immagini con Tensorflow
- Tecniche di data visualization: dal grafico allo storytelling
- Come scraperare un blog e raccogliere i suoi articoli in Python
- Tagging di contenuti con la logica fuzzy in Python
Livello: Avanzato
Qui sono evidenziati prevalentemente progetti a sfondo applicativo, con applicazioni di machine learning tradizionale e deep learning
- Clustering di serie temporali per la previsione del mercato azionario in Python - Parte 1
- Addestrare un modello Word2Vec da zero con Gensim
- Previsione delle serie temporali con LSTM in Tensorflow
- Early Stopping in TensorFlow - impedire l'overfitting di una rete neurale
- Convertire testi in tensori per il Deep Learning
- Come tokenizzare e fare padding di sequenze in Tensorflow
- Competizione Kaggle: Valutare il livello di competenza linguistica col machine learning
Diventa membro della community
Newsletter bisettimanale su news e tech di settore
Nessuno spam. Disiscriviti quando vuoi.
Segui il blog sui social
Se ti piace il contenuto proposto in questo blog, assicurati di seguire i canali social"
https://www.diariodiunanalista.it/materiale-educativo/,"Materiale Educativo
Lsta delle risorse educative vettate dallo staff per formarti nel campo della Data Science e Machine Learning
In questa pagina troverai una lista delle risorse educative vettate dallo staff per formarti nel campo della Data Science e Machine Learning.
Questo materiale include corsi, playlist, podcast e blog di settore che mi hanno aiutato a crescere la mia conoscenza nel campo.
Spero possano fare lo stesso per voi.
Se questo contenuto ti aiuta nel migliorarti come professionista, considera registrarti al blog per mostrare il tuo sostegno :)
Registrati a Diario Di Un Analista
Ottieni accesso a tutti i post gratis, senza alcuna pubblicità. Ricevi aggiornamenti sulle nuove pubblicazioni e news di settore.
No spam. Cancella l'iscrizione quando vuoi.
Canali e playlist YouTube
Corsi
Prompt engineering per ChatGPT su Coursera
Fondamenti di PyTorch e Deep Learning di Microsoft
Deep Learning Fundamentals di S. Raschka
Corso di Deep Learning con Fast.ai e PyTorch
NeuroMatch Academy: Deep Learning
Corso di Deep Reinforcement Learning di Huggingface
Repository Git
Documenti, articoli e blog curati su data science e machine learning in produzione"
https://www.diariodiunanalista.it/newsletter/,"Questo è l'archivio della nostra newsletter settimanale consegnata settimanalmente. Ci piacerebbe tanto che ti unissi a noi!
🎺
Gli argomenti vengono più importanti di settore vengono sintentizzati in brevi paragrafi e elenchi in modo che tu possa assimilarli e comprenderli nella maniera più veloce possibile, senza sovraccaricarti.
È quindi un formato adatto a professionisti di tutti i livelli, che hanno poco tempo ma che vogliono rimanere al passo coi tempi e le novità più gettonate.
Puoi sfogliare i rilasci precedenti per farti una idea del contenuto che riceverai!
È quindi un formato adatto a professionisti di tutti i livelli, che hanno poco tempo ma che vogliono rimanere al passo coi tempi e le novità più gettonate.
Puoi sfogliare i rilasci precedenti per farti una idea del contenuto che riceverai!
Forniamo un riepilogo settimanale delle migliori notizie sulIa IA e sugli avanzamenti tecnologici divise in due newsletter dedicate
Consegnate ogni mercoledì e sabato rispettivamente nella tua casella di posta.
No spam. Disiscriviti quando vuoi."
https://www.diariodiunanalista.it/posts/5-strumenti-per-avanzare-nella-carriera-di-data-scientist/,"La data science non è solo caratterizzata da strumenti relativi al codice.
VSCode, PyCharm o EMACS / VIM non sono gli unici strumenti che un analista si deve preoccupare di usare per soddisfare le esigenze professionali.
In questo articolo parlerò di strumenti gestionali, per la comunicazione e l'organizzazione di conoscenza.
A mano a mano che un data scientist avanza di carriera, i requisiti della nuova posizione lavorativa cambiano e si evolvono...includendo proprio competenze e strumenti manageriali.
Parlerò di strumenti per
- la gestione del progetto (project management)
- la scrittura di documentazione, appunti e grafici
- la raccolta di conoscenza (knowledge base)
Tutti questi strumenti sono gratis completamente oppure con il piano di abbonamento base.
Inutile dirlo ma l'utilizzo di questi strumenti mi ha aiutato a fare un salto di qualità importante nella gestione del lavoro e delle priorità.
Infatti, essere bravi a scrivere codice non è l'unica skill che serve ad un data scientist.
Occorrono competenze primariamente gestionali e comunicative e si vuole fare carriera come magari AI Manager, anche la leadership.
Iniziamo!
Notion
Notion è uno strumento a dir poco incredibile. Permette, in un singolo pacchetto, di
- gestire progetti (feature aggiunta da poco ma ben fatta)
- scrivere e organizzare contenuto con tag, stato, autori e scadenze (e molto altro)
- creare wiki aziendali
- creare documentazione estesa per processi e flussi di qualsiasi genere
A mio avviso, quello che rende Notion lo strumento ""perfetto"" è la sua flessibilità. Lo si può usare praticamente per ogni lavoro e casistica.
Comprende teamspaces, ambienti collaborativi di team dove l'amministratore può fornire permessi e poteri a diverse persone del suo team per diverse attività di editing.
Infine, Notion permette di condividere facilmente il suo contenuto: infatti è possibile creare facilmente una pagina web che è visitabile da chiunque con una URL.
Io oggi uso Notion tutti i giorni, sia per lavoro che per progetti personali. È il coltellino svizzero di cui avevo bisogno e non posso fare a meno delle sue funzioni.
Puoi scaricare Notion qui (link affiliato), oppure usare la versione online 👇
FigJam di Figma
A mano a mano che assumiamo una posizione di leadership, ci muoveremo sempre di più nello spazio decisionale, comunicativo e manageriale.
Gestire un team porta con sé una grossa sfida: passare concetti e rappresentazioni in maniera efficiente.
Inoltre, per noi che lavoriamo nell'ambito tecnico-informatico, la creazione di diagrammi di flusso diventa molto importante.
FigJam di Figma permette di creare diagrammi di flussi belli, chiari e in maniera molto facile.
L'ambiente collaborativo di team è a pagamento, ma è gratis per l'utilizzo personale.
In ogni caso è possibile esportare i propri flow chart e condividerli anche con la soluzione personale.
Anche Miro.com è una buona soluzione, ma a mio avviso Figma è più facile e intuitivo da usare.
Scarica Figma qui
Obsidian
Obsidian è uno strumento per la persona ossessionata dalla scrittura (un po' come lo sono io...).
Si tratta di uno strumento open source per la creazione e gestione della conoscenza (knowledge base).
Si pone come applicativo per il note-taking ma in realtà è molto di più.
Immagina la potenza di scrittura di un blocco note, ma con la possibilità di espanderlo con qualsivoglia funzionalità, a piacere.
Obsidian permette di scaricare e installare una miriade di plugin creati dalla community per personalizzare completamente (e a volte in maniere veramente folli) il proprio foglio di appunti.
È possibile, ad esempio
- eseguire script di trasformazione del testo automatizzata, ad esempio selezionare e evidenziare parti di testo in maniera automatica
- mappare il contenuto in base a diverse specifiche, come link interni oppure argomenti
- importare widget e funzionalità esotiche (basta cercare nel contenuto della community per trovare quel che si vuole)
- essendo open source, si può scaricare tutto in locale e sviluppare plugin
Tra l'altro è basato su Markdown, il che lo rende estremamente flessibile dal punto di vista della scrittura e non legato ad alcun software o metodologia proprietaria.
Io oggi uso Obsidian per scrivere i miei articoli di blog, ma è possibile usarlo al posto di Notion per esempio per la creazione di contenuti.
Una delle funzioni più interessanti per uno scrittore è quella di riuscire a collegare argomenti tra di loro. Obsidian permette di farlo direttamente nel suo pacchetto base.
Trovi Obsidian qui 👇
Microsoft Teams
Qui starai storcendo il naso se non sei un fan di Microsoft.
Nemmeno io lo sono, ma Teams ha proprio cambiato il modo di comunicare con il mio team a lavoro grazie all'esperienza di scrittura pulita, le funzionalità di OneDrive integrate e le applicazioni della suite MS direttamente accessibili dallo strumento.
Brilla a mio avviso la gestione delle riunioni, delle email e degli alert direttamente nello strumento.
Io sono un apprezzatore di soluzioni open source. MS Teams non lo è. Per me questa è l'unica pecca.
Ma d'altro canto, nemmeno Zoom o Skype sono open source. MS Teams offre una esperienza di comunicazione superiore ad entrambi. Senza alcun dubbio.
Grazie Teams, la mia efficienza nella gestione del mio tempo è aumentata, che mi ha aiutato a fare più cose in meno tempo.
Puoi scaricare Microsoft Teams qui
Discord
Discord ha avuto un impatto incredibile nella mia formazione da data scientist.
Come, ti starai chiedendo?
Beh perché non è solo uno strumento di chat.
È uno strumento che mette insieme persone e community.
Infatti, grazie a Discord ho stretto amicizie e imparato cose molto importanti per la mia carriera facendo parte di community private e pubbliche relative alla data science e ad altri ambienti professionali.
Imparare in compagnia è 1000x meglio che imparare da soli, e permette di confrontarci con gente migliore di noi (che è l'unico modo di imparare veramente secondo me).
Alcune delle community di data science che seguo sono
- ServerlessML gestito da Pau Labarta Bajo
- RealWorldML gestito sempre da Pau
- MLSpace, gestito da Abhishek Thakur (GM Kaggle 4x)
Scarica Discord qui
Per Riassumere
1. Notion
2. FigJam
3. Obsidian.md
4. Microsoft Teams
5. Discord
Sicuramente ci sono alcuni strumenti che non ho menzionato ma che hanno impattato positivamente la mia carriera e la mia abilità di migliorarmi come data scientist.
Ma non preoccuparti, perché alla fine ciò che funziona per me non deve per forza funzionare per te.
Quello che funziona per te è quello che dovresti continuare ad usare.
Detto questo, quali sono gli strumenti che utilizzi nella data science (o anche altra professione!) che ti hanno aiutato a migliorarti come professionista?
Condividi le tue esperienze!
Ti piace quello che stai leggendo?
Iscriviti al blog per rimanere sempre aggiornato sui contenuti di settore. No pubblicità. No spam.
Solo contenuto utile per la tua carriera nel ML e DS.
Puoi rimuovere l'iscrizione quando vuoi seguendo il link nella email.
A presto,
Andrea
Commenti dalla community"
https://www.diariodiunanalista.it/posts/6-cose-da-fare-prima-di-addestrare-il-tuo-modello/,"In questo articolo scriverò di una metodica utile per prepararsi alla fase di addestramento di un modello. Si tratta di un processo formato da 6 step, ognuno utile per assicurare che un corretto flusso di dati sia immesso nel nostro modello. Seguire questi passaggi mi ha reso più efficiente nel lavoro e più sicuro di me quando mostravo i miei risultati.
Voglio ringraziare Andriy Burkov e il suo lavoro con il libro Machine Learning Engineering. Ne ho tratto ispirazione per scrivere questo articolo ed è nel complesso un'ottima lettura: leggetelo se non l'avete già fatto.
Cominciamo.
1. Il file di schema
Il file schema viene utilizzato per tenere traccia di quali sono le tue feature, di come si comportano e delle loro proprietà generali. È utile perché garantisce che tutto il team sia aggiornato su ciò che viene fornito al modello, in tutte le fasi dello sviluppo. Fornisce inoltre al team una linea guida formale da seguire durante il debug del modello.
Crea un file che soddisfi almeno questi criteri:
- contenga il nome delle feature
- il loro tipo (categoriale, numerico, …)
- i valori minimi e massimi consentiti
- media e deviazione standard
- se consente la presenza di zeri
- se consente la presenza di valori non definiti (NaN, None, ...)
Sentitevi liberi di creare qualsiasi tipo di file per contenere queste informazioni. Ecco un esempio di un file schema.json che contiene queste informazioni per un set di dati sintetico.
2. Il file README.md
Una delle mie attività preferite è scrivere i miei pensieri prima e durante il mio progetto di machine learning. Sfrutto il file README.md, che è il primo file che creo nel mio repository.
È essenziale che tutti i nostri ragionamenti vadano in questo file in modo che team (e anche te stesso lungo la strada) segua sempre la stessa linea. È facile gettarsi a capofitto in un'idea solo per scoprire delle ore dopo che non aveva senso secondo il brief del progetto. Esaminare il README ci aiuterà a cristallizzare le nostre intenzioni e ad essere più efficienti.
Il modo in cui strutturo il mio file README non è standard, ma generalmente segue questo schema:
- scrivo dell'obiettivo e dell'idea generale dietro di esso
- elenco le possibili metodologie che possono essere utilizzate per affrontare il problema
- elenco pro e contro di ciascuno, commentando il più possibile ogni approccio
- elenco gli aspetti sfidanti del progetto e di cosa avrei bisogno in termini di risorse e conoscenze per risolvere il problema in questo momento
- l'impatto che il progetto ha sul business
Il punto 5 ci aiuta nello storytelling post-progetto, vale a dire la fase in cui presentiamo e spieghiamo i nostri risultati ad una platea. Documentare i passaggio e parlare del mio processo ci permetterà di avere una solida storia da raccontare agli stakeholder (le persone importanti che ruotano intorno al progetto, come investitori oppure il nostro capo). La comunicazione è importante quanto le nostre capacità analitiche.
3. Fissare un obiettivo di performance raggiungibile
Dovremmo parlare spesso con gli stakeholder per capire le loro aspettative sul tuo lavoro. Parla con loro del livello di prestazioni che si aspettano di vedere e della soglia di soddisfazione. Questo è un valore minimo di prestazioni che dovresti mirare a raggiungere.
Per avere un'idea di come potrebbe funzionare il modello, prendiamo questo in considerazione:
- se un essere umano può fare lo stesso lavoro senza molti problemi, allora è lecito ritenere che il modello performerà a livelli simili
- se inserisci nel modello dati di alta qualità, ovvero dati che contengono informazioni rilevanti sul target, è lecito ritenere che il modello perfomerà bene
- se un software può ottenere buoni risultati senza usare un approccio basato sul machine learning, allora è lecito ritenere che il modello performerà a livelli simili
- se un altro algoritmo di machine learning può ottenere buoni risultati su un set di dati simile,
- allora è lecito ritenere che il modello performerà a livelli simili
4. Scegliere una (e una sola) metrica di performance
Questo è strettamente correlato al punto precedente. Un modello ha una performance metric che possiamo assegnargli prima dell'addestramento. Ad esempio, un compito di regressione potrebbe richiedere di impostare le metriche RMSE (Root Mean Squared Error) o MAE (Mean Absolute Error).
Bisogna scegliere la metrica che ha più senso per il problema che abbiamo davanti. Scegli una e una sola metrica di performance e non cambiarla. Confronta e monitora diversi modelli per capire come cambia la metrica tra i modelli. Parlo di selezione del modello e valutazione delle metriche di performance qui.
5. Definire una baseline
Dovremmo sempre confrontare i tuoi modelli con una baseline. Questa è una condizione ""di base"" dalla quale partiremo. Se confrontiamo il nostro risultato con una baseline sapremo sempre se stiamo performando meglio o peggio delle aspettative.
Una baseline varierà a seconda del nostro compito. Ecco alcuni esempi:
- una baseline può essere basata sulle performance di un essere umano. Il nostro modello viene quindi confrontato con le prestazioni umane nello stesso compito
- una baseline può essere una previsione casuale: l'algoritmo sceglie un valore casuale dal set di addestramento y
- una baseline può seguire una regola specifica: per un compito di classificazione può restituire la classe più frequente mentre per una regressione può restituire il valore medio di y
- una baseline può essere un modello semplice e rudimentale
Se il tuo modello performa meglio della baseline, allora sai che stai fornendo valore alla tua azienda/team.
6. Dividere i dati in tre partizioni
Kaggle e i vari famosi creatori di contenuti sul web hanno trattato a fondo questo aspetto, ma è ancora qualcosa a cui prestare molta attenzione prima dell'addestramento.
Dobbiamo assicurarci che i nostri dati siano divisi in tre parti: set addestramento, di validazione e di test. Ecco le differenze:
- il set di addestramento (in inglese, train set) è usato per addestrare il tuo modello, ed è ciò che il nostro modello ""vede"" per imparare a predire il target
- il set di validazione non viene mostrato al modello, ed è usato per testare algoritmi e loro parametri
- il set di test non viene mostrato al modello e viene utilizzato per valutare l'intera pipeline
Mi piace usare questa metafora:
Il modello è come un bambino a scuola. Il bambino che studia a scuola è il tuo modello che impara dal train set, il bambino che fa gli esercizi a casa è il tuo modello che viene testato sul set di validazione e il bambino all'esame finale è il modello sul test set.
Ricorda che i tuoi set di validazione e test devono provenire dalla stessa distribuzione statistica del tuo set di addestramento, altrimenti il modello verrebbe esposto a dati non utili. È come il bambino che studia un capitolo su cui non sarà mai messo alla prova.
Conclusione
Ecco un sunto dei temi toccati
- Usa un file schema per tenere traccia delle tue feature e delle loro proprietà
- Scrivi tutte le informazioni rilevanti nel tuo file README.md durante tutte le fasi del progetto
- Imposta metriche di performance raggiungibili: parla con i tuoi stakeholder per capire le loro aspettative
- Scegli una e una sola metrica di performance
- Confronta i tuoi modelli con una baseline per capire il valore che stai aggiungendo all'operazione
- Assicurati di partizionare i dati nel modo corretto.
Commenti dalla community"
https://www.diariodiunanalista.it/posts/addestrare-un-modello-word2vec-da-zero/,"Queste rappresentazioni, chiamate embedding, sono utilizzate in molte attività di elaborazione del linguaggio naturale, come il clustering di parole, la classificazione e la generazione di testo.
L'algoritmo Word2Vec ha scandito l'inizio di una epoca nel mondo dell'NLP quando è stato introdotto per la prima volta da Google nel 2013.
Esso si basa sulle rappresentazioni di parole create da una rete neurale (da qui il termine embedding, incorporamenti in italiano) addestrata su corpus di dati molto grandi.
L'output di Word2Vec sono vettori, uno per ogni parola presente nel dizionario di addestramento, che catturano in maniera efficace relazioni tra le parole.
Vettori vicini nello spazio vettoriale hanno significati simili in base al contesto e i vettori distanti tra loro hanno significati diversi. Ad esempio, le parole ""forte"" e ""potente"" sarebbero vicini mentre ""forti"" e ""Parigi"" sarebbe relativamente lontani all'interno dello spazio vettoriale.
Questo è un notevole miglioramento rispetto alle performance del modello bag-of-words, che si basa sul semplice conteggio dei token presenti in un corpus di dati testuali.
In questo articolo andremo a esplorare Gensim, una libreria Python molto famosa per addestrare modelli di machine learning basati sul testo, per addestrare un modello Word2Vec da zero.
Useremo come dataset uno script presente nell'articolo
per reperire tutti i testi presenti in questo blog e usarli per addestrare un modello Word2Vec e mostrare come le parole usate nella scrittura siano relazionate tra di loro.
Questo permetterà a voi lettori di applicare lo script presente in questo post a qualsiasi scenario reale (a patto che il sito non sia completamente renderizzato in JavaScript) e creare voi stessi gli embedding e visualizzarli.
Iniziamo!
La ricetta per il progetto
Stiliamo una lista di azioni da fare per gettare le basi del progetto
- Creiamo un nuovo ambiente virtuale
(leggi qui per capire come: Come impostare un ambiente di sviluppo per il machine learning)
- Installiamo le dipendenze, tra le quali Gensim
- Eseguiamo lo script per scaricare i dati dal nostro blog target (in questo caso useremo proprio Diario Di Un Analista)
- Prepariamo il nostro corpus per consegnarlo a Word2Vec
- Addestriamo il modello e lo salviamo
- Usiamo TSNE e Plotly per visualizzare gli embedding comprendere visivamente lo spazio vettoriale generato da Word2Vec
- BONUS: Useremo la libreria Datapane per creare un report interattivo in HTML da condividere con chi vogliamo
Alla fine del percorso avremo tra le mani un'ottima base per sviluppare ragionamenti più complessi, come clustering degli embedding.
Darò per scontato che abbiate già configurato il vostro ambiente correttamente, quindi non spiegherò in questo articolo come farlo. Partiamo subito con lo scaricare i dati del blog.
Le dipendenze da installare
Prima di iniziare assicuriamoci di installare le seguenti dipendenze a livello di progetto, eseguendo
pip install XXXXX nel terminale.
trafilatura
pandas
gensim
nltk
tqdm
scikit-learn
plotly
datapane
Inizializziamo anche un oggetto
logger per ricevere i messaggi di Gensim in terminale.
Reperire il corpus di dati testuali
Come menzionato useremo lo script nel mio articolo linkato per scraperare questo blog e reperire gli articoli partendo dalla sitemap.
Creiamo uno script e incolliamo il seguente codice Python.
import pandas as pd
from tqdm import tqdm
from trafilatura.sitemaps import sitemap_search
from trafilatura import fetch_url, extract
from pprint import pprint
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
def get_urls_from_sitemap(resource_url: str) -> list:
""""""
Funzione che crea un DataFrame Pandas di URL e articoli.
""""""
urls = sitemap_search(resource_url)
return urls
def extract_article(url: str) -> dict:
""""""
Estrae un articolo da una URL con Trafilatura
""""""
downloaded = fetch_url(url)
article = extract(downloaded, favor_precision=True)
return article
def create_dataset(list_of_websites: list) -> pd.DataFrame:
""""""
Funzione che crea un DataFrame Pandas di URL e articoli.
""""""
data = []
for website in tqdm(list_of_websites, desc=""Websites""):
urls = get_urls_from_sitemap(website)
for url in tqdm(urls, desc=""URLs""):
d = {
'url': url,
""article"": extract_article(url)
}
data.append(d)
time.sleep(0.5)
df = pd.DataFrame(data)
df = df.drop_duplicates()
df = df.dropna()
return df
if __name__ == ""__main__"":
list_of_websites = [""https://www.diariodiunanalista.it/""]
df = create_dataset(list_of_websites)
df.to_csv(""dataset.csv"", index=False)
Abbiamo ora un file .csv nella nostra cartella di progetto che conterrà i dati in questo formato:
I dati testuali che andremo ad utilizzare sono sotto la colonna article. Vediamo come appare un testo preso a caso
Vediamo come questo debba essere processato prima di essere consegnato al modello Word2Vec. Dobbiamo andare a rimuovere le stopword italiane, pulire da punteggiatura, numeri e altri simboli. Questo sarà il prossimo step.
Preparazione del corpus di dati
La prima cosa da fare è importare delle dipendenze fondamentali per il preprocessing.
# librerie per la manipolazione del testo
import re
import string
import nltk
from nltk.corpus import stopwords
# nltk.download('stopwords') <-- eseguiamo questo comando per scaricare le stopword nel progetto
# nltk.download('punkt') <-- essenziale per la tokenizzazione
stopwords.words(""italian"")[:10]
>>> ['ad', 'al', 'allo', 'ai', 'agli', 'all', 'agl', 'alla', 'alle', 'con']
Ora creiamo una funzione
preprocess_text che prende in input un testo e restituisce una versione pulita dello stesso.
def preprocess_text(text: str, remove_stopwords: bool) -> str:
""""""Funzione che pulisce il testo in input andando a
- rimuovere i link
- rimuovere i caratteri speciali
- rimuovere i numeri
- rimuovere le stopword
- trasformare in minuscolo
- rimuovere spazi bianchi eccessivi
Argomenti:
text (str): testo da pulire
remove_stopwords (bool): rimuovere o meno le stopword
Restituisce:
str: testo pulito
""""""
# rimuovi link
text = re.sub(r""http\S+"", """", text)
# rimuovi numeri e caratteri speciali
text = re.sub(""[^A-Za-z]+"", "" "", text)
# rimuovere le stopword
if remove_stopwords:
# 1. crea token
tokens = nltk.word_tokenize(text)
# 2. controlla se è una stopword
tokens = [w.lower().strip() for w in tokens if not w.lower() in stopwords.words(""italian"")]
# restituisci una lista di token puliti
return tokens
Applichiamo questa funzione al dataframe Pandas usando una funzione lambda con
.apply.
df[""cleaned""] = df.article.apply(lambda x: preprocess_text(x, remove_stopwords=True))
Otteniamo una serie pulita.
Esaminiamo un testo per vedere l'effetto del nostro preprocessing.
Il testo ora sembra essere pronto ad essere processato da Gensim. Continuamo.
Addestramento di Word2Vec
La prima cosa da fare è creare una variabile
texts che conterrà i nostri testi.
texts = df.cleaned.tolist()
Ora siamo pronti ad addestrare il modello. Word2Vec può accettare tanti parametri, ma per ora non preoccupiamocene. Addestrare il modello è semplice, e richiede una riga di codice.
from gensim.models import Word2Vec
model = Word2Vec(sentences=texts)
Il nostro modello è pronto e gli embedding sono stati creati. Per verificarlo, proviamo a trovare il vettore per la parola overfitting.
Di default, Word2Vec crea vettori 100-dimensionali. Questo parametro può essere modificato, insieme a tanti altri, quando istanziamo la classe. In ogni caso, più dimensioni sono associate ad una parola, più informazioni avrà la rete neurale sulla parola stessa e la sua relazione alle altre.
Ovviamente questo ha un costo computazionale e di memoria più elevato.
Nota bene: una delle limitazioni più importanti di Word2Vec è l'inabilità di generare vettori per parole non presenti nel vocabolario (chiamate OOV - out of vocabulary words).
Per gestire parole nuove, quindi, bisogna addestrare un nuovo modello oppure aggiungere i vettori manualmente.
Calcolare la similarità tra due parole
Con il coseno di similarità possiamo calcolare quanto distanti siano vettori nello spazio.
Con il comando di seguito istruiamo Gensim a trovare le prime 3 parole più simili a overfitting
model.wv.most_similar(positive=['overfitting'], topn=3))
Vediamo come la parola ""quando"" sia presente in questo risultato. Sarà il caso di includere nelle stop word anche avverbi simili per pulire i risultati.
Per salvare il modello, basta fare
model.save(""./percorso/al/modello"").
Visualizzare gli embedding con TSNE e Plotly
I nostri vettori sono 100-dimensionali. È un problema visualizzarli a meno che non facciamo qualcosa per ridurre la loro dimensionalità.
Useremo la tecnica TSNE per ridurre la dimensionalità dei vettori e creare due componenti, una per l'asse X e l'altra per la Y su un grafico a dispersione.
Nella .gif in basso è possibile vedere le parole embeddate nello spazio grazie alle funzionalità di Plotly.
Ecco il codice per generare questa immagine.
def reduce_dimensions(model):
num_components = 2 # numero di dimensioni da mantenere in seguito alla compressione
# estraiamo il vocabolario dal modello e i vettori in modo da associarli nel grafico
vectors = np.asarray(model.wv.vectors)
labels = np.asarray(model.wv.index_to_key)
# applichiamo TSNE
tsne = TSNE(n_components=num_components, random_state=0)
vectors = tsne.fit_transform(vectors)
x_vals = [v[0] for v in vectors]
y_vals = [v[1] for v in vectors]
return x_vals, y_vals, labels
def plot_embeddings(x_vals, y_vals, labels):
import plotly.graph_objs as go
fig = go.Figure()
trace = go.Scatter(x=x_vals, y=y_vals, mode='markers', text=labels)
fig.add_trace(trace)
fig.update_layout(title=""Word2Vec - Visualizzazione embedding con TSNE"")
fig.show()
return fig
x_vals, y_vals, labels = reduce_dimensions(model)
plot = plot_embeddings(x_vals, y_vals, labels)
Questa visualizzazione può essere utile per notare tendenze semantiche e sintattiche nei dati.
Ad esempio, è molto utile per mettere in risalto anomalie, come gruppi di parole che tendono a raggrupparsi insieme per un qualche motivo.
Parametri di Word2Vec
Guardando sul sito di Gensim vediamo che i parametri sono parecchi. I più importanti sono
vectors_size,
min_count,
window e
sg.
- vectors_size: definisce le dimensioni del nostro spazio vettoriale.
- min_count: le parole al di sotto della frequenza min_count vengono rimosse dal vocabolario prima dell'addestramento.
- window: distanza massima tra la parola corrente e quella prevista all'interno di una frase.
- sg: definisce l'algoritmo di addestramento. 0 = CBOW (continuous bag of words), 1 = Skip-Gram.
Non andremo nel dettaglio in ognuno di questi. Suggerisco al lettore interessato di dare una occhiata alla documentazione di Gensim.
Proviamo a riaddestrare il nostro modello con i seguenti parametri
VECTOR_SIZE = 100
MIN_COUNT = 5
WINDOW = 3
SG = 1
new_model = Word2Vec(sentences=texts, vector_size=VECTOR_SIZE, min_count=MIN_COUNT, sg=SG)
x_vals, y_vals, labels = reduce_dimensions(new_model)
plot = plot_embeddings(x_vals, y_vals, labels)
Vediamo come la rappresentazione cambi molto. Il numero di vettori è uguale a quello precedente (Word2Vec ha come default 100), mentre
min_count,
window e
sgsono stati cambiati dai valori di default.
Suggerisco al lettore di cambiare questi parametri per comprendere quale rappresentazione sia più la adeguata alla propria casistica.
BONUS: Creare un report interattivo con Datapane
Siamo arrivati alla fine dell'articolo. Concludiamo il progetto andando a creare un report interattivo in HTML con Datapane, che permetterà all'utente di visualizzare direttamente in browser il grafico creato precedentemente con Plotly.
e questo è il codice Python
import datapane as dp
app = dp.App(
dp.Text(text='# Visualizzazione degli embedding creati con Word2Vec'),
dp.Divider(),
dp.Text(text='## Grafico a dispersione'),
dp.Group(
dp.Plot(plot),
columns=1,
),
)
app.save(path=""test.html"")
Datapane è altamente customizzabile. Consiglio al lettore di studiare la documentazione per integrare estetica e altre funzionalità.
Conclusioni
Abbiamo visto come costruire degli embedding da zero usando Gensim e Word2Vec. La cosa è molto semplice da fare se si ha un dataset strutturato e se si conosce l'API di Gensim.
Con gli embedding possiamo fare veramente tante cose, ad esempio
- fare clustering dei documenti, visualizzando tali cluster nello spazio vettoriale
- fare ricerca su similarità tra le parole
- usare gli embedding come feature in un modello di machine learning
- gettare le basi per la machine translation
e molto altro. Se siete interessati ad un argomento che estende quello trattato qui, lasciate un commento nel box qui sotto 👍
Con questo progetto potete arricchire il vostro portfolio di template per NLP e comunicare ad uno stakeholder expertise nel trattare documenti testuali nel contesto del machine learning.
Come sempre, se avete bisogno di me o volete condividere qualche pensiero, contattatemi sui social o attraverso il blog.
A presto 👋
Commenti dalla community"
https://www.diariodiunanalista.it/posts/algoritmi-di-clustering-guida-completa-alla-comprensione-e-allapplicazione/,"In questo articolo, esploreremo il mondo affascinante degli algoritmi di clustering, svelando il loro significato e dimostrando perché rappresentano uno strumento cruciale nell'analisi dei dati.
Gli algoritmi di clustering sono come i detective dei dati, il cui compito principale è identificare pattern e relazioni nascoste nei tuoi dati.
Ci aiutano a rispondere a domande come:
- Quali sono le similitudini tra i miei clienti?
- Come posso suddividere i miei prodotti in categorie significative?
- Quali sono le tendenze nei dati che potrebbero sfuggire all'occhio umano?
La capacità di queste tecniche di poter rispondere a domande del genere rende la cluster analysis una delle metodiche più utilli (e quindi pagate) nell'ambito dell'analisi dei dati e della data science.
Nel corso di questo articolo, esamineremo in dettaglio cosa sono gli algoritmi di clustering, come funzionano e perché sono un elemento fondamentale in vari settori, dalla marketing intelligence all'analisi delle reti sociali.
Anche se non sei un esperto in analisi dei dati, non preoccuparti - cercherò di spiegare tutto in modo chiaro e accessibile, in modo che alla fine dell'articolo sarai in grado di comprendere l'importanza dei clustering nella tua sfera di interesse.
Prima di immergerci nei dettagli tecnici, cominciamo con una panoramica generale su cosa siano esattamente questi algoritmi di clustering e perché dovresti interessartene.
Cosa sono gli algoritmi di clustering?
Per iniziare, diamo una definizione semplice ma esaustiva:
Gli algoritmi di clustering sono procedure matematiche progettate per organizzare un insieme di dati in gruppi omogenei o ""cluster"" in base alle loro caratteristiche simili.
Immagina di avere un cesto pieno di frutta mista e il tuo obiettivo è separare le mele dalle banane, gli agrumi dai kiwi. Gli algoritmi di clustering farebbero proprio questo, ma su un grande set di dati, in modo più sofisticato e accurato di quanto potremmo fare manualmente.
Nel contesto del machine learning, gli algoritmi di clustering ricadono nella categoria degli algoritmi non supervisionati. Significa che essi sono in grado di raggiungere i risultati per cui sono progettati senza aver bisogno di dati da cui apprendere.
Se vuoi saperne di più su cosa sia il machine learning e le differenze tra l'apprendimento supervisionato e non supervisionato, ti suggerisco di leggere il seguente articolo
Perché sono importanti?
Ora che sa cosa sono, la domanda successiva è: perché dovresti preoccuparti del clustering e degli algoritmi ad essi associati? Ecco alcune ragioni chiave:
Comprendere i dati
Gli algoritmi di clustering aiutano a rendere comprensibili grandi quantità di dati.
Immagina di avere migliaia di punti dati che rappresentano gli utenti di un sito web. Con il clustering, puoi suddividerli in gruppi basati su comportamenti o preferenze comuni, permettendoti di capire meglio chi sono i tuoi utenti e come interagiscono con il tuo sito.
Nell'immagine di esempio - gli utenti nel cluster giallo sono quelli che mostrano un comportamento ideale: poche pagine visitate, alto numero di acquisti. Questi sono gli utenti che vogliamo targettare con delle attività di marketing!
Identificare anomalie nei dati
Il clustering può rivelare pattern o tendenze che potrebbero non essere evidenti a una prima occhiata. Ad esempio, potrebbero aiutarti a identificare gruppi di clienti che acquistano prodotti simili ma che potresti non aver mai associato in precedenza.
Ho scritto un articolo dedicato a questo use case 👇
Aumentare l'efficienza
In molti settori, come la logistica o la produzione, l'uso del clustering può portare a una migliore organizzazione delle risorse. Ad esempio, potresti ottimizzare le consegne suddividendo le destinazioni in cluster geografici.
Personalizzazione
Il clustering può essere utilizzati per personalizzare l'esperienza dell'utente. Ad esempio, un sito di e-commerce potrebbe suggerire prodotti simili a quelli già acquistati da un cliente, basandosi sui cluster a cui appartiene.
Segmentazione del mercato
Nel campo del marketing, il clustering puòaiutare a identificare segmenti di mercato specifici, consentendo alle aziende di adattare le loro strategie di marketing in modo più mirato.
In sostanza, fare clustering sui propri dati può far emergere notevoli insight in base proprio ai dati che abbiamo e al contesto lavorativo.
A breve esploreremo alcuni dei principali tipi di algoritmi di clustering e come ciascuno di essi affronta il compito di raggruppare dati in modo efficiente e accurato. Faremo questo in Python e Sklearn.
Alcuni degli algoritmi di clustering più comuni
Qui vedremo alcuni degli algoritmi di clustering più usati nell'ambito della analisi dei dati.
Utilizzando Scikit-Learn, nota libreria di machine learning in Python, è possibile creare rapidamente delle visualizzazioni per questi algoritmi. Nell'immagine in basso è possibile notare come, sullo stesso set di dati, diversi algoritmi di clustering restituiscano raggruppamenti notevolmente diversi.
Questo perché ognuno di questi algoritmi ha il proprio approccio unico per raggruppare i dati, il che li rende adatti a diverse situazioni.
K-Means
Questo è uno degli algoritmi di clustering più popolari. Si basa sulla divisione dei dati in un numero specifico di cluster (k) in modo che ogni punto dati sia assegnato al cluster più vicino al suo centro. È ampiamente utilizzato in applicazioni come la segmentazione dei clienti e la compressione delle immagini.
Su questo blog è presente un articolo dedicato proprio al K-Means
Clustering gerarchico
Questo algoritmo crea una gerarchia di cluster, a partire da singoli punti dati e combinandoli gradualmente in cluster più grandi. Questo permette di visualizzare i dati a diverse scale di dettaglio ed è utile quando non si conosce a priori il numero di cluster desiderato.
Ti piace quello che stai leggendo?
Iscriviti al blog per rimanere sempre aggiornato sui contenuti di settore. No pubblicità. No spam.
Solo contenuto utile per la tua carriera nel ML e DS.
Puoi rimuovere l'iscrizione quando vuoi seguendo il link nella email.
DBSCAN
DBSCAN (Density-Based Spatial Clustering of Applications with Noise) è un algoritmo che trova cluster basati sulla densità dei punti dati. È particolarmente efficace nel rilevare cluster di forme irregolari e può identificare punti dati isolati come rumore.
Gaussian Mixture Models (GMM)
GMM è un modello statistico che assume che i dati siano generati da una miscela di distribuzioni gaussiane. È spesso utilizzato per problemi di clustering probabilistico e può essere esteso per modellare cluster con diverse forme e dimensioni.
Altri tipi di algoritmi di clustering oltre a quelli tradizionali
Finora abbiamo esplorato alcuni dei concetti fondamentali relativi agli algoritmi di clustering e ai loro utilizzi.
Tuttavia, il mondo dell'analisi dei dati è vasto e complesso, e ci sono molti altri algoritmi di clustering avanzati che vale la pena conoscere. In questa sezione, ci concentreremo su tre di questi: il biclustering, il fuzzy clustering e lo spectral clustering.
Biclustering
Il biclustering, a volte chiamato co-clustering, è un approccio da considerare quando hai bisogno di suddividere i dati in cluster sia lungo le righe che lungo le colonne. In altre parole, non cerchi solo di raggruppare le istanze dei dati, ma anche le loro caratteristiche.
Questo è particolarmente utile in applicazioni in cui si desidera identificare sottogruppi di dati che mostrano comportamenti simili su un sottoinsieme di attributi.
Un esempio pratico potrebbe essere l'analisi dei geni in un esperimento di espressione genica, dove vuoi trovare gruppi di geni che vengono regolati in modo simile in diverse condizioni sperimentali. Alcuni algoritmi biclustering noti includono il Spectral Co-Clustering e il BiMax.
Fuzzy clustering
Nel clustering fuzzy, a differenza del K-Means tradizionale, i punti dati possono appartenere a più di un cluster con gradi diversi di appartenenza.
Invece di assegnare rigidamente ciascun punto a un solo cluster, l'algoritmo fuzzy attribuisce a ciascun punto un valore di appartenenza a ciascun cluster. Questo approccio è utile quando i dati possono avere una natura ambigua o quando potrebbero appartenere a più cluster in base a diverse caratteristiche.
Ad esempio, nell'analisi delle immagini mediche, potresti utilizzare il fuzzy clustering per assegnare una probabilità di appartenenza di ciascun pixel a diverse regioni dell'immagine, come tessuto sano o tumorale.
Spectral clustering
Lo spectral clustering è un approccio basato sulla teoria dei grafi e la matrice di similarità tra i punti dati.
Questo metodo è particolarmente efficace nel rilevare cluster con forme non necessariamente sferiche e può essere applicato a dati di alta dimensionalità. La chiave dello spectral clustering è la trasformazione dei dati in uno spazio diverso, dove la struttura dei cluster è più evidente.
Questo tipo di clustering è utilizzato in una varietà di applicazioni, tra cui rilevamento delle comunità in reti sociali e segmentazione delle immagini.
Come valutare l'efficacia degli algoritmi di clustering e interpretare i risultati ottenuti
Ora che abbiamo esaminato vari tipi di algoritmi di clustering, è importante comprendere come valutare l'efficacia di questi algoritmi e come interpretare i risultati ottenuti. Ecco alcune delle tecniche e metriche chiave utilizzate per questa valutazione.
Indici di Validità del Cluster (CVI)
Gli indici di validità del cluster sono misure quantitative utilizzate per valutare l'efficacia di un algoritmo di clustering.
Questi indici forniscono una valutazione oggettiva della qualità dei cluster formati. Alcuni esempi comuni di indici di validità del cluster includono:
- Indice di Silhouette: Questo indice misura quanto ogni punto dati è simile ai punti del suo stesso cluster rispetto ai punti degli altri cluster. Varia da -1 a 1, dove valori più alti indicano cluster più coesi.
- Indice del Gomito (Elbow Method): Questo metodo aiuta a determinare il numero ottimale di cluster (k) confrontando la variazione della somma dei quadrati all'interno dei cluster (WCSS) per diversi valori di k. Il punto in cui la variazione inizia a diminuire drasticamente è spesso scelto come il numero ottimale di cluster. È spessissimo utilizzato per valutare la qualità dei cluster del K-Means.
- Indice di Davies-Bouldin: Questo indice valuta la media delle similarità tra ciascun cluster e il cluster più simile ad esso. Un valore basso indica cluster ben separati.
- Indice di Dunn: Questo indice misura la distanza minima tra i centroidi dei cluster diviso dalla massima distanza tra i punti di ciascun cluster. Un valore alto indica cluster compatti e ben separati.
- Indice di Calinski-Harabasz (Varianza tra-cluster / Varianza intra-cluster): Questo indice confronta la varianza tra i cluster con la varianza all'interno dei cluster. Un valore più alto suggerisce cluster di alta qualità.
La visualizzazione dei dati
Tecniche di visualizzazione dei dati multidimensionali come la PCA o la t-distributed Stochastic Neighbor Embedding (t-SNE) possono essere molto utili per visualizzare la separazione dei nostri cluster.
È possibile usare la PCA o t-SNE per proiettare i tuoi dati clusterizzati in uno spazio bidimensionale o tridimensionale in modo da poterli visualizzare facilmente e valutare visivamente. Ho trattato l'argomento in dettaglio a questo articolo 👇
Interpretazione dei risultati
Interpretare i risultati del clustering è un passaggio critico. Dopo aver eseguito l'algoritmo e calcolato le metriche di validità del cluster, devi considerare il contesto e gli obiettivi del tuo problema. Alcune delle domande chiave da porsi includono:
- Sono i cluster risultanti significativi dal punto di vista pratico? Hanno senso per il tuo contesto / dominio lavorativo?
- Come cambiano i risultati al variare dei parametri dell'algoritmo (come il numero di cluster k nel caso di K-Means)?
- Quali sono le caratteristiche principali che distinguono i cluster? Puoi utilizzare tecniche di visualizzazione o analisi delle caratteristiche per rispondere a questa domanda.
- Come possono i risultati del clustering essere utilizzati per prendere decisioni o formulare ipotesi?
- C'è un miglioramento nei risultati dopo aver applicato tecniche di preprocessing dei dati?
- Come si confrontano i risultati del clustering con i risultati desiderati o con l'expertise del dominio?
Il lavoro di data scientist è arriva al culmine della complessità proprio durante la fase di interpretazione dei dati: il processo è un'arte tanto quanto una scienza.
È importante utilizzare una combinazione di metriche quantitative e comprensione qualitativa per determinare se il clustering è utile per il tuo problema e per trarre conclusioni significative dai dati.
Conclusione
In questo articolo, abbiamo esplorato l'affascinante mondo degli algoritmi di clustering e le diverse metriche utilizzate per valutarne l'efficacia.
Abbiamo iniziato comprendendo cos'è il clustering e perché è importante nell'analisi dei dati. Abbiamo discusso di diversi tipi di algoritmi di clustering, dai classici come il K-Means agli avanzati come il biclustering, il fuzzy clustering e lo spectral clustering.
Abbiamo anche approfondito come valutare l'efficacia dei cluster utilizzando gli indici di validità del cluster: questi strumenti forniscono un modo obiettivo per determinare il numero ottimale di cluster e valutare la coesione e la separazione dei cluster formati.
Infine, abbiamo discusso l'importanza di interpretare i risultati del clustering alla luce del contesto del problema e degli obiettivi specifici. La visualizzazione dei risultati tramite tecniche come t-SNE può essere di grande aiuto per capire come i dati sono stati suddivisi nei cluster.
Conoscere le tecniche di clustering qui discusse ti metterà in una posizione molto potente dal punto di vista professionale: stai di fatto imparando metodiche applicabili ad vasta gamma di settori, dalla segmentazione del mercato al rilevamento delle comunità in reti sociali.
Mentre continui il tuo viaggio nell'analisi dei dati, ricorda che l'arte del clustering richiede pratica e sperimentazione.
Ogni set di dati è unico, e il tuo spirito di esplorazione e la tua comprensione delle metriche di validità del cluster ti guideranno nel prendere decisioni informate e nell'ottenere intuizioni preziose dai tuoi dati.
Commenti dalla community"
https://www.diariodiunanalista.it/posts/algoritmi-di-machine-learning-guida-introduttiva-per-comprendere-i-principi-e-le-applicazioni/,"Se sei qui è perché vuoi saperne di più sul machine learning e scoprire cosa si intende per con il termine ""algoritmo"" o ""modello"".
A differenza dell'articolo già presente su questo blog intitolato Cos'è il Machine Learning: come spiego il concetto ad un neofita, qui mi focalizzerò a darti una introduzione ai principali algoritmi che compongono lo spazio del machine learning tradizionale e non.
Questi algoritmi sono il cuore pulsante di molte applicazioni moderne - di fatto, permettono ai computer di apprendere dai dati e prendere decisioni intelligenti senza essere esplicitamente programmati.
- a categorizzare gli algoritmi del machine learning
- quali sono i principali algoritmi e perché dovresti conoscerli
- come scegliere uno o più algoritmi per il tuo caso d'uso
- risorse per iniziare a scrivere codice Python per implementare l'algoritmo scelto
Questo articolo, insieme a quello linkato sopra, rappresenta un buon punto di ingresso nel mondo della data science e machine learning.
Ti condivido anche la pagina Inizia Qui che contiene una lista curata di link di blog da seguire in ordine per iniziare il tuo percorso anche programmatico.
Iniziamo!
Categorizzazione degli Algoritmi di Machine Learning
Prima di addentrarci nei dettagli degli algoritmi e modelli, è importante comprendere la tassonomia di tali concetti.
I software basati sul machine learning ossono essere suddivisi in diverse categorie, quali:
Apprendimento Supervisionato
In questo tipo di apprendimento, il modello viene addestrato su un set di dati in cui sono presenti sia le variabili di input che quelle di output desiderate. L'obiettivo è imparare una mappatura tra input e output in modo che il modello possa fare previsioni accurate su nuovi dati.
Apprendimento Non Supervisionato
Qui, il modello viene addestrato su dati senza etichette. L'obiettivo principale è scoprire pattern o strutture nascoste nei dati, come clustering o riduzione della dimensionalità.
Nell'immagine in alto vediamo come un algoritmo di clustering sia in grado di esprimere due gruppi: quello in alto e quello in basso. Questo avviene senza la presenza di etichette di addestramento.
Apprendimento per Rinforzo
Questo tipo di apprendimento è simile all'apprendimento supervisionato, ma il modello apprende attraverso l'interazione con un ambiente in cui prende decisioni e riceve feedback in base alle sue azioni. È ampiamente utilizzato in applicazioni di intelligenza artificiale come i giochi.
Esistono anche altre categorie (come l'approccio evolutivo o genetico) ma non verranno trattati in questo articolo.
Ecco un breve video in inglese di Udacity che spiega il paradigma del reinforcement learning
Algoritmo vs Modello di Machine Learning: qual è la differenza?
Un altro ingrediente fondamentale per la tua comprensione dell'articolo è differenziare adeguatamente cosa si intende, nella data science e ML, per algoritmo e modello.
Definizione di algoritmo
Un algoritmo è una sequenza di istruzioni logicamente organizzate e definite che descrive un processo o una procedura computazionale.
Nient'altro. Un algoritmo non è altro che una ricetta che un PC segue per raggiungere un risultato.
Uno script Python è un algoritmo, un albero decisionale è un algoritmo, una ricerca binaria è un algoritmo.
Definizione di modello
Un modello è un software che viene inserito nell'algoritmo e che ci serve per trovare la soluzione al nostro problema. Poiché spesso non conosciamo la vera soluzione, queste vengono chiamate predizioni.
Quindi, un modello di machine learning è spesso contenuto o implementato all'interno di un algoritmo. L'insieme viene chiamato conveniente mente algoritmo.
Come menziono nel mio articolo introduttivo al machine learning (link sopra):
Un algoritmo non è altro che una serie di istruzioni seguite da un computer. [...]
Un modello invece è un software che viene inserito nell'algoritmo e che ci serve per trovare la soluzione al nostro problema.
Ora hai le basi per comprendere quello che segue: i principali algoritmi di Machine Learning tradizionale.
Principali Algoritmi di Machine Learning
Copriremo, per comodità e perché fondamentalmente accessibili a tutti, gli algoritmi principali di Scikit-Learn. In particolare copriremo gli algoritmi di regressione, classificazione e clustering.
Per ultimo, menzionerò anche XGBoost, LightGBM e CatBoost - alcuni dei modelli di machine learning tradizionali più famosi e utilizzati grazie alle loro performance su dati tabellari. Questi non fanno parte di Scikit-Learn.
Algoritmi di Regressione
Partiremo con gli algoritmi di regressione - cioè quelli che creano previsioni numeriche continue sui dati in input. Ad esempio, un algoritmo di regressione può predire il prezzo di un oggetto oppure quanti giorni occorrono ad un progetto per raggiungere un certo obiettivo.
Regressione Lineare
La regressione lineare è l'algoritmo introduttivo al machine learning. L'algoritmo classico si chiama OLS (Ordinary Least Squares), che adatta un modello lineare con coefficienti \( w = (w_{1}, ..., w_{n}) \) per ridurre al minimo la somma residua dei quadrati tra gli i dati osservati e le previsioni fatte dall'approssimazione lineare.
Quando si parla di algoritmo lineare significa che esiste una equazione che descrive la relazione tra le variabili dipendente y e indipendente X.
La linea blu rappresenta la previsione del modello, in nero invece i punti del dataset osservati. Il modello vuole ridurre la distanza tra i punti neri e i punti in blu. Quando i punti neri sono sulla linea blu allora l'errore è 0 e l'osservazione reale è uguale alla predizione del modello.
L'espressione matematica è espressa con una retta \( y = mx + b \) la quale vuole sempre minimizzare la distanza tra i punti e la retta stessa.
Ci sono molti tipi di algoritmo di regressione e non sarà necessario documentarli tutti in questo articolo. Di fatto, ognuno di questi algoritmi ha qualcosa che l'altro non ha e che lo rende adatto in base al contesto.
Alcuni dei più comuni algoritmi lineari per la regressione sono:
- Regressione Lasso, che applica una regolarizzazione L1
- Regressione Ridge, che applica una regolarizzazione L2
- Elastic Net, che applica entrambe le regolarizzazioni
- Stocastic Gradient Descent (SGD), che applica l'algoritmo della discesa del gradiente
Se vuoi saperne di più sulla regolarizzazione, ho scritto un articolo per te qui
Regressioni non lineari
Esistono algoritmi che non vogliono adattare una linea ai dati, bensì approssimare le osservazioni con logiche non lineari.
Alcuni di questi algoritmi sono gli alberi decisionali e le reti neurali. Questi algoritmi usano logiche uniche per esprimere la relazione tra caratteristiche e target.
Vediamo in basso l'immagine di come un albero decisionale vada a descrivere tale relazione, che è molto diversa da una linea come nell'immagine in alto.
Mentre un modello lineare creerebbe una relazione del genere
\[ Prezzo\ della\ frutta = m_{1}Freschezza + \newline m_{2}Freschezza + b_{0} \]
Un albero farebbe un ragionamento basato su nodi: Nodi: Maturi - Sì o no | Fresco - Sì o No | Taglia - <5, >5 ma <10 e >10 e così via.
Una rete neurale invece ha proprietà molto particolari che si basano fondamentalmente sulle funzioni di attivazione. Sostanzialmente, una rete neurale è lineare nelle variabili ma può restituire mappature non lineari proprio grazie alle funzioni di attivazione, come ad esempio ReLU.
Se vuoi leggere di più sulle reti neurali e su come funzionano, ti linko un articoo interessante
Alcuni dei modelli di regressione non lineari più comuni sono:
- KNN (K-Nearest Neighbors)
- Regressione Gaussiana
- Alberi decisionali
- Reti Neurali
Algoritmi di Classificazione
Quello che differenzia un compito di regressione da uno di classificazione è il tipo di previsione che il modello deve fare.
Nel caso della regressione abbiamo detto che l'output è un numero continuo reale, come il prezzo di una casa in vendita.
Nel caso della classificazione abbiamo, per l'appunto, delle classi. Ad esempio, un classificatore è un modello di machine learning che può prevedere le classi spam o non spam quando riceve in input il testo di una email.
Dietro le quinte, un modello di classificazione è in realtà uno di regressione, poiché ad ogni classe corrisponde un numero...ma questo viene poi convertito in una etichetta come spam o non spam.
Parecchi degli algoritmi che fanno regressione quindi possono fare anche classificazione. In Sklearn tipicamente un modello di regressione la forma nome_modello +
Regressor oppure nome_modello +
Classifier.
Regressione Logistica
La regressione logistica è il modello di machine learning di classificazione più comune.
Nonostante il suo nome, il modello è implementato come modello lineare per la classificazione piuttosto che per la regressione in termini di nomenclatura.
La regressione logistica è nota in letteratura anche come regressione logit. In questo modello, le probabilità che descrivono i possibili risultati sono modellate utilizzando una funzione logistica.
La funzione logistica crea la famosa curva sigmoide, dove tipicamente i valori maggiori di 0.5 vengono convertiti come 1, mentre quelli inferiori a 0.5 vengono convertiti come 0. Questo rende la sigmoide utile per modellare decisioni binarie.
Macchine a vettori di supporto (Support Vector Machines, SVM)
Le SVM sono dei modelli di machine learning molto comuni per la classificazione, nonostante esista anche la versione dedicata alla regressione.
La matematica dietro le SVM è complessa, ma l'idea è quella di trovare le equazioni che disegnano delle linee rette tra i vettori più vicini alla linea di demarcazione tra le diverse classi, chiamati proprio vettori di supporto.
L'obiettivo delle SVM è trovare un iperpiano nello spazio delle caratteristiche che può separare in modo ottimale le diverse classi di punti. L'iperpiano può essere una linea in 2D, piano in 3D o iperpiano in spazi superiori che massimizza la distanza tra i punti più vicini delle diverse classi. Questi punti più vicini sono chiamati vettori di supporto.
La distanza tra l'iperpiano e il vettore di supporto più vicino è chiamata margine. Le SVM cercano di massimizzare questo margine poiché una maggiore distanza offre una migliore generalizzazione e una maggiore capacità dell'algoritmo di classificare nuovi dati in modo accurato.
A volte, i dati non sono linearmente separabili, il che significa che non possono essere separati da un singolo iperpiano. Una delle particolarità delle SVM è la abilità di affrontare dataset non linearmente separabili grazie al trucco del kernel (kernel trick).
In questo caso, le SVM utilizzano provano a mappare i dati in uno spazio di dimensioni superiori, dove potrebbero essere linearmente separabili. Questo consente alle SVM di affrontare problemi di separazione più complessi.
Per questo motivo però le SVM sono tipicamente pesanti da usare poiché richiedono più memoria e tempi di esecuzione rispetto agli algoritmi menzionati.
Naïve Bayes
Gli algoritmi Bayesiani naïve sono molto utili per classificare documenti e in generale del testo.
Si basano sul teorema di Bayes, che è una formula per calcolare la probabilità condizionata di un evento basandosi su altre informazioni correlate all'evento.
Il teorema di Bayes afferma che
\[ P(A | B) = \frac{P(B | A) \times P(A)}{P(B)} \]
Dove:
- \( P(A | B) \) rappresenta la probabilità condizionata che l'evento A si verifichi dato che l'evento B si è verificato.
- \( P(B | A) \) rappresenta la probabilità condizionata che l'evento B si verifichi dato che l'evento A si è verificato.
- \( P(A) \) rappresenta la probabilità a priori dell'evento A.
- \( P(B) \) rappresenta la probabilità a priori dell'evento B.
La componente naïve del Naive Bayes deriva dall'assunzione che le caratteristiche utilizzate per la classificazione siano indipendenti tra loro, cioè che la presenza o l'assenza di una caratteristica non influisce sulla presenza o l'assenza di altre caratteristiche. Questa assunzione semplifica i calcoli e rende il modello più efficiente.
Il modello calcola le probabilità a priori per ciascuna classe (\(C \)) e le probabilità condizionali per ciascuna caratteristica (\(X \)) dato che l'istanza appartiene a una classe.
Per classificare una nuova istanza, il classificatore calcola la probabilità \( P(C | X) \) per ciascuna classe possibile. La classe con la probabilità massima diventa la predizione del modello.
Random Forest
Entriamo nel merito degli ensemble: gruppi di logiche e modelli che sono deboli presi singolarmente, ma che lavorando insieme raggiungono performance migliori.
Come prima, algoritmi del genere possono soddisfare sia compiti di regressione che di classificazione.
La particolarità del random forest (tradotto foresta di alberi casuali) è che è l'algoritmo che molto spesso fornisce i migliori risultati di benchmark per la maggior parte dei task di classificazione su dati tabellari.
Il random forest utilizza una ""foresta"" di alberi decisionali. Ogni albero crea una predizione indipendenti su un sottoinsieme casuale dei dati e fa delle previsioni basate su di essi.
Ogni albero nella foresta fa una previsione. Nel nostro esempio, uno potrebbe dire ""Sì, penso che pioverà domani"" e un altro potrebbe dire ""No, penso che non pioverà"". Quindi, ogni albero vota per la sua previsione.
Il risultato finale del Random Forest è ottenuto prendendo la previsione che ottiene più voti tra tutti gli alberi. In altre parole, si prende la votazione di maggioranza tra gli alberi.
L'idea chiave qui è che combinando le opinioni di molti alberi diversi, si ottiene una previsione più affidabile e robusta, che è proprio il concetto dell'ensemble. Questo rende il Random Forest molto potente perché riduce il rischio che un singolo albero prenda decisioni errate a causa di dati casuali o rumore.
Ti piace quello che stai leggendo?
Iscriviti al blog per rimanere sempre aggiornato sui contenuti di settore. No pubblicità. No spam.
Solo contenuto utile per la tua carriera nel ML e DS.
Puoi rimuovere l'iscrizione quando vuoi seguendo il link nella email.
Oltre Sklearn: XGBoost
XGBoost (Extreme Gradient Boosting) è un algoritmo di machine learning molto potente, che è considerato come lo stato dell'arte della predizione su dati tabellari.
L'obiettivo principale di XGBoost è trovare il miglior equilibrio tra la complessità degli alberi (quanto sono profondi e complessi) e la precisione della previsione.
L'algoritmo cerca di ottenere alberi che siano abbastanza complessi da adattarsi ai dati, ma non così complessi da soffrire di overfitting (ovvero, adattarsi troppo ai dati di allenamento e non generalizzare bene a nuovi dati).
XGBoost è basato su alberi di decisione, simili a quanto abbiamo discusso per il random forest. La differenza sta nel fatto che XGB addestra questi alberi uno alla volta. Inizia con un albero e poi ne aggiunge altri in modo incrementale. Ogni nuovo albero cerca di correggere gli errori commessi dai precedenti.
Gli alberi deboli hanno dei pesi associati - questi pesi rappresentano quanto ciascun albero è abile nel risolvere il problema. XGBoost assegna un peso maggiore agli alberi che contribuiscono di più alla riduzione dell'errore complessivo.
Per fare una previsione, XGBoost combina le previsioni di tutti gli alberi deboli, ponderate in base ai loro pesi. In questo modo, si ottiene una previsione finale che è una combinazione delle previsioni di tutti gli alberi.
Attraverso questo meccanismo, XGBoost è l'algoritmo più comune e gettonato per risolvere problemi tabellari di classificazione principalmente.
LightGBM
Acronimo di Light Gradient Boosting Machine, LGBM è considerato l'erede di XGBoost grazie alla sua abilità di essere addestrato e fare inferenze più velocemente.
La caratteristica chiave di LightGBM è che cerca di creare alberi più leggeri rispetto ad XGBoost. Invece di espandere l'albero in modo completo durante l'addestramento, LightGBM lo espande in modo graduale. Questo significa che l'albero ha meno profondità e meno nodi, rendendolo più leggero e più veloce da addestrare.
LGBM offre quindi performance simili e a volte superiori a XGBoost perché fondamentalmente applica delle logiche simili a quest'ultimo, preferendo performance più veloci.
Per questo motivo, LightGBM sta superando XGBoost in popolarità come algoritmo per modellare i dati tabellari più performante sia per task di regressione che classificazione.
CatBoost
CatBoost è simile a XGBoost e LightGBM, ma ha alcune caratteristiche uniche.
Come XGBoost e LightGBM, CatBoost si basa sulla creazione di alberi di decisione, la differenza sta nella gestione delle variabili categoriali. Le variabili categoriali sono quelle che rappresentano categorie, come il genere (maschio o femmina).
CatBoost è progettato per trattare automaticamente queste variabili, senza richiedere una codifica speciale. Questo semplifica notevolmente il processo di preparazione dei dati.
Come altri algoritmi di boosting, CatBoost combina le previsioni di tutti gli alberi nel modello per ottenere una previsione finale. Questo processo di combinazione produce una previsione complessiva più accurata.
Dove puoi imparare praticamente il machine learning?
Passiamo ora ai next step. Hai imparato in linea teorica cosa sia un modello, un algoritmo e il machine learning in generale.
Hai anche imparato a classificare i compiti in supervisionato e non supervisionato e ti è chiaro quali siano gli algoritmi più comuni nel settore.
Cosa fare ora?
Il mio consiglio è di seguire questa scaletta:
- Continua a leggere navigando la pagina introduttiva agli argomenti trattati in questo blog - in questa pagina raccolgo i temi trattati in livelli da principiante ad esperto.
- Naviga il materiale educativo per orientarti in risorse esterne a questo blog che reputo attendibili e altamente qualificate all'apprendimento. Questo materiale include blog, canali YouTube e altro.
- Registrati su Kaggle - Kaggle è la casa dei data scientist e permette di muovere i primi step tra corsi, competizioni fittizie e reali e di conversare con persone di tutti i livelli sui forum
- Unisciti ai server Discord che più ti interessano. Basta fare una breve ricerca per trovare alcnui dei server dedicati al machine learning più importanti.
Questi step ti porteranno a poter affrontare qualsiasi progetto di data science o machine learning da principiante per lanciarti nel settore.
In particolare, il materiale educativo contiene risorse proprio orientate a farti muovere i primi step verso un progetto reale.
Conclusioni
Grazie per aver letto questo articolo. Spero ti abbia intrigato abbastanza da navigare di più lo spazio del machine learning!
Leggendolo, hai imparato alcuni dei concetti introduttivi alla disciplina e alcuni dei suoi algoritmi fondamentali ""tradizionali"" appartenenti alla famosa libreria Scikit-Learn.
Oltre a questi, ti ho esposto a tre dei modelli più rilevanti nello spazio che non fanno parter di sklearn quali XGBoost, LightGBM e CatBoost. Ognuno di questi ha vantaggi notevoli quando si vogliono addestrare modelli tradizionali allo stato dell'arte sui dati tabellari.
I prossimi step che ti ho suggerito sono quelli di continuare l'apprendimento navigando il materiale educativo, la pagina introduttiva agli argomenti trattati in questo blog oppure Kaggle, dove puoi unirti a migliaia di appassionati di data science e ML per competere su progetti reali oppure scambiare opinioni sul forum.
Buona fortuna nel tuo viaggio!
Commenti dalla community"
https://www.diariodiunanalista.it/posts/analisi-esplorativa-dei-dati-con-python-e-pandas/,"Questo articolo ha l'obiettivo di comunicarti cosa sia una analisi esplorativa e come farla praticamente con Python e Pandas.
L’analisi esplorativa del dato (exploratory data analysis, EDA) è di fondamentale importanza perché permette all’analista di conoscere a fondo il dataset sul quale lavora, stipulare o scartare ipotesi e creare dei modelli predittivi su basi solide.
Essa utilizza tecniche manipolazione dei dati e diversi concetti di statistica per comprendere con quali variabili lavoriamo, come esse sono relazionate tra di loro e come possono impattare sul business.
Infatti, è grazie alla EDA che possiamo porci delle domande che abbiano senso lato business.
In questo articolo mostrerò un template per l’analisi esplorativa che ho usato negli anni e che si è rivelata solida per molti progetti e domini.
Questa è messa in atto attraverso l’utilizzo della libreria Pandas - strumento essenziale per qualsiasi analista che lavora con Python.
Essa è composta da diversi step:
- Importazione di un dataset di lavoro
- Comprensione del quadro generale
- Preparazione
- Comprensione delle variabili
- Studio delle relazioni tra variabili
- Brainstorming
Questo template è frutto di tante iterazioni e mi permette di giungere velocemente al pormi delle domande sensate sui dati che ho di fronte.
Alla fine del processo, saremo in grado di consolidare un report per il business oppure di continuare con la fase di modellazione del dato.
Vediamo nell’immagine in basso come la fase di brainstorming sia collegata con quella della comprensione delle variabili e come questa sia a sua volta collegata di nuovo con quella del brainstorming.
Questo processo descrive come possiamo muoverci per formulare nuove domande finché non siamo soddisfatti.
Vedremo alcune delle funzionalità di Pandas più comuni e importanti e anche alcune tecniche per manipolare il dato in modo da comprenderlo a fondo.
Ti piace quello che stai leggendo?
Iscriviti al blog per rimanere sempre aggiornato sui contenuti di settore. No pubblicità. No spam.
Solo contenuto utile per la tua carriera nel ML e DS.
Puoi rimuovere l'iscrizione quando vuoi seguendo il link nella email.
Motivazione per il processo di analisi esplorativa
Ho scoperto con il tempo e con l’esperienza che una grossa fetta di aziende cercano insight e valore che provengono da attività fondamentalmente descrittive.
Questo significa che spesso e volentieri le aziende sono disposte ad allocare parecchie risorse per acquisire la consapevolezza necessaria del fenomeno che noi analisti stiamo andando a studiare.
La conoscenza di qualcosa.
Se siamo abili a investigare il dato e a porci le domande giuste, il processo EDA diventa estremamente potente.
Unendo abilità di data visualization, un analista abile è in grado di costruire una carriera solo facendo leva su queste abilità.
Non occorre nemmeno andare nella modellistica.
Un buon approccio alla EDA permette quindi di fornire valore aggiunto a molti contesti di business, soprattutto dove il nostro cliente/capo trova difficoltà di natura interpretative o di accesso al dato.
Questa è l’idea di base che mi ha portato a mettere giù un template del genere.
Ho scritto un thread su Twitter che mette nero su bianco il mio pensiero al riguardo
[Data Analysis] 🧵— Andrea D'Agostino (@theDrewDag) May 4, 2022
Exploratory data analysis is a fundamental step in any analysis work. You don't have to be a data scientist and be proficient at modeling to be a useful asset to your client if you can do great EDA.
Here's a template of a basic yet powerful EDA workflow👇
Librerie necessarie per l’analisi esplorativa
Prima di iniziare vediamo quali sono le librerie fondamentali richieste per svolgere l’EDA.
Ci sono molte librerie utili (alcune anche che fanno parte del lavoro in automatico) ma qui vedremo solo quelle che utilizza questo template
# manipolazione dati
import pandas as pd
import numpy as np
# visualizzazione
import matplotlib.pyplot as plt
from matplotlib import rcParams
import seaborn as sns
# applichiamo uno stile piacevole alla vista e settiamo i parametri di visualizzazione
plt.style.use(""ggplot"")
rcParams['figure.figsize'] = (12, 6)
# usiamo sklearn per caricare un dataset di esempio
from sklearn.datasets import load_wine
1. Importazione di un dataset di lavoro
La pipeline di analisi dati inizia con l’importazione o la creazione di un dataset di lavoro. La fase di analisi esplorativa inizia subito dopo.
Importare un dataset è semplice con Pandas attraverso delle funzioni dedicate alla lettura del dato. Se il nostro dataset è un file .csv, basta scrivere
df sta per dataframe, che è l’oggetto di Pandas simile ad un foglio Excel. Questa nomenclatura viene usata spesso. La funzione
read_csv prende come input il percorso del file che vogliamo leggere. Ci sono molti altri argomenti che possiamo specificare.
Il formato .csv non è l’unico che possiamo importare - ce ne sono di fatto molti altri come Excel, Parquet e Feather.
Per facilità, in questo esempio useremo Sklearn per importare il wine dataset. Questo dataset è molto usato nel settore per fini educativi e contiene informazioni sulla composizione chimica di vini per un task di classificazione.
Non useremo un .csv ma un dataset presente in Sklearn per creare il dataframe
# carichiamo il dataset
wine = load_wine()
# convertiamo il dataset in un dataframe Pandas
df = pd.DataFrame(data=wine.data, columns=wine.feature_names)
# creiamo la colonna per il target
df[""target""] = wine.target
Ora che abbiamo importato un dataset usabile, passiamo all’applicazione della pipeline di EDA.
2. Comprensione del quadro generale
In questa prima fase il nostro obiettivo è capire cosa abbiamo di fronte, ma senza andare nel dettaglio.
Cerchiamo di capire il problema che vogliamo andare a risolvere, andando a ragionare sull’intero dataset e sul significato delle variabili.
Questa fase può essere lenta e a volte anche noiosa, ma ci darà modo di farci una opinione del nostro dataset.
Prendiamo degli appunti
Solitamente apro Excel oppure un file di testo in VSCode per segnare queste informazioni:
- Variabile: nome della variabile
- Tipo: il tipo o formato della variabile. Questo può essere categoriale, numerica, booleana e così via
- Contesto: informazioni utili per comprendere lo spazio semantico della variabile. Nel caso del nostro dataset, il contesto è sempre quello chimico-fisico, quindi è facile. In un altro contesto, ad esempio quello dell’immobiliare, una variabile potrebbe appartenere ad un segmento particolare, come quello dell’anatomia del materiale oppure quello sociale (quanti vicini di casa ci sono?)
- Aspettativa: quanto è rilevante questa variabile rispetto al nostro task? Decliniamo la nostra opinione in “Alta, Media, Bassa”.
- Commenti: se abbiamo o meno dei commenti da fare sulla variabile
Di tutte queste, la colonna Aspettativa è una delle più importanti perché ci aiuta a sviluppare il “sesto senso” dell’analista - a mano a mano che accumuliamo esperienza sul campo riusciremo a mappare mentalmente quali variabili sono rilevanti e quali no.
In ogni caso, il punto di svolgere questa attività è che ci mette in condizione di fare delle riflessioni preliminari sui nostri dati, il che ci aiuta a iniziare il processo di analisi.
Proprietà e funzioni utili di Pandas
Faremo leva su diverse funzioni e proprietà di Pandas per comprendere il quadro generale. Vediamone alcune.
.head() & tail()
Due delle funzioni più usate in assoluto in Pandas sono
.head() e
.tail(). Queste due permettono di visualizzare un numero arbitrario di righe (di default 5) dall’inizio o dalla fine del dataset. Molto utili per accedere ad una piccola parte del dataframe in modo veloce.
.shape
Se applichiamo
.shape sul dataset, Pandas ci restituisce una coppia di numeri che rappresentano la dimensionalità del nostro dataset. Questa proprietà è molto utile per capire il numero di colonne e la lunghezza del dataset.
.describe()
La funzione
describe fa esattamente questo: fornisce delle informazioni puramente descrittive del dataset. Queste informazioni includono delle statistiche che riassumono la tendenza centrale della variabile, la loro dispersione, la presenza di valori vuoti e la loro forma.
.info()
A differenza di
.describe(),
.info() ci fornisce un sommario più breve del nostro dataset. Ci restituisce informazioni sul tipo di dato, valori non nulli e utilizzo di memoria.
Esistono anche
.dtypes e
.isna() che ci danno rispettivamente il tipo di dato e se il valore è nullo o meno. Usare
.info() però permette di accedere a queste informazioni in un singolo comando.
Qual è il nostro obiettivo?
Questa è una domanda importante che dobbiamo sempre porci. Nel nostro caso, vediamo come il target sia una variabile categoriale numerica che copre i valori di 0, 1 e 2. Questi numeri identificano il tipo di vino.
Se usiamo la documentazione di Sklearn su questo dataset vediamo sia stato costruito proprio per task di classificazione. Se volessimo fare modeling, l’idea sarebbe quindi di usare le caratteristiche del vino per predirne il tipo.
In un setting di analisi dati invece, vedremo come i diversi tipi di vino abbiano caratteristiche differenti e come queste sono distribuite.
3. Preparazione
In questa fase vogliamo iniziare a pulire il nostro dataset in modo da continuare l’analisi. Alcune delle domande che ci faremo sono
- esistono variabili inutili o ridondanti?
- ci sono delle colonne duplicate?
- la nomenclatura ha senso?
- ci sono delle nuove variabili che vogliamo creare?
Vediamo come applicare questi ragionamenti al nostro dataset.
Rispondendo ad ogni domanda qui, vediamo come
- Tutte le variabili sembrano essere misure chimico-fisiche. Questo significa che potrebbero essere tutte utili e contribuire a definire il segmentare il tipo di vino. Non abbiamo motivo di rimuovere delle colonne
- Per verificare la presenza di righe duplicate possiamo usare
.isduplicated().sum()- questo ci stamperà il numero di righe duplicated nel nostro dataset
- La nomenclatura può essere sicuramente ottimizzata. Ad esempio
od280/od315_of_diluted_winesè difficile da capire. Poiché indica una metodologia di ricerca che serve per capire la concentrazione di proteine nel liquido, la chiameremo
protein_concentration
df.rename(columns={""od280/od315_of_diluted_wines"": ""protein_concentration""}, inplace=True)
- Una delle metodiche più comuni di feature engineering è quella di creare delle nuove feature che siano la combinazione lineare / polinomiale di quelle esistenti. Questa diventa utile per fornire più informazioni ad un modello predittivo per migliorare le sue performance. Non lo faremo nel nostro caso
Essendo un dataset fantoccio, esso è praticamente già preparato per noi. Rimangono comunque dei punti utili da seguire per dataset più complessi.
4. Comprensione delle variabili
Mentre nel punto precedente siamo a descrivere il dataset per intero, ora cerchiamo di descrivere puntualmente tutte le variabili che ci interessano. Per questo motivo, questa fase può essere anche chiamata analisi univariata.
Variabili categoriali
In italiano conteggio_valori, è una delle funzioni più importanti per comprendere quanti valori di una determinata variabile ci sono nel nostro dataset. Prendiamo ad esempio la variabile target.
È possibile anche esprimere i dati in percentuale passando
normalize=True
A questa funzione possiamo aggiungere anche un grafico con
df.target.value_counts().plot(kind=""bar"")
plt.title(""Conteggio valori della variabile target"")
plt.xlabel(""Tipo di vino"")
plt.xticks(rotation=0)
plt.ylabel(""Conteggio"")
plt.show()
value_counts() può essere usata con qualsiasi variabile, ma funziona al meglio con variabili categoriali come il nostro target.
Questa funzione ci informa anche di quanto bilanciate siano le classi all’interno del dataset. Vediamo come la classe 2 sia inferiore alle altre due classi - nella fase di modeling forse possiamo implementare delle tecniche di bilanciamento del dato per non confondere il nostro modello.
Variabili numeriche
Se invece vogliamo analizzare una variabile numerica, possiamo descrivere la sua distribuzione con
describe() come abbiamo visto prima e possiamo visualizzarla con
.hist().
Prendiamo ad esempio la variabile magnesium
Prima
.describe()
e poi l’istogramma
Valutiamo anche la curtosi e asimmetria della distribuzione:
print(f""Curtosi: {df['magnesium'].kurt()}"")
print(f""Asimmetria: {df['magnesium'].skew()}"")
Da queste informazioni vediamo quindi come la distribuzione:
- non segua una curva normale
- mostri dei picchi
- abbia valori di curtosi e asimmetria maggiori di 1
Facciamo questo per ogni variabile, e avremo un quadro descrittivo pseudo completo del loro comportamento.
Questo lavoro ci serve per comprendere a fondo ogni variabile, e sblocca lo studio della relazione tra variabili.
4. Studio delle relazioni tra variabili
Ora l’idea è quella di trovare relazioni interessanti che mostrano influenza di una variabile sull’altra, preferibilmente sul target.
Questo lavoro sblocca le prime opzioni di intelligence - in un contesto lavorativo come il marketing digitale o nell’advertising online, queste informazioni offrono valore e possibilità di agire strategicamente.
Possiamo iniziare a esplorare le relazioni con l’aiuto di Seaborn e pairplot.
sns.pairplot(df)
Com’è possibile notare, pairplot visualizza in un diagramma a dispersione tutte le variabili una contro l’altra. È molto utile per cogliere le relazioni più importanti senza dover andare a spulciare ogni singola combinazione manualmente.
Attenzione però - è computazionalmente pesante da calcolare, quindi è più adatto per dataset con un numero di variabili relativamente basso come questo.
Analizziamo il pairplot partendo dal target
Il modo migliore per comprendere la relazione tra una variabile numerica e variabile categoriale è attraverso un boxplot. Creiamo un boxplot per alcohol, flavanoids, color_intensity e proline.
Perché queste variabili? Perché a occhio mostrano delle segmentazioni un po’ più marcate per un determinato tipo. Ad esempio, guardiamo proline vs target
sns.catplot(x=""target"", y=""proline"", data=df, kind=""box"", aspect=1.5)
plt.title(""Boxplot per target e proline"")
plt.show()
Vediamo infatti come la mediana di proline del vino tipo 0 sia più lata di quella degli altri due tipi. Che sia un fattore differenziante? Troppo presto per dirlo. Potrebbero esserci altre variabili da considerare. Vediamo flavanoids ora
Anche qui, il vino tipo 0 sembra avere dei valori più alti di flavanoidi. Possibile che i vini tipo 0 abbiano livelli combinati più alti di proline e flavanoids?
Con Seaborn è possibile creare un diagramma a dispersione e visualizzare a quale classe di vini appartiene un punto. Basta specificare il parametro
hue
sns.scatterplot(x=""proline"", y=""flavanoids"", hue=""target"", data=df, palette=""Dark2"", s=80)
plt.title(""Relazione tra proline, flavanoids e target"")
plt.show()
view raw
La nostra intuizione ci ha visto bene! I vini di tipo 0 mostrano pattern chiari di flavanoidi e prolina. In particolare, i livelli di prolina sono molto più alti mentre il livello di flavanoidi è stabile intorno al valore di 3.
Vediamo ora come Seaborn possa di nuovo aiutarci ad espandere la nostra esplorazione grazie alla mappa di calore (heatmap). Andremo a creare una matrice di correlazione con Pandas e a isolare le variabili più correlate tra loro
corrmat = df.corr()
hm = sns.heatmap(corrmat,
cbar=True,
annot=True,
square=True,
fmt='.2f',
annot_kws={'size': 10},
yticklabels=df.columns,
xticklabels=df.columns,
cmap=""Spectral_r"")
plt.show()
La mappa di calore è utile perché ci permette di cogliere efficientemente quali sono le variabili che sono correlate fortemente tra loro. Notiamo come al diminiure della variabile target (che va interpretato come una tendenza andare allo 0, quindi alla al tipo di vino 0) aumentino i flavanoidi, i fenoli totali, la prolina e le altre proteine. E viceversa.
Vediamo anche le relazioni tra le altre variabili, escludendo il target. Ad esempio c’è una correlazione molto forte tra alcol e prolina. Ad alti livelli di alcol corrispondono alti livelli di prolina.
Tiriamo le somme con la fase di brainstorming.
5. Brainstorming
Abbiamo raccolto molti dati a sostegno dell’ipotesi che il vino di classe 0 abbia una composizione chimica particolare.
Rimane ora isolare quali sono le condizioni che differenziano il tipo 1 dal tipo 2. Lascerò questo esercizio al lettore. A questo punto dell’analisi abbiamo diverse cose che possiamo fare:
- creare un report per gli stakeholder
- fare modeling
- continuare con l’esplorazione per chiarire ulteriormente delle domande di business
L’importanza di porsi le domande giuste
A prescindere dalla strada che prendiamo dopo l’EDA, porsi le domande giuste è quello separa un analista dati bravo da uno mediocre.
Possiamo conoscere lo strumento quanto vogliamo, ma questa abilità è inutile se non siamo in grado di recuperare informazioni dai dati.
Porsi le domande giuste permette all’analista di “essere in sync” con lo stakeholder, oppure di implementare un modello predittivo che funzioni veramente.
Ancora una volta, esorto il lettore interessato ad aprire il suo editor di testo preferito e a popolarlo di domande ogniqualvolta sorga il dubbio su qualcosa. Siate maniacali - se la risposta è nei dati, allora è compito nostro trovarla e comunicarla nel migliore dei modi.
Quale visualizzazione scegliere?
Pur non essendo un articolo incentrato sulla data visualization, l'analisi esplorativa si basa fortemente su questo argomento e abbiamo visto diversi grafici, ognuno efficace a modo suo a comunicare il rispettivo insight.
Se il lettore è interessato volerne sapere di più sull'argomento, consiglio fortemente il libro Storytelling with Data: A Data Visualization Guide for Business Professionals di Cole Nussbaumer Knaflic.
Storytelling with Data
A Data Visualization Guide for Business Professionals
Cole N. Knaflic
Un libro molto valido che insegna le basi della visualizzazione dei dati e anche concetti avanzati. Assolutamente consigliato a chi analizza e comunica dati per mestiere.
Un libro eccellente, che spiega nel minimo dettaglio ma senza mai stancare perché una visualizzazione è da preferire ad un altra, presentando prove e use case.
Leggi la mia recensione di Storytelling with Data qui 👇
Conclusione
Va specificato quanto il processo descritto finora sia di natura iterativa. Infatti, l’analisi esplorativa va avanti finché non abbiamo risposto a tutte le domande di business.
Per me è impossibile mostrare tutte le tecniche possibili di esplorazione del dato - mancano sia le richieste di business che il dataset. Tuttavia posso trasmettere al lettore l’importanza di applicare un template come il seguente per essere efficienti nell’analisi.
E tu hai una metodologia per le tue analisi esplorative? Che ne pensi di questa? Condividi il tuo pensiero con un commento 👇
Grazie per l’attenzione e a presto! 👋
Commenti dalla community"
https://www.diariodiunanalista.it/posts/bag-of-words-cosa-e-e-come-funziona/,"Il modello più comune per rappresentare numericamente del testo è il modello bag of words.
L'idea è molto semplice: ogni documento del nostro corpus viene rappresentato contando quante volte ogni parola appare in esso.
Questa rappresentazione numerica è la base per i modelli linguistici multinomiali e il modello vettoriali.
Prendiamo ad esempio due frasi inerenti al cinema prese da Wikiquote. Ogni frase qui viene considerato un documento estratto da un corpus di testi più grande.
Chiunque controlli il cinema, controlla il mezzo più potente di penetrazione delle masse!
Con un'espressione sintetica si può dire che il cinema scientifico ci ha permesso di ""vedere l'invisibile""
Rappresentiamo questi due testi in una matrice, in questo modo
Questa viene chiamata matrice documento-caratteristica (document-feature matrix): ogni riga rappresenta un diverso documento e ogni colonna definisce la caratteristica usata per rappresentare il documento.
Questa matrice offre una rappresentazione parsimoniosa dei testi processati che può essere di fatto molto al ricercatore.
Supponendo di non sapere nulla dei testi da processare, calcolare una document-feature matrix permette di estrarre facilmente l'argomento principale dei testi andando a sommare le colonne per ogni termine presente nel corpus.
cinema è la parola più comune e infatti i nostri testi sono proprio inerenti al cinema.
Questo, in soldoni, è il modello bag of words e sebbene questo sia abbastanza semplice alla base, permette di estrarre informazioni rilevanti da un corpus di testi come abbiamo appena visto.
L'algoritmo per modello bag of words
Ecco l'algoritmo per implementare da zero il modello bag of words:
- Scegliere l'unità di analisi
- Tokenizzare il testo
- Ridurre la complessità
- Creare la matrice documento-caratteristica
1. Scegliere l'unità di analisi
In questa fase il ricercatore deve decidere che unità logica analizzare col modello BoW. Spesso e volentieri si tratta del documento stesso, come abbiamo visto prima.
A volte però l'esigenza del ricercatore può essere diversa, e potrebbe aver bisogno di studiare un documento più corto o più lungo.
Ad esempio potrebbe essere utile usare come unità di analisi solamente il titolo oppure i primi paragrafi dell'articolo.
A volte invece potrebbe essere utile considerare come documento l'unione di diversi articoli, uno dopo l'altro.
Nell'esempio che ho usato sopra, un documento è una frase estratta da Wikiquote nell'ambito del cinema.
Sembra molto semplice come concetto, ma in realtà questa è una delle fasi più delicate che si estende anche ad altri modelli linguistici.
Non sempre abbiamo idea di come sia fatto il testo dalla quale vogliamo estrarre informazioni.
Questa considerazione apre importanti riflessioni sulla struttura del testo e sulla domanda di ricerca.
- Tra i nostri testi tipicamente ben strutturati ce ne sono alcuni che non hanno un titolo. Come ci comportiamo in questo caso?
- Stiamo processando dei testi fisici e scannerizziamo questi documenti per avere una rappresentazione digitale. Come trattiamo le porzioni di testo che si trovano nei pressi della cornice o nell'intestazione?
Queste sono solo alcune domande rilevanti che dovremmo porci quando vogliamo estrarre informazioni da un testo, ed esulano il singolo modello bag of words.
2. Tokenizzare il testo
Tokenizzare significa dividere il testo in unità discrete. Queste unità sono solitamente le parole che compongono un documento. Tipicamente viene effettuata una separazione a livello di spazio tra una parola e l'altra, ma esistono tokenizzatori che applicano regole di divisione anche più complesse.
Ogni singola parola viene chiamata token.
Tokenizzare ci permette di contare efficacemente le parole all'interno di un documento e creare la document-term matrix.
A volte la tokenizzazione può includere anche più di una parola, come nel caso degli n-grammi. Un n-gramma è un insieme di più parole che hanno senso. Ad esempio, un bigramma è l'unione di due token, mentre un trigramma è l'unione di tre token.
Un esempio di bigramma può essere l'unione tra due nomi propri di persona, primo e secondo nome, in modo da mantenere in una singola unità logica il nome dell'individuo.
Esempio: Mattia Lorenzo E. -> Tokenizzazione -> [Mattia, Lorenzo] -> Bigramma: mattia_lorenzo
La creazione di n-grammi avviene solitamente attraverso l'utilizzo di mezzi statistici che calcolano la probabilità di occorrenza del primo token con il secondo.
3. Ridurre la complessità
Teoricamente, una volta tokenizzati i nostri documenti, potremmo già passare alla creazione della matrice documento-caratteristica.
Tuttavia, è molto utile andare a ridurre la complessità delle unità logiche selezionate per aiutarci nel lavoro di ricerca e analisi. Il motivo è che andremmo a lavorare con un set di token molto grande e gran parte di questi risulterebbero rumorosi e non utili.
Prendiamo nuovamente come esempio questa frase
Chiunque controlli il cinema, controlla il mezzo più potente di penetrazione delle masse!
Se prestiamo attenzione alla matrice vediamo come alcuni termini non siano presenti (come gli articoli) e come tutte le parole siano in minuscolo.
Queste sono solo due tecniche di riduzione della complessità. Vediamone alcune delle più comuni.
- ridurre a minuscolo
- rimozione della punteggiatura
- rimozione delle stop word
- lemmatizzazione
- filtro frequenza
Ridurre a minuscolo
Ridurre a minuscolo rimuove le differenze tra token uguali ma scritti interpretati dalla macchina in modo diverso a causa della lettera maiuscola. È lo strato di riduzione della complessità sicuramente più semplice e facile da implementare.
Rimozione della punteggiatura
Un segno di punteggiatura come una virgola potrebbe essere inclusa come token e fare quindi parte del vocabolario. Vogliamo evitare questo scenario. Rimuovere la punteggiatura aiuta a ridurre drasticamente il vocabolario senza perdere significato.
Rimozione delle stop word
Una stop word è una parola bannata dal nostro vocabolario. Tipicamente queste parole includono articoli determinativi e indeterminativi, preposizioni e altro. Nella nostra lista di stop word possiamo anche inserire parole che non vogliamo nel nostro vocabolario per motivi di ricerca.
Lemmatizzazione
Anche se non lo abbiamo visto nell'esempio menzionato prima, la lemmatizzazione trasforma una token nel suo lemma. Un lemma è una unità linguistica che isola la base di una parola.
Prendendo l'esempio del verbo camminare, non vogliamo nel nostro vocabolario siano presenti camminavo, camminai, camminerò. Di fatto, si parla sempre del verbo camminare. Il lemma quindi sarà camminare.
Filtro frequenza
L'ultimo step diventa quello di rimuovere token che appaiono molto raramente (o, a volte, molto spesso) nel corpus di dati. Diventa difficile fare ragionamenti su un token che appare una sola volta nel corpus e in base al modello che usiamo possiamo andare a risparmiare parecchia potenza computazionale.
4. Creare la matrice documento-caratteristica
Dopo che abbiamo tokenizzato e ridotto la complessità siamo pronti a creare la matrice documento-caratteristica.
Questa sarà una matrice \( W = N \times J \) dove ogni elemento \( W_{ij} \) è il numero di volte che un token appare all'interno del corpus di dati.
La matrice è spesso sparsa - vuol dire che contiene molti 0. Questo perché ogni documento contiene solo una piccola parte dei token unici contenuti in tutto il corpus.
Limitazioni del modello bag of words
Il modello bag of words è il modello più basilare che c'è per rappresentare testo in formato numerico.
Il suo algoritmo è semplice e facile da comprendere, ma queste due caratteristiche positive vengono bilanciate da diverse limitazioni, quali
- difficoltà ad interpretare il contesto: avendo solo la frequenza di ogni token, diventa difficile comprendere il significato di tale token. Il contesto è una informazione importante che perdiamo con questo modello
- grossa richiesta computazionale per corpus molto grandi. Il fatto che la matrice documento-caratteristica sia sparsa di natura rende difficile lavorare con questo oggetto per corpus molto
Implementare il modello bag of words in Python
Implementare il modello bag of words in Python è molto semplice. Useremo
sklearn e il suo
CountVectorizer, che è la sua implementazione del BoW model.
Implementeremo anche tutti gli step che portano alla creazione del modello, quali tokenizzazione, riduzione della complessità e creazione della matrice...vedremo perché lavorare con
sklearn renda tutto più semplice 🙂
Iniziamo col definire il nostro corpus e le librerie da importare.
import pandas as pd
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
ita_stopwords = stopwords.words('italian')
corpus = [
'''Chiunque controlli il cinema, controlla il mezzo più potente di penetrazione delle masse!''',
'''Con un'espressione sintetica si può dire che il cinema scientifico ci ha permesso di ""vedere l'invisibile''',
]
Definiamo il modello
bow_model = CountVectorizer(
lowercase=True, # riduzione a minuscole
stop_words=ita_stopwords, # rimozione delle stop word italiane
analyzer='word', # identificazione delle unità logiche e tokenizzazione sullo spazio tra parole
ngram_range=(1, 2) # estrazione di unigrammi e bigrammi
)
Vediamo come gli argomenti passati al modello diano indicazioni su come vogliamo trattare gli step menzionati in questo articolo. Dichiariamo esplicitamente la conversione a minuscole, l'utilizzo di stopword con NLTK, il tipo di tokenizzazione e il range di n-grammi, che qui contiene unigrammi e bigrammi.
Applichiamolo sui nostri documenti
X = bow_model.fit_transform(corpus)
Usando
.get_feature_names_out() possiamo vedere gli elementi creati nella matrice finale. Notiamo gli unigrammi e bigrammi, l'assenza di punteggiatura, di stop word italiane e le parole in minuscolo.
Per ottenere una matrice documento-caratteristica formattata, useremo Pandas
bow_df = pd.DataFrame(X.todense(), columns=bow_model.get_feature_names_out())
bow_df
E abbiamo la nostra matrice, molto simile a quella prima creata in Excel a inizio articolo. La differenza qui è che abbiamo anche i bigrammi.
Conclusioni
Il modello Bag of Words è uno dei modelli più semplici e intuitivi per convertire testo in formato numerico.
Questa conversione permette di sfruttare l'informazione contenuta nella frequenza dei token, ma ha lo svantaggio di essere difficile da computare per grossi corpus di dati e che non ci sono informazioni relativi al contesto.
Nell'articolo Raggruppamento (clustering) di testi con TF-IDF utilizzo il modello TF-IDF, che nasce sempre dal BoW model, per applicare una tecnica di clustering chiamata KMeans. Questo modello è più complesso del bag of words, ma il clustering sarebbe stato possibile anche con quest'ultimo.
Il BoW model è un modello fondamentale, che permette la creazione di tanti altri modelli più complessi, come il TF-IDF o i modelli multinomiali, che tratteremo in articoli futuri.
Commenti dalla community"
https://www.diariodiunanalista.it/posts/benchmark-di-modelli-di-machine-learning/,"Una delle prime fasi del processo di data science è quella della selezione del modello.
Nella stragrande maggioranza dei casi, noi non sapremo quale sarà il modello più performante per il nostro set di dati.
Nasce quindi l'esigenza di poter iterare tra diversi modelli candidati e registrare le performance di ognuno in modo da poter scegliere quale addestrare e migliorare.
In questo articolo, vedremo insieme come utilizzare Python per confrontare e valutare le prestazioni dei modelli di apprendimento automatico.
Utilizzeremo la cross-validazione con Sklearn per testare i modelli e Matplotlib per visualizzare i risultati.
La motivazione principale per fare ciò è quella di avere una comprensione chiara e precisa delle prestazioni dei modelli e quindi di migliorare il processo di selezione del modello.
La cross-validazione è un metodo affidabile per testare i modelli su dati diversi dai dati di training. Ci consente di valutare le prestazioni del modello su dati che non sono stati utilizzati per allenare il modello stesso, il che ci fornisce una stima più precisa delle prestazioni del modello sui dati reali.
Se vuoi leggere di più su cosa è e come funziona la cross-validazione ti consiglio di cliccare sull'articolo qui in basso
Useremo un approccio orientato agli oggetti in modo da poterlo riutilizzare per altri progetti di machine learning facilmente. Infatti, questo metodo è altamente replicabile.
- a creare del codice Python per testare modelli predittivi con Sklearn usando OOP (object-oriented programming)
- a fare cross-validazione con Sklearn per valutare le performance del modello sul set di validazione
- a visualizzare i risultati con Matplotlib
La classe Benchmark
Per iniziare, creeremo una classe chiamata
Benchmark che avrà la responsabilità di testare i modelli. La classe accetterà un dizionario di modelli, dove la chiave sarà il nome del modello e il valore sarà l'oggetto del modello stesso.
La classe genererà anche i dati di prova utilizzando la funzione
make_classification di scikit-learn.
import numpy as np
from sklearn import model_selection
from sklearn import metrics
from sklearn import datasets
import matplotlib.pyplot as plt
class Benchmark:
""""""
Questa classe consente di confrontare e
valutare le prestazioni dei modelli di machine learning usando la cross-validation.
Parametri
----------
models : dict
Dizionario di modelli, dove la chiave è il nome del modello e il valore è l'oggetto del modello.
""""""
def __init__(self, models):
self.models = models
def test_models(self, X=None, y=None, cv=5):
""""""
Testa i modelli utilizzando i dati forniti e cross-validation.
Parametri
----------
X : array o DataFrame Pandas, shape (n_samples, n_features)
Feature per i dati di prova.
y : array o Serie Pandas, shape (n_samples,)
Target per i dati di training.
cv : int
Numero di fold nella cross-validation
Restituisce
-------
best_model : str
Nome del modello con il punteggio più alto.
""""""
if X is None or y is None:
X, y = datasets.make_classification(
n_samples=100,
n_features=10,
n_classes=2,
n_clusters_per_class=1,
random_state=0
)
self.results = {}
for name, model in self.models.items():
scores = model_selection.cross_val_score(model, X, y, cv=cv)
self.results[name] = scores.mean()
self.best_model = max(self.results, key=self.results.get)
return f""The best model is: {self.best_model} with a score of {self.results[self.best_model]:.3f}""
La funzione principale della classe sarà
test_models, che accetterà i dati di prova e utilizzerà la cross-validation per testare i modelli.
La funzione salverà i risultati in una variabile legata all'istanza e restituirà il modello con il punteggio più alto attraverso le varie iterazioni della cross-validation.
Per visualizzare i risultati, aggiungeremo una funzione chiamata
plot_cv_results alla classe.
Questa funzione utilizzerà la libreria Matplotlib per creare un grafico a barre che mostra il punteggio di validazione incrociata medio per ogni modello.
def plot_cv_results(self):
plt.figure(figsize=(15,5))
x = np.arange(len(self.results))
plt.bar(x, list(self.results.values()), align='center', color ='g')
plt.xticks(x, list(self.results.keys()))
plt.ylim([0, 1])
plt.ylabel('Cross-Validation Score')
plt.xlabel('Models')
plt.title('Model Comparison')
for index, value in enumerate(self.results.values()):
plt.text(index, value, str(round(value,2)))
plt.show()
Infine, per utilizzare la classe, istanzieremo l'oggetto
Benchmark passando il dizionario di modelli e chiamando la funzione
test_models con i dati di prova. Successivamente, utilizzeremo la funzione
plot_cv_results per visualizzare i risultati.
from sklearn import linear_model, ensemble
models = {
'logistic': linear_model.LogisticRegression(),
'randomforest': ensemble.RandomForestClassifier(),
'extratrees': ensemble.ExtraTreesClassifier(),
'gbm': ensemble.GradientBoostingClassifier()
}
benchmark = Benchmark(models)
print(benchmark.test_models())
benchmark.plot_cv_results()
E questa è la visualizzazione.
In questo modo, possiamo facilmente confrontare e valutare le prestazioni dei modelli e quindi scegliere il modello che offre le migliori prestazioni per il nostro problema specifico.
In questo esempio abbiamo utilizzato la funzione
make_classification per generare i dati di prova, ma naturalmente è possibile utilizzare qualsiasi dataset a proprio piacimento.
Inoltre, la classe
Benchmark può essere estesa per includere altre funzionalità, come ad esempio la possibilità di salvare i risultati in un file o di eseguire il test dei modelli su più dataset.
Quali sono i next step?
Seguendo la pipeline usuale del machine learning, il prossimo step sarà quello di fare un tuning degli iperparametri del modello migliore (in questo caso
ExtraTreesClassifier). Questo sempre se le nostre feature sono da considerarsi definitive.
Qualora non lo fossero, uno step intermedio sarebbe quello di fare feature selection / engineering, e ripetere lo step di benchmarking ogni volta che si cambiano tali feature.
Ti rimando ad un articolo sulla feature selection, che si sposa bene con l'articolo qui trattato della model selection
Conclusione
La classe Benchmark che abbiamo creato è solo un esempio di come è possibile implementare questa tecnica in un progetto, ma può essere facilmente adattato e personalizzato per soddisfare le esigenze specifiche del proprio progetto.
Il principale vantaggio di usare questo approccio è quello di automatizzare il processo di confronto e valutazione dei modelli, il che può risparmiare tempo e ridurre gli errori umani.
Template del codice
Ecco qui l'intera codebase
class Benchmark:
def __init__(self, models):
self.models = models
def test_models(self, X=None, y=None, cv=5):
if X is None or y is None:
X, y = datasets.make_classification(
n_samples=100,
n_features=10,
n_classes=2,
n_clusters_per_class=1,
random_state=0
)
self.results = {}
for name, model in self.models.items():
scores = model_selection.cross_val_score(model, X, y, cv=cv)
self.results[name] = scores.mean()
self.best_model = max(self.results, key=self.results.get)
return f""The best model is: {self.best_model} with a score of {self.results[self.best_model]:.3f}""
def plot_cv_results(self):
plt.figure(figsize=(15,5))
x = np.arange(len(self.results))
plt.bar(x, list(self.results.values()), align='center', color ='g')
plt.xticks(x, list(self.results.keys()))
plt.ylim([0, 1])
plt.ylabel('Cross-Validation Score')
plt.xlabel('Models')
plt.title('Model Comparison')
for index, value in enumerate(self.results.values()):
plt.text(index, value, str(round(value,2)))
plt.show()
from sklearn import linear_model, ensemble
models = {
'logistic': linear_model.LogisticRegression(),
'randomforest': ensemble.RandomForestClassifier(),
'extratrees': ensemble.ExtraTreesClassifier(),
'gbm': ensemble.GradientBoostingClassifier()
}
benchmark = Benchmark(models)
print(benchmark.test_models())
benchmark.plot_cv_results()
Commenti dalla community"
https://www.diariodiunanalista.it/posts/classificazione-binaria-di-immagini-con-tensorflow/,"In questo post vedremo come costruire un modello di classificazione binaria con TensorFlow per differenziare tra cani e gatti in immagini. Prendendo spunto da una famosa competizione su Kaggle e il relativo dataset, useremo questo compito per imparare come
- importare dal web un dataset compresso
- costruire un modello di classificazione con strati di convoluzione e max pooling
- creare un generatore di immagini con ImageDataGenerator per gestire efficacemente le immagini di addestramento e di validazione
- compilare e addestrare il modello
- visualizzare le trasformazioni applicate alle immagini nei vari strati della rete neurale
- fare delle previsioni su immagini mai viste prima
Poiché fare deep learning non è alla portata di qualsiasi PC di uso domestico, useremo Google Colab con runtime impostato su GPU.
Importare il dataset compresso dal web
Useremo un dataset ridotto di 3000 immagini di cani e gatti presi dal famoso dataset di Kaggle formato da 25000 immagini. Il dataset completo pesa più di 500 MB e caricarli/scaricarli su Colab può essere frustrante. Useremo questa versione ridotta che in ogni caso ci permetterà di testare efficacemente il nostro modello.
La URL al dataset è questa:
https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip
Usiamo il comando wget per scaricare il file compresso nel nostro file system:
import zipfile
import os
## --- DOWNLOAD DATASET
# scarichiamo il file
!wget --no-check-certificate https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip
# scompattiamo la cartella con zipfile
local_zip = './cats_and_dogs_filtered.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall()
zip_ref.close()
## --- DOWNLOAD DATASET
## -- DEFINIZIONE VARIABILI
# dichiariamo la posizione dei nostri file di addestramento e validazione
base_dir = 'cats_and_dogs_filtered'
train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'validation')
# puntiamo alle cartelle di gatti e cani per il training
train_cats_dir = os.path.join(train_dir, 'cats')
train_dogs_dir = os.path.join(train_dir, 'dogs')
# puntiamo alle cartelle di gatti e cani per la validazione
validation_cats_dir = os.path.join(validation_dir, 'cats')
validation_dogs_dir = os.path.join(validation_dir, 'dogs')
Usando il comando wget e i pacchetti os e zipfile siamo in grado di scaricare e organizzare i nostri file di addestramento in maniera efficiente. Abbiamo ora un modo per puntare ai nostri file con delle variabili specifiche, che useremo in ImageDataGenerator di TensorFlow.
Diamo una occhiata ad un set di immagini così da farci una idea su quello che andremo a classificare
Vediamo come le immagini siano molto diverse tra loro e come a volte siano presenti anche entità estranee come esseri umani o altri oggetti. Costruiremo un modello di deep learning in grado di differenziare efficacemente tra cani e gatti nonostante questi elementi non inerenti.
Se usiamo
len(os.listdir(train_cats_dir))per contare il numero di immagini nelle varie cartelle vediamo che il loro numero ammonta a 3000, con 1000 immagini di cani e di gatti nel training e 500 immagini rispettivamente per la validazione.
Breve introduzione a convoluzioni e pooling
Nel modello che vedremo a breve useremo strati di convoluzioni e di max pooling. Entrambi gli strati sono ampiamente usati in compiti di computer vision per via delle trasformazioni che applicano sulla immagine in input e beneficiano la rete neurale perché la aiutano nell'identificazione dei pattern andando a enfatizzare delle caratteristiche essenziali presenti in esse.
Convoluzione
Una convoluzione è essenzialmente un filtro che viene applicato ad una immagine. Ragionando a livello di pixel, che sono le entità che andremo a trasformare quando si parla di immagini, una convoluzione guarda il suo valore e quelli dei suoi pixel vicini e trasforma il pixel target usando una griglia di valori mappati ad ogni pixel considerato.
Se per esempio usiamo una griglia 3x3, allora considereremo tutti i pixel vicini del nostro pixel target. Quando applichiamo la convoluzione, il pixel target viene trasformato e assume il valore corrispondente alla moltiplicazione del valore originale di ogni pixel considerato e del rispettivo valore nella griglia di convoluzione. Il valore finale corrisponde alla somma di ogni prodotto. Vediamo con una immagine.
Considerando il pixel target con valore 192, allora una convoluzione applicato su di esso considererà tutti i pixel intorno ad esso come vicini e il suo nuovo valore sarà il seguente:
L'idea alla base di una convoluzione è quella di far emergere alcune caratteristiche di una immagine, come ad esempio rendere bordi e contorni più salienti rispetto al background.
Le convoluzioni vengono spesso accompagnate dal pooling (in italiano, aggregazione, compressione), che permette alla rete neurale di comprimere l'immagine e estrarre gli elementi davvero salienti della stessa.
In TensorFlow, un tipico strato di convoluzione viene applicato con
tf.keras.layers.Conv2D(filters, kernel_size, activation, **kwargs). In filters inseriremo il numero di filtri di convoluzione da applicare, invece con kernel_size indicheremo la grandezza della griglia. Con activation specificheremo invece la funzione di attivazione. I parametri sono molti e consiglio al lettore di studiare meglio il materiale sulla documentazione ufficiale di TensorFlow.
Pooling
Fare pooling vuol dire applicare una compressione all'immagine. Se per esempio volessimo applicare uno strato di pooling 2D con TensorFlow, questo significherebbe prendere il pixel di riferimento, quello sotto di esso e i due al suo lato sinistro, in modo da formare una griglia di quattro valori. Di questi valori si conserva solo il valore più grande.
Guardando attentamente questa immagine vediamo come fare pooling riduce una immagine di 16 pixel in una da 4, andando proprio a prendere i pixel dal valore più grande in blocchi di 4 e ripetendo il processo.
Questo meccanismo viene applicato dopo la convoluzione, andando così a preservare le caratteristiche messe in risalto dalla stessa e amplificando ancora di più questo effetto. Il pooling riduce anche la dimensione dell'immagine, velocizzando quindi l'addestramento negli strati più avanzati di una rete neurale.
Il pooling è solitamente applicato applicato prendendo il valore massimo, ma ci sono anche altre logiche, come ad esempio quelle basate sulla media e somma.
In TensorFlow, un tipico strato di pooling viene applicato con
tf.keras.layers.MaxPooling2D(pool_size, **kwargs). In
pool_size inseriremo la grandezza della griglia. I parametri sono molti e consiglio al lettore di studiare meglio il materiale sulla documentazione ufficiale di Keras.
Creazione del modello con TensorFlow
Ora che è un po' più chiaro cosa siano convoluzione e aggregazione, procediamo con la creazione di un modello di classificazione binaria con TensorFlow in grado di sfruttare le caratteristiche che rendono cani e gatti identificabili. Useremo l'API sequenziale di TensorFlow perché è facile da comprendere e da implementare.
Una nota sulla input_shape
È importante notare che dovremmo fornire al modello immagini dalle dimensioni uniformi. Questa dimensione è arbitraria e per questo modello useremo una dimensione di 150x150 pixel. Ogni immagine verrà quindi ridimensionata da TensorFlow in modo da essere quadrata.
Poiché stiamo usando immagini a colori, dovremmo anche fornire questa informazione. La input_shape sarà quindi (150, 150, 3), dove 3 sta proprio per i tre bit di informazione che codificano il colore. Vedremo tra poco come assicurarci che le nostre immagini siano di questa dimensione quando sfrutteremo ImageDataGenerator.
Vediamo come implementare l'architettura della rete neurale.
import tensorflow as tf
model = tf.keras.models.Sequential([
# poiché Conv2D è il primo strato della rete neurale, dovremmo specificare anche la dimensione dell'input
tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),
# applichiamo uno strato di aggregazione 2D
tf.keras.layers.MaxPooling2D(2,2),
# e ripetiamo il processo
tf.keras.layers.Conv2D(32, (3,3), activation='relu'),
tf.keras.layers.MaxPooling2D(2,2),
tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
tf.keras.layers.MaxPooling2D(2,2),
# appiattiamo il risultato per fornirlo allo strato denso per la classificazione
tf.keras.layers.Flatten(),
# e definiamo 512 neuroni per l'elaborazione dell'output processato dagli strati precedenti
tf.keras.layers.Dense(512, activation='relu'),
# un singolo neurone di output. Il risultato sarà 0 se l'immagine è un gatto, 1 se è un cane
tf.keras.layers.Dense(1, activation='sigmoid')
])
Come menzionato, convoluzioni e aggregazioni vanno spesso insieme. Il loro numero però è arbitrario e va testato dall'analista. Magari aumentando o diminuendo questo numero di strati la performance aumenta. L'unico modo per capirlo è di sperimentare.
L'output dell'ultimo neurone viene infine sottoposto alla funzione di attivazione sigmoide che restituisce 0 oppure 1.
Utilizziamo ora
model.summary() per comprendere come il dato viene trasformato dalla rete neurale e come questo venga convertito in una classe binaria.
Vediamo come, a cascata, la nostra immagine venga ridotta dalla convoluzione e successivamente compressa ulteriormente dal pooling. Dobbiamo porre particolare attenzione alla colonna Output Shape, in quanto ci mostra proprio il percorso del dato nella rete. Vediamo come nel primo strato conv2d l'output shape sia 148, 148, 64.
Analizziamo un attimo meglio questa informazione. Come mai se le nostre immagini sono 150x150, la rete neurale prende in input una immagine 148x148? La risposta è perché la convoluzione che stiamo usando utilizza una griglia 3x3. I primi pixel intorno all'immagine non hanno dei pixel vicini per permettere la sovrapposizione del filtro. Viene quindi rimosso un pixel sull'asse X e Y, riducendo il margine dell'immagine proprio di 1 pixel. Il 64 sta per il numero di convoluzioni applicati all'immagine.
In seguito alla prima convoluzione vediamo come lo strato di max pooling vada a ridurre la dimensione dell'immagine, riducendolo esattamente della metà. Il processo continua fino a che non arriviamo allo strato flatten, che prende l'output arrivato a quel punto e lo appiattisce in un singolo vettore.
Questo viene fornito ad uno strato denso di 512 neuroni e poi si arriva alla fine della rete con l'output singolo, 0 oppure 1.
Per dire a TensorFlow che l'architettura del modello è conclusa dobbiamo usare il comando
compile. Useremo l'ottimizzatore Adam, una loss function di crossentropia binaria e l'accuratezza come metrica di performance.
model.compile(optimizer=""adam"",
loss='binary_crossentropy',
metrics = ['accuracy'])
Procediamo ora con lo scrivere la pipeline di pre-processing delle immagini da fornire al modello.
Preprocessing e consegna delle immagini al modello
Il prossimo step è quello di fare preprocessing sulle immagini per assicuraci che siano adatte al nostro modello. Esse verranno ridimensionate a prescindere dalla dimensione originale, convertite in float64 e associate alla loro etichetta (cane o gatto). Queste informazioni verranno poi consegnate al modello.
Creeremo due generatori: uno per l'addestramento e uno per la validazione. Ognuno di questi, inoltre, convertirà le immagini in valori numerici normalizzati tra 0 e 255. 255 è il valore massimo di un pixel, quindi un pixel di intensità 255 diventerà 1 mentre un pixel ""spento"" sarà 0 e ogni valore intermedio sarà proprio compreso tra 0 e 1.
In TensorFlow tutto questo viene fatto con
ImageDataGenerator. Una delle particolarità che rende ImageDataGenerator così potente è che genera etichette per le nostre immagini automaticamente, basandosi sulla gerarchia e nomenclatura delle cartelle che contengono le immagini.
Vediamo come implementare i generatori in Python. Questi ora verranno usati per addestrare il modello, ma non dovremmo preoccuparci di riscalare manualmente le immagini o di fare labeling. Il lavoro sporco lo fa tutto TensorFlow ;)
from tensorflow.keras.preprocessing.image import ImageDataGenerator
# riscaliamo tutte le nostre immagini con il parametro rescale
train_datagen = ImageDataGenerator(rescale = 1.0/255)
test_datagen = ImageDataGenerator(rescale = 1.0/255)
# utilizziamo flow_from_directory per creare un generatore per il training
train_generator = train_datagen.flow_from_directory(train_dir,
batch_size=20,
class_mode='binary',
target_size=(150, 150))
# utilizziamo flow_from_directory per creare un generatore per la validazione
validation_generator = test_datagen.flow_from_directory(validation_dir,
batch_size=20,
class_mode='binary',
target_size=(150, 150))
Addestramento del modello
Addestreremo il modello su 2000 immagini e lo valideremo su 1000. Faremo questo per 15 epoche.
history = model.fit(
train_generator, # passiamo il generatore per il training
steps_per_epoch=100,
epochs=15,
validation_data=validation_generator, # passiamo il generatore per la validazione
validation_steps=50,
verbose=2
)
Steps_per_epoch denota il numero di batches da selezionare per un'epoca. Se vengono selezionati 500 steps, la rete userà 500 batch per completare un'epoca. Vediamo le performance del modello durante il training.
Vediamo come l'accuracy del nostro modello sia intorno al 71% sul set di validazione. Non male ma nemmeno benissimo - su un dataset così piccolo 71% è soddisfacente a mio avviso! Aumentare il numero di immagini darebbe sicuramente risultati più solidi.
Visualizzare le rappresentazioni neurali
Una delle cose più interessanti è vedere come una rete neurale convoluzionale estragga le informazioni salienti dalle immagini e le rappresenti mentre passano tra i vari strati. Useremo un modello di Keras per fare ciò, e gli passeremo gli input del modello convoluzionale addestrato precedentemente.
Questa porzione di codice è un po' avanzata, quindi sentitevi liberi di saltarla oppure eseguirla meramente per l'output (che è molto interessante!)
import numpy as np
import random
from tensorflow.keras.preprocessing.image import img_to_array, load_img
# definiamo un nuovo modello custom di Keras che riceve una immagine in input
# e restituisce le rappresentazioni degli strati del modello precedente
successive_outputs = [layer.output for layer in model.layers]
visualization_model = tf.keras.models.Model(inputs=model.input, outputs=successive_outputs)
# prepariamo una immagine a caso dal nostro dataset
cat_img_files = [os.path.join(train_cats_dir, f) for f in train_cat_fnames]
dog_img_files = [os.path.join(train_dogs_dir, f) for f in train_dog_fnames]
img_path = random.choice(cat_img_files + dog_img_files)
img = load_img(img_path, target_size=(150, 150)) # questa è una immagine grezza in formato PIL
x = img_to_array(img) # array numpy con dimensione (150, 150, 3)
x = x.reshape((1,) + x.shape) # array numpy con dimensione 1, 150, 150, 3)
# normalizziamo i valori dei pixel per 1/255
x /= 255.0
# facendo una previsione non facciamo altro che ottenere
# le rappresentazioni ""intermedie"" di questa immagine dal modello precedente
successive_feature_maps = visualization_model.predict(x)
# mappiamo gli strati di questo modello con il loro nome
layer_names = [layer.name for layer in model.layers]
# plottiamo il tutto
for layer_name, feature_map in zip(layer_names, successive_feature_maps):
if len(feature_map.shape) == 4: # se è uno strato convoluzionale o di pooling
n_features = feature_map.shape[-1] # numero di feature
size = feature_map.shape[ 1] # dimensione
# creiamo una griglia per visualizzare i dati
display_grid = np.zeros((size, size * n_features))
# un po' di post processing per capirci qualcosa
for i in range(n_features):
x = feature_map[0, :, :, i]
x -= x.mean()
x /= x.std ()
x *= 64
x += 128
x = np.clip(x, 0, 255).astype('uint8')
display_grid[:, i * size : (i + 1) * size] = x
# mostriamo il grafico
scale = 20. / n_features
plt.figure( figsize=(scale * n_features, scale) )
plt.title ( layer_name )
plt.grid ( False )
plt.imshow( display_grid, aspect='auto', cmap='viridis' )
Ecco il risultato
Vediamo come le feature più salienti vengano passate di strato in strato e che rendono tale il cane preso in esempio. Vediamo come spiccano le orecchie, gli occhi e il muso. Queste feature vengono mantenute attraverso tutte (o quasi) le rappresentazioni negli strati e servono a far comprendere alla rete neurale com'è fatto un cane. Molto interessante!
Questa tecnica di visualizzare le rappresentazioni della rete neurale è utile perché ci aiuta a comprendere cosa mettono in risalto le convoluzioni e le aggregazioni. Se ci sono cose che non vanno questo è il primo luogo dove andare a guardare. Ad esempio, la rete potrebbe mettere in risalto feature non inerenti che la portano a sbagliare la predizione. In questo caso una analisi manuale è d'obbligo e dovremmo agire sull'architettura della rete.
Valutazione del modello
Prima di passare alla previsione di immagini nuove, vediamo come scrivere il codice che permette mostrare su grafico l'andamento di loss e di accuracy nel set di addestramento e di validazione.
# recuperiamo le metriche che ci interessano da history
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(len(acc))
# plottiamo la accuracy con matplotlib
plt.plot(epochs, acc)
plt.plot(epochs, val_acc)
plt.title('Accuracy in training e validazione')
plt.figure()
# plottiamo la loss con matplotlib
plt.plot(epochs, loss)
plt.plot(epochs, val_loss)
plt.title('Loss in training e validazione')
Ecco i risultati - si nota dell'overfitting nel training set. Questo è dovuto alle dimensioni ridotte del dataset, come menzionato. Si nota come nel training la accuracy raggiunga velocemente, già dopo la seconda epoca, una accuracy tra il 95-99%. L'overfitting si verifica quando un modello esposto a un numero insufficiente di esempi apprende pattern che non si generalizzano a nuovi dati, ovvero quando il modello inizia a utilizzare feature irrilevanti per fare previsioni.
L'overfitting è IL problema numero uno nel machine learning, ed è un termine che leggerete parecchie volte in questo blog. Da analisti, il nostro primo obiettivo è quello di evitare l'overfitting e di rendere un modello quanto più generalizzabile possibile.
Il lettore interessato può leggere di più su overfitting e perché è uno degli ostacoli più importanti nel machine learning.
Predizioni su immagini nuove
Siamo arrivati alla conclusione di questo articolo. Grazie per la tua attenzione! Ricordati di lasciare un commento o di condividere questo post con un collega se hai voglia :) Vediamo ora come caricare una immagine su Colab e usarla per effettuare una classificazione usando il nostro modello predittivo.
Useremo questa immagine di un cucciolo di Labrador per testare il modello.
Ecco il codice
import numpy as np
from google.colab import files
from keras.preprocessing import image
uploaded = files.upload()
for fn in uploaded.keys():
# predizione di una immagine caricata
path = '/content/' + fn # carichiamo l'immagine su Colab
img = image.load_img(path, target_size=(150, 150)) # e usiamo load_img per scalarla alla dimensione target
# scaliamo i valori
x = image.img_to_array(img)
x /= 255
x = np.expand_dims(x, axis=0)
# appiattiamo l'output
images = np.vstack([x])
# eseguiamo la predizione
classes = model.predict(images, batch_size=10)
print(classes[0])
if classes[0] > 0.5:
print(fn + "" è un cane!"")
else:
print(fn + "" è un gatto!"")
E infine ecco la predizione corretta del nostro modello!
Commenti dalla community"
https://www.diariodiunanalista.it/posts/clustering-di-serie-temporali-per-la-previsione-del-mercato-azionario-in-python-parte-1/,"Se sei un analista, indipendentemente dal tuo settore lavorativo, sei sicuramente consapevole che esistono molti approcci diversi per la previsione del mercato azionario. I più rilevanti oggi utilizzano principalmente reti neurali applicate a dati in sequenza, come LSTM, o coinvolgono l'analisi del sentiment e la signal detection.
Questi approcci sono decisamente interessanti e per una buona ragione: ci sono molte risorse su Internet che trattano gli argomenti menzionati. Questa è una ottima cosa, poiché gli possiamo avviare i nostri progetti seguendo una linea di base condivisa e poi espanderli con le proprie metodologie.
Oggi voglio proporre un approccio diverso, quello che ho letteralmente sognato un paio di settimane fa e che mi ha fatto riflettere per le possibili opportunità. Sto parlando del clustering di serie temporali applicato ai mercati azionari.
Il mio pensiero principale è stato
E se potessi identificare pattern nei dati, trovare quelli simili, raggrupparli insieme e assegnare opportunità di investimento a ciascuno di essi?
Questo mi permetterebbe di individuare sequenze rilevanti che potrebbero prevedere una tendenza specifica verso l'alto o verso il basso.
Ho dedicato alcune ore a questo progetto e vorrei condividere con voi la prima parte di questa serie. Mentre continuo a lavorare su questo progetto, condividerò le mie scoperte in un altro articolo.
Tieni presente che questo è il mio approccio: se ti piace il ragionamento generale ma faresti qualcosa di diverso, fammelo sapere con un commento 👌
Spero che tu sia entusiasta come lo sono io per questo progetto. Iniziamo!
L'intuizione alla base del progetto
L'idea è che se prendiamo una serie temporale, indipendentemente dalla sua natura e contesto (che si tratti di un titolo azionario o altro), possiamo dividerla in sequenze e studiare ogni sequenza in isolamento.
Questo ci permette di fare ipotesi su questi segmenti e confrontarli l'uno contro l'altro.
In tal proposito, questa è l'ipotesi per questo problema
Se divido una serie temporale in sequenze di egual misura, posso dividere i tali sequenze in due parti, che chiamerò A e B. B quindi seguirà sempre A.
Studiando A, sono in grado di trovare dei pattern in comune e raggruppare tali A insieme. Questo, di conseguenza, mi permette di calcolare la probabilità che la B segua un andamento simile dato il segmento che la precede A.
L'approccio probabilistico mi sembra quello più adatto. Vedremo tra poco perché.
In conclusione, voglio trovare sequenze che occorrono con un certo grado di somiglianza nell'intera serie temporale. Voglio raggruppare queste sequenze simili e vedere come si comporta la sequenza che segue in termini di trend, che viene calcolato usando la pendenza.
Ecco un diagramma di come ho immaginato di risolvere questo problema
Se scopriamo che alcuni A condividono un alto grado di somiglianza, possiamo raggrupparli insieme e trovare la probabilità che B abbia un trend verso l'alto o verso il basso contando quanti B hanno una pendenza positiva o negativa.
L'algoritmo
Data la spiegazione sopra, ecco come ho progettato l'algoritmo
- Dividere le serie temporali in sequenze di lunghezza N
- Dividere ogni sequenza in due parti in base alla dimensione K per creare sequenze A e B
- Calcolare la somiglianza S tra tutte le sequenze di A
- Raggruppare tutte le sequenze simili di A in base a una soglia
- Per ogni gruppo G calcolare la probabilità P di un trend positivo, negativo o stabile nella sequenza B
- In una nuova serie temporale, identificare una sequenza simile ad una A precedentemente trovata per ottenere la probabilità della successiva porzione B
Se questi passaggi non sono perfettamente chiari, non preoccuparti. Spiegherò ogni passaggio con un'argomentazione dettagliata e con il codice.
Requisiti
Scriveremo la logica da zero in Python, quindi faremo affidamento solo su una manciata di librerie.
# data manipulation
import pandas as pd
import numpy as np
# viz
import matplotlib.pyplot as plt
import seaborn as sns
# time and date libs
import datetime
# stock data access
import pandas_datareader as pdr
L'unica menzione che vale la pena fare qui è che useremo
pandas_datareader come fornitore di dati del mercato azionario.
La serie temporale
Utilizzeremo
pandas_datareader per inserire i dati sulle azioni su Apple di 1000 giorni fa. Ciò includerà 689 punti dati. Questo perché i fine settimana e le altre festività sono escluse.
Non c'è un motivo particolare per cui ho scelto questo numero, sembravano solo dati sufficienti. Definiamo una funzione per reperire i dati
def get_data(ticker: str, start_date: datetime, end_date: datetime) -> pd.DataFrame:
""""""
Get stock data input ticker
""""""
data = pdr.get_data_yahoo(ticker, start=start_date, end=end_date)
return data
# get 1000 days of data for Apple starting from today
start_date = datetime.datetime.now() - datetime.timedelta(days=1000)
end_date = datetime.datetime.now()
data = get_data('AAPL', start_date=start_date, end_date=end_date)
data.head()
Useremo la colonna di Close come nostra serie temporale.
Procediamo ora con la suddivisione della serie temporale secondo la logica sopra citata.
Divisione delle serie temporali
Definiremo due funzioni che divideranno le serie temporali prima in sequenze e poi in sequenze ""interne"", A e B. Ecco un esempio:
Supponiamo di avere un array Numpy di 20 numeri
s = np.array([i for i in range(30)])
Vogliamo prendere questa serie e dividerla in N segmenti di uguale dimensione. Possiamo raggiungere questo obiettivo in questo modo
def split_time_series(series, n):
""""""
Split a time series into n segments of equal size
""""""
split_series = [series[i:i+n] for i in range(0, len(series), n)]
# if the last sequence is smaller than n, we discard it
if len(split_series[-1]) < n:
split_series = split_series[:-1]
return np.array(split_series)
Se applichiamo questa funzione a s con n = 6 otteniamo una sequenza divisa completa senza elementi scartati (perché non c'è resto quando si divide 30 per 6).
Ora dobbiamo dividere ogni sequenza in due parti in base alla dimensione K per creare sequenze A e B. Definiamo due nuove funzioni chiamate
split_sequence e
split_sequences per questo.
def split_sequence(sequence, k):
""""""
Split a sequence in two, where k is the size of the first sequence
""""""
return np.array(sequence[:int(len(sequence) * k)]), np.array(sequence[int(len(sequence) * k):])
def split_sequences(sequences, k=0.80):
""""""
Applies split_sequence on all elements of a list or array
""""""
return [split_sequence(sequence, k) for sequence in sequences]
view raw
In
split_sequence, il parametro K ci permette di controllare la dimensione della prima sequenza. Simile al parametro
test_size in train_test_split di Sklearn.
split_sequences invece è solo responsabile dell'applicazione della logica a un array.
Applichiamolo al nostro esempio per capire come funziona
Le sequenze A e B sono definite così
[seq[0] per seq in sss], [seq[1] per seq in sss]
quindi gli elementi accoppiati dal primo e dal secondo elenco nell'array Numpy.
Applichiamo tutto questo alle nostre serie temporali.
N = 15 # window size --> possiamo modificare questo parametro per sperimentare
K = 0.70 # split size --> 70% dei dati è in A, 30% in B
SEQS = split_time_series(list(data['Close'].values), N) # crea sequenze di lunghezza N
SPLIT_SEQS = split_sequences(SEQS, K) # divide le sequenze in due
A = [seq[0] for seq in SPLIT_SEQS]
B = [seq[1] for seq in SPLIT_SEQS]
Se diamo un'occhiata a A[0] e B[0], vediamo la prima parte dei dati di chiusura di Apple, a dimostrazione del fatto che la preelaborazione ha avuto successo.
Ora siamo pronti per confrontare le sequenze per similarità.
Calcolo della similarità
Questo è il terzo passaggio dell'algoritmo. Per calcolare la similarità, utilizzeremo una combinazione di correlazione di Pearson e dynamic time warping (distorsione temporale dinamica). La correlazione ci aiuta a far corrispondere la direzione generale delle tendenze, mentre DTW ci aiuta a calcolare la distanza tra i punti. Ci sono molte risorse online su questi due argomenti e ti suggerisco di fare una ricerca più mirata se sei interessato.
La formula è la seguente ed è un prodotto ponderato di correlazione e DWT
\( DWT * (1 — correlation) \)
Anche se forse non ottimale (aiutami a trovare una soluzione migliore!), sembra essere una soluzione funzionale, come vedrai brevemente.
L'idea qui è quella di creare una matrice di similarità S per raccogliere i punteggi per coppie per tutte le sequenze appartenenti ad A. Ricorda che vogliamo trovare elementi simili in A per calcolare la probabilità di trovare una certa B che seguono una certa tendenza.
Gli elementi che condividono una similarità al di sopra di una certa soglia arbitraria verranno raggruppati insieme. Lo vedremo dopo, però. Per ora, scriviamo il codice per il calcolo della similarità.
def compute_correlation(a1, a2):
""""""
Calculate the correlation between two vectors
""""""
return np.corrcoef(a1, a2)[0, 1]
def compute_dynamic_time_warping(a1, a2):
""""""
Compute the dynamic time warping between two sequences
""""""
DTW = {}
for i in range(len(a1)):
DTW[(i, -1)] = float('inf')
for i in range(len(a2)):
DTW[(-1, i)] = float('inf')
DTW[(-1, -1)] = 0
for i in range(len(a1)):
for j in range(len(a2)):
dist = (a1[i]-a2[j])**2
DTW[(i, j)] = dist + min(DTW[(i-1, j)], DTW[(i, j-1)], DTW[(i-1, j-1)])
return np.sqrt(DTW[len(a1)-1, len(a2)-1])
# create empty matrix
S = np.zeros((len(A), len(A)))
# populate S
for i in range(len(A)):
for j in range(len(A)):
# weigh the dynamic time warping with the correlation
S[i, j] = compute_dynamic_time_warping(A[i], A[j]) * (1 - compute_correlation(A[i], A[j]))
Per chiarezza, stampiamo solo le prime due righe della matrice
Come possiamo vedere, serie identiche hanno un punteggio di 0. Pertanto, sequenze simili tendono ad avvicinarsi a 0 man mano che diventano sempre più simili. Tracciamo questo con una heatmap.
# plot heatmap of S
fig, ax = plt.subplots(figsize=(20, 10))
sns.heatmap(S, cmap='nipy_spectral_r', square=True, ax=ax)
plt.title(""Heatmap of sequence similarities"", fontsize=20, fontweight='bold')
plt.xticks(range(len(A)), range(len(A)))
plt.yticks(range(len(A)), range(len(A)))
plt.show()
La mappa di calore rivela un'informazione importante: le sequenze più simili si trovano principalmente all'inizio e alla fine della nostra serie temporale.
Non ho idea del perché succeda questa cosa: sarei interessato a sentire i tuoi pensieri al riguardo. Andiamo avanti.
Raggruppamento delle sequenze simili
Il passaggio successivo consiste nel raggruppare insieme sequenze simili in A. L'idea qui è semplice: se le sequenze condividono un punteggio di similarità al di sotto di una certa soglia le raggruppiamo insieme in un dizionario chiamato G. G conterrà tutti i cluster significativi per questo progetto.
# populate G
G = {}
THRESHOLD = 6 # arbitrary value - tweak this to get different results
for i in range(len(S)):
G[i] = []
for j in range(len(S)):
if S[i, j] < THRESHOLD and i != j and (i, j) not in G and (j, i) not in G and j not in G[i]:
G[i].append(j)
# remove any empty groups
G = {k: v for k, v in G.items() if v}
Una nota sulla soglia
threshold: ho utilizzato il valore di 6 andando per tentativi, poiché sembra produrre i migliori risultati in termini di raggruppamento di similarità (controlla sotto). Potresti voler utilizzare altri valori: sperimenta e fammi sapere come funziona per te.
Vediamo come appare G stampando una parte del dizionario.
Bene ma non benissimo! Un dizionario in realtà non trasmette le informazioni in modo efficace. Vediamo che alcune delle sequenze sono raggruppate insieme, ma che aspetto hanno?
Visualizzazione dei gruppi
E ora la parte più interessante... la visualizzazione dei gruppi!
Per ogni chiave del dizionario G, che si riferisce alla sequenza seed A, tracciamo un grafico dedicato per tutte le sequenze simili al seed.
Ricorda: tutte le sequenze simili condividono un basso valore di distorsione temporale dinamica e una tendenza simile informata dalla correlazione. I parametri utilizzati per ottenere questo raggruppamento sono
N = 15
K = 0.70
THRESHOLD = 6
Creiamo una funzione chiamata
plot_similar_sequences che prende in argomento G
import math
def plot_similar_sequences(G):
n_col = round(math.sqrt(len(G)))
if (math.sqrt(len(G)) > int(math.sqrt(len(G)))):
n_col = int(math.sqrt(len(G))) + 1
fig, ax = plt.subplots(n_col, n_col, figsize=(n_col * 20, 20 * n_col))
r = 0
c = 0
for key in G.keys():
for j in G[key]:
if (r >= n_col):
print(""Errore"")
if (c >= n_col):
c = 0
r = r + 1
ax[r][c].set_title(f'Group {key}', fontdict={""fontsize"": 30, ""weight"": 600})
ax[r][c].plot(A[j], label=j, linestyle='--', linewidth=10, alpha=0.50)
ax[r][c].plot(A[key], label=f'target {j}', linewidth=10, color='black')
ax[r][c].annotate(f'{key}', xy=(len(A[key]) - 1, A[key][-1]), xytext=(len(A[key]) - 1, A[key][-1]))
ax[r][c].plot(np.mean(A[key], axis=0), label='average', color='black', linestyle='--')
c = c + 1
plt.show()
Ringrazio Giovanni Moschese per la sua contribuzione nella creazione della funzione per la visualizzazione dei gruppi.
L'output è il seguente
La logica sembra funzionare bene, poiché le linee nere, che sono le sequenze seme, sono circondate da sequenze simili sia in termini di anatomia che di direzione.
Controlliamo un'altra combinazione di parametri.
N = 25
K = 0.60
THRESHOLD = 25
I risultati possono cambiare drasticamente in base ai parametri di input. Ciò è previsto poiché la dimensione della finestra, la dimensione della divisione della sequenza A/B e la soglia influiscono in maniera importante sulla logica dell'algoritmo di clustering.
Alcuni modelli spiccano davvero, come questo nel gruppo 22
e gruppo 26
La maggior parte delle sequenze attorno alla sequenza seed sono effettivamente simili e condividono visivamente diverse caratteristiche. Credo che sia necessaria una metodologia più accurata per valutare queste proprietà.
Anche se sono sicuro che esiste un modo migliore per calcolare il punteggio, questi risultati mi sembrano promettenti!
Calcolare le probabilità
Ci avviciniamo all'ultima sezione di questo articolo. Ora calcoleremo la probabilità di avere un trend rialzista, ribassista o stabile per le sequenze B. Ricordiamo che B segue sempre A — il che significa che il primo elemento di \( B_i \) segue il rispettivo elemento di \( A_i \).
Definiremo un'altra funzione di supporto chiamata
classify_trend che è responsabile del calcolo della pendenza e della comprensione della direzione generale della sequenza B.
def classify_trend(b, threshold=0.05):
""""""
Classify the trend of a vector
""""""
# compute slope
slope = np.mean(np.diff(b) / np.diff(np.arange(len(b))))
# if slope is positive, the trend is upward
if slope + (slope * threshold) > 0:
return 1
# if slope is negative, the trend is downward
elif slope - (slope * threshold) < 0:
return -1
# if slope is close to 0, the trend is flat
else:
return 0
# flatten list
flattened_G = [item for sublist in G.values() for item in sublist]
trends = [classify_trend(B[i]) for i in flattened_G]
# what is the probability of seeing a trend given the A sequence?
PROBABILITIES = {}
for k, v in G.items():
for seq in v:
total = len(v)
seq_trends = [classify_trend(B[seq]) for seq in v]
prob_up = len([t for t in seq_trends if t == 1]) / total
prob_down = len([t for t in seq_trends if t == -1]) / total
prob_stable = len([t for t in seq_trends if t == 0]) / total
PROBABILITIES[k] = {'up': prob_up, 'down': prob_down, 'stable': prob_stable}
# Let's pack all in a Pandas DataFrame for an easier use
probs_df = pd.DataFrame(PROBABILITIES).T
# create a column that contains the number of elements in the group
probs_df['n_elements'] = probs_df.apply(lambda row: len(G[row.name]), axis=1)
probs_df.sort_values(by=[""n_elements""], ascending=False, inplace=True)
e questi sono i risultati
Visualizziamo le distribuzioni di probabilità.
Ho scelto di ordinare gli elementi per
n_elements, poiché più le sequenze sono raggruppate, più rilevante è il raggruppamento. In effetti, i gruppi 1 e 14 hanno rispettivamente 8 e 7 elementi, che sono molti modelli simili.
Questo non è sorprendente, poiché entrambe le sequenze A sono sostanzialmente piatte. C'è sicuramente spazio per l'ottimizzazione qui, come, ad esempio, la rimozione di tutte le linee piatte. Ma lo vedremo in futuro. In ogni caso, il 75% delle sequenze B nel Gruppo 1 ha una direzione verso l'alto, mentre il 57% delle sequenze B nel Gruppo 14 ha una direzione verso il basso. Forse le due linee sono davvero diverse per alcuni aspetti, o questo effetto è solo casuale - non possiamo dirlo al momento. Tracciamo le sequenze B.
La maggior parte di queste sequenze B ha un trend effettivamente rialzista. C'è del lavoro da fare per garantire che questo non sia solo rumore.
Controlliamo il gruppo 14
E in effetti, la maggior parte di loro ha un trend ribassista. Anche se la logica sembra essere corretta, forse il calcolo della pendenza ha bisogno di essere rivisito: forse possiamo aumentare la soglia dal valore predefinito 0,05... ma lo vedremo nel prossimo articolo.
Giusto per pura curiosità, tracciamo il Gruppo 22, che sembra interessante anche dal punto di vista visivo.
Sono solo io o le sequenze B sembrano davvero simili tra loro? Sebbene ciò richieda un'analisi approfondita, se ciò si rivela vero, allora A potrebbe davvero prevedere B e questo, di conseguenza, spiegherebbe perché vediamo B simili.
Conclusione della parte 1
In questa prima parte abbiamo visto come funziona l'algoritmo e il potenziale che potrebbe avere nel prevedere le tendenze del mercato azionario.
In realtà, se funziona, potrebbe essere potenzialmente un approccio generale...applicabile a qualsiasi serie temporale. Amplierò l'analisi nel prossimo futuro mentre continuo a lavorare sul progetto.
Condividi i tuoi commenti, dubbi e pensieri sul metodo. Mi piacerebbe integrare feedback e contributo! 😊
Commenti dalla community"
https://www.diariodiunanalista.it/posts/come-calcolare-la-similarita-tra-testi-di-un-sito-web-con-tf-idf-in-python/,"Calcolare la similarità tra due testi è una attività molto utile nell'ambito del data mining e dell'elaborazione del linguaggio naturale (NLP, natural language processing). Questa permette sia di isolare anomalie, ad esempio testi molto simili tra di loro o molto diversi, e di raggruppare entità simili in categorie utili.
In questo articolo andremo ad utilizzare uno script pubblicato qui per scraperare un blog e creare un piccolo corpus su cui applicare un algoritmo di calcolo della similarità basato su TF-IDF in Python.
In particolare andremo ad utilizzare una libreria chiamata Trafilatura per recuperare dalla sitemap tutti gli articoli di un blog e li inseriremo in un dataframe Pandas per l'elaborazione.
Invito il lettore a leggere l'articolo che ho linkato sopra per comprendere come funziona l'algoritmo di estrazione più in dettaglio.
Per semplicità, nell'esempio andremo ad analizzare proprio diariodiunanalista.it al fine di comprendere se esistono articoli troppo simili tra di loro.
Questo ha notevoli ripercussioni SEO - infatti articoli simili tra di loro danno luogo al fenomeno della cannibalizzazione del contenuto: quando due pezzi appartenenti allo stesso sito web competono per la stessa posizione su Google.
I requisiti
Le librerie che ci serviranno saranno Pandas, Numpy, NLTK, Sklearn, TQDM, Matplotlib e Seaborn.
Importiamole nel nostro script Python.
import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
import string
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
Inoltre, servirà lanciare il comando
nltk.download('stopwords') per installare e stopword di NLTK. Una stopword è una parola che non contribuisce in maniera importante al significato di una frase e ci serviranno per preprocessare i nostri testi.
Creazione del dataset
Andiamo ad eseguire il software creato nell'articolo menzionato sopra.
if __name__ == ""__main__"":
list_of_websites = [
""https://www.diariodiunanalista.it/"",
]
df = create_dataset(list_of_websites)
df.to_csv(""dataset.csv"", index=False)
Andiamo a dare una occhiata al nostro dataset.
Dalla tassonomia delle URL notiamo come tutti i post siano raccolti sotto /posts/ - questo ci permette di isolare solo gli articoli veri e propri, tralasciando pagine, categorie, tag e altro.
Usiamo il seguente codice per applicare questa selezione
posts = df[df.url.str.contains('post')]
posts.reset_index(inplace=True)
Abbiamo il nostro corpus. Al momento di scrittura di questo pezzo stiamo intorno a 30 articoli - si tratta quindi di un corpus molto piccolo. Andrà comunque bene per il nostro esempio.
Preprocessing dei testi
Applicheremo un minimo di preprocessing dei testi per replicare una pipeline reale di applicazione. Questa può essere espansa per integrare i requisiti del lettore.
Step di preprocessing
Andremo ad applicare questi step di preprocessing:
- rimozione punteggiatura
- applicazione di minuscole
Il tutto verrà fatto in una funzione molto semplice, che utilizza la libreria standard string e NLTK.
Questa funzione verrà utilizzata dal vettorizzatore TF-IDF (che definiremo a breve) per normalizzare il testo.
L'algoritmo di calcolo della similarità
Per prima cosa, andiamo a definire le nostre stopword salvandole in una variabile
ita_stopwords = stopwords.words('italian')
Ora importiamo
TfIdfVectorizer da Sklearn, passandogli la funzione di preprocessing e le stopword.
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(tokenizer=preprocess, stop_words=ita_stopwords)
Il vettorizzatore TF-IDF andrà a convertire ogni testo in una rappresentazione vettoriale dello stesso. Questo ci consentirà di trattare ogni testo come una serie di punti in uno spazio multidimensionale.
Il modo in cui andremo a calcolare la similarità sarà attraverso la computazione del coseno tra i vettori che costituiscono i testi che mettiamo a confronto. Il valore di similitudine è compreso tra -1 e +1. Un valore di +1 indica due testi essenzialmente uguali, mentre invece -1 indica una completa dissociazione.
Invito il lettore interessato a leggere di più sull'argomento sulla pagina Wikipedia dedicata.
Definiamo una funzione chiamata
compute_similarity che userà il vettorizzatore per convertire i testi in numero e applica la funzione per calcolare il coseno di similarità con i vettori TF-IDF.
def compute_similarity(a, b):
tfidf = vectorizer.fit_transform([a, b])
return ((tfidf * tfidf.T).toarray())[0,1]
Testiamo il funzionamento
Prendiamo in esempio due testi presi da Wikipedia.
Figlio secondogenito del giudice sardo Ugone II di Arborea e di Benedetta, proseguì e intensificò l'eredità culturale e politica del padre, volta al mantenimento dell'autonomia del giudicato di Arborea e alla sua indipendenza, che ampliò all'intera Sardegna. Considerato una delle più importanti figure nel '300 sardo, contribuì allo sviluppo dell'organizzazione agricola dell'isola grazie alla promulgazione del Codice rurale, emendamento legislativo successivamente incluso da sua figlia Eleonora nella ben più celebre Carta de Logu
L'incredibile Hulk è un film del 2008 diretto da Louis Leterrier. Il protagonista è interpretato da Edward Norton, il quale contribuì anche alla stesura della sceneggiatura insieme a Zak Penn; il supereroe è incentrato principalmente sulla versione Ultimate si sottopone all'esperimento di proposito, e non viene investito dai raggi gamma nel tentativo di salvare Rick Jones come nell'universo Marvel tradizionale. Il personaggio mantiene comunque i tratti del ""gigante buono"" della versione classica che vuole solo essere lasciato in pace dagli uomini, e non il bestiale assassino dell'altro universo.
Applichiamo la funzione
compute_similarity per testare quanto simili siano questi due testi. Ci aspettiamo un valore abbastanza basso poiché trattano di argomenti diversi e non usano la stessa terminologia.
I due testi mostrano una similarità molto bassa, vicino allo 0. Vediamo ora con due testi abbastanza simili.
Copierò parte del secondo testo nel primo, mantenendo una lunghezza simile
La similarità è ora di 0.33. Sembra funzionare bene.
Ora applichiamo questo metodo a tutti i testi presenti, in maniera accoppiata (pairwise).
M = np.zeros((posts.shape[0], posts.shape[0])) # creiamo una matrice 30x30 per contenere i risultati di testo_i con testo_j
for i, row in tqdm(posts.iterrows(), total=posts.shape[0], desc='1st level'): # definiamo i
for j, next_row in posts.iterrows(): # definiamo j
M[i, j] = compute_similarity(row.article, next_row.article) # popoliamo la matrice con i risultati
Andiamo nel dettaglio di quello che fa questo pezzo di codice.
- Creiamo una matrice 30x30 chiamata
M
- Iteriamo riga per riga sul dataframe per accedere all'articolo \( i \)
- Iteriamo riga per riga sullo stesso dataframe nuovamente, per accedere all'articolo \( j \)
- Lanciamo
compute_similaritysu \( articolo_i \) e su \( articolo_j \) per ottenere la similarità
- Salviamo questo valore in
Ma posizione \( i \), \( j \)
M può essere facilmente convertita in un dataframe Pandas per la visualizzazione di una heatmap attraverso Seaborn.
labels = posts.url.str.split('/').str[3:].str[1] # estraiamo i titoli degli articoli dalle url
similarity_df = pd.DataFrame(M, columns=labels, index=labels) # creiamo un dataframe
mask = np.triu(np.ones_like(similarity_df)) # applichiamo una maschera per rimuovere la parte superiore della heatmap
# creiamo la visualizzazione
plt.figure(figsize=(12, 12))
sns.heatmap(
similarity_df,
square=True,
annot=True,
robust=True,
fmt='.2f',
annot_kws={'size': 7, 'fontweight': 'bold'},
yticklabels=similarity_df.columns
xticklabels=similarity_df.columns,
cmap=""YlGnBu"",
mask=mask
)
plt.title('Heatmap delle similarità tra testi', fontdict={'fontsize': 24})
plt.show()
La mappa di calore mette in risalto le anomalie usando colori più accesi o spenti in base al valore di similarità ottenuto.
Facciamo un piccolo cambiamento al codice per selezionare solo gli elementi che hanno similarità superiore a 0.40.
top = similarity_df[similarity_df > 0.4] # andiamo a modificare qui
mask = np.triu(np.ones_like(top))
sns.heatmap(
top,
square=True,
annot=True,
robust=True,
fmt='.2f',
annot_kws={'size': 7, 'fontweight': 'bold'},
yticklabels=top.columns
xticklabels=top.columns,
cmap=""YlGnBu"",
mask=mask
)
plt.title('Heatmap delle similarità tra testi', fontdict={'fontsize': 24})
plt.show()
Vediamo 4 pagine con un indice di similarità maggiore di 0.4.
In particolare vediamo queste combinazioni:
- 6 cose da fare prima di addestrare il tuo modello -> il più grande ostacolo nel machine learning - l'overfitting
- 6 cose da fare prima di addestrare il tuo modello -> cosa è la cross-validazione nel machine learning
- cosa è la cross-validazione nel machine learning -> il più grande ostacolo nel machine learning - l'overfitting
- cosa è il machine learning -> qual è la differenza tra machine learning e deep learning
La similarità tra alcune di queste coppie è presente anche tra altre coppie che mostrano similarità alta.
Questi articoli sono accomunati dall'argomento, cioè quello del machine learning e di alcune best practice. Vale la pena girare questo script alla mia prossima pubblicazione in queste categorie, cosi da non incorrere in similarità troppo alta!
Conclusioni
In questo articolo abbiamo visto un semplice ma efficace algoritmo per identificare pagine o articoli simili di un sito web, scraperato con un metodo altrettanto efficiente.
I next step includerebbero una analisi più approfondita per capire perché questi articoli abbiano una similarità alta. Strumenti di data mining e NLP, come Spacy, fanno molto comodo e permettono una analisi POS (part of speech) e NER (named entity recognition).
Studiare le keyword più usate sarebbe altrettanto efficace.
Template
Ecco qui l'intera codebase
import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
import string
from sklearn.feature_extraction.text import TfidfVectorizer
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
###
Ricordiamo di creare il dataset con lo script presente qui
https://www.diariodiunanalista.it/posts/come-scraperare-un-blog-e-raccogliere-i-suoi-articoli
###
posts = df[df.url.str.contains('post')]
posts.reset_index(inplace=True)
remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)
ita_stopwords = stopwords.words('italian')
def preprocess(text):
return nltk.word_tokenize(text.lower().translate(remove_punctuation_map))
vectorizer = TfidfVectorizer(tokenizer=preprocess, stop_words=ita_stopwords)
def compute_similarity(a, b):
tfidf = vectorizer.fit_transform([a, b])
return ((tfidf * tfidf.T).toarray())[0,1]
M = np.zeros((posts.shape[0], posts.shape[0]))
for i, row in tqdm(posts.iterrows(), total=posts.shape[0], desc='1st level'):
for j, next_row in posts.iterrows():
M[i, j] = compute_similarity(row.article, next_row.article)
labels = posts.url.str.split('/').str[3:].str[1]
similarity_df = pd.DataFrame(M, columns=labels, index=labels)
mask = np.triu(np.ones_like(similarity_df))
plt.figure(figsize=(12, 12))
sns.heatmap(
similarity_df,
square=True,
annot=True,
robust=True,
fmt='.2f',
annot_kws={'size': 7, 'fontweight': 'bold'},
yticklabels=similarity_df.columns
xticklabels=similarity_df.columns,
cmap=""YlGnBu"",
mask=mask
)
plt.title('Heatmap delle similarità tra testi', fontdict={'fontsize': 24})
plt.show()
Commenti dalla community"
https://www.diariodiunanalista.it/posts/come-da-analista-ho-sviluppato-uno-strumento-per-generare-nft/,"Il karma è un boomerang. Ciò è dimostrato dal fatto che oggi un artista possa non solo guadagnarsi da vivere, ma anche guadagnare milioni online.
Sono all'inizio dei miei trenta e per anni ho visto solo una manciata di artisti (ovvero persone che disegnano, dipingono e creano contenuti visivi) raggiungere il successo nella loro carriera.
Queste persone sono state abbastanza fortunate e talentuose da mostrare il loro lavoro alle persone giuste, che in cambio hanno pagato molto bene. Ma non tutti gli artisti hanno questo lusso.
La maggior parte lavorano duramente ma alcuni di loro non riescono nemmeno a dedicarsi alle proprie passioni a causa di vincoli economici. Non sono un artista, ma sono felice di vedere come la storia che circonda questo lavoro sta per cambiare, e questo è solo l'inizio.
Ma non sono qui solo per parlare di artisti: quello che voglio davvero fare è condividere con voi le opportunità che ci sono per noi appassionati di dati. In fatti, spiegherò come ho utilizzato le mie competenze nel data science per creare un utile software di generazione NFT per la comunità crypto.
Perché sfruttare lo spazio degli NFT?
Definire bene lo spazio NFT è complicato. È una nicchia delle cryptovalute estremamente rischiosa ma anche redditizia che attrae ogni giorno sempre più persone. Ciò che lo rende interessante è che questo campo (al momento) sfrutta principalmente le opere d'arte per creare valore. Oggi gli artisti hanno quindi opportunità per realizzare grandi profitti vendendo collezioni NFT.
Non introdurrò cosa sono gli NFT. Ci sono molte risorse qui online che approfondiscono questo argomento. Tuttavia, farò una breve introduzione su come sono stato coinvolto nel campo.
Di recente ho lanciato un mio progetto NFT come co-fondatore con un amico artista (maggiori informazioni qui). L'idea è che l'artista creerebbe l'opera d'arte e io lo aiuterei a costruire e impostare la struttura tecnologica. Per questo progetto abbiamo scelto la blockchain di Solana.
La creazione dell'artwork è andata molto bene e in pochissimo tempo abbiamo avuto gli asset pronti per la generazione NFT. Una breve ricerca online ha fornito una soluzione popolare: Hashlips Art Engine. Ho esplorato rapidamente la base di codice e mi sono reso conto che il software era ben scritto e offriva funzionalità interessanti.
Ma quello che avevamo in mente non era ottenibile attraverso Hashlips: volevamo codificare classi di rarità esplicite (come leggendaria, epica, rara, comune, …) e controllarne il loro comportamento durante la generazione.
Volevamo impostare le probabilità che i livelli si verificassero durante la generazione ed escludere anche che combinazioni specifiche avvenissero insieme. Inoltre, essendo l'artista l'utilizzatore ipotetico del software, la sua interfaccia utente doveva essere abbastanza semplice per gli utenti non tecnici in modo che potessero generare la loro collezione NFT solo focalizzandosi sull'artwork.
Mi sono preso la briga di scrivere questo software io stesso in Python e l'ho reso open source cosicché la community possa usarlo e migliorarlo come meglio crede.
Come funziona il software?
Ho iniziato a scrivere il software partendo dal file di configurazione. Gli utenti possono impostare i parametri della loro collezione semplicemente manipolando una serie di dizionari Python. Anche se può sembrare scoraggiante per le persone che sono completamente nuove alla lingua, quelli con una conoscenza introduttiva di base nelle strutture dati si sentiranno a casa.
L'idea era quella di consentire agli utenti di controllare le classi di rarità e la loro probabilità di occorrenza in un'immagine. Vediamo un esempio.
Supponiamo che tu sia un artista e desideri creare una raccolta NFT a tema gotico basata su mostri. Prendi un orco come esempio. Vuoi dare a questo orco la possibilità di brandire armi, armature e altro. A sua volta, ciascuno di questi elementi dovrebbe essere più forte o più debole in base ad una probabilità.
Questo è proprio quello che questo software permette di fare.
Crei le tue classi di oggetti per ogni livello, come leggendario, epico, raro, ecc. e poi assegni le probabilità a queste classi. Ad esempio, un'armatura leggendaria può verificarsi una volta ogni dieci oggetti, ovvero il 10%.
È inoltre possibile configurare il comportamento degli oggetti speciali. Gli oggetti speciali sono oggetti così rari da essere persino più preziosi degli oggetti leggendari. Puoi specificare un elenco di oggetti unici e la probabilità relativa e ottenere un insieme di oggetti che si comportano in modo diverso all'interno della stessa classe di rarità.
Abbiamo pubblicato un video su YouTube che copre in dettaglio il funzionamento del software (in lingua inglese).
I requisiti
Questo è tutto ciò che serve per eseguire correttamente il software:
- Python 3.7+
- Livelli (layer) di immagini
Non c'è altro. Abbiamo letteralmente bisogno di Python, inizializzare un ambiente virtuale (che è facoltativo, ma altamente consigliato) e installare Pillow. Informazioni dettagliate su come installare il software si trovano nel repository Github.
Il file config.py
Esaminiamo brevemente il file di configurazione per vedere cosa dobbiamo modificare per personalizzare la nostra raccolta NFT. Al momento della stesura di questo articolo, il software è adatto solo per la blockchain di Solana. In futuro consentirà anche le raccolte su Ethereum.
Metadati del progetto
./src/config.py è il file che bisogna modificare per personalizzare il progetto. Tutto inizia con il dizionario dei metadati del progetto Solana.
Questi parametri iniziali servono come base per la creazione di file di metadati per ogni NFT. In Solana, ogni NFT è associato a un file metadata.json e l'enumerazione inizia da 0.
Ordine dei layer
Successivamente bisogna specificare il nome e il numero di livelli da combinare per creare un'immagine.
Le immagini vengono assemblate attraverso la libreria Pillow. È infatti l'unica libreria esterna utilizzata in questo software, il resto è tutto parte della standard library di Python.
L'ordine dei livelli dovrebbe seguire un principio semplice: i livelli dovrebbero essere ordinati dallo sfondo al primo piano. Questo per garantire la corretta sovrapposizione degli elementi durante la creazione dell'immagine.
Classi di rarità, probabilità e oggetti speciali
Il dizionario successivo da modificare è contenuto nella variabile SETUP. È qui che avviene la magia (non è davvero magia, solo semplice matematica!).
Puoi aggiungere classi sotto la chiave scheme e elementi a quella classe in una semplice lista Python.
I nomi delle classi di rarità sono arbitrari. Abbiamo usato la convenzione ludicizzata di leggendario, epico e così via, ma è possibile usare quello che si preferisce.
La chiave p è dove si definisce la probabilità di quella classe. Se Legendary ha p = 0,1 significa che la classe leggendaria per quel livello si verificherà circa una volta ogni dieci NFT. Bello, vero? :) Questo è valido per tutte le classi che si decide di aggiungere.
Tutti gli elementi della classe hanno la stessa probabilità di verificarsi. Quindi, come controlliamo la rarità degli oggetti speciali? Specificando un dizionario, possiamo moltiplicare la probabilità degli elementi nella classe per quel moltiplicatore. Stiamo essenzialmente moltiplicando due probabilità.
Nell'esempio seguente, la classe leggendaria ha una probabilità del 10% di apparire. Dato che tutti gli oggetti all'interno di quella classe hanno la stessa probabilità di apparire (seguono una distribuzione uniforme), la probabilità che un oggetto appaia è di circa il 16% (1/6).
Abbiamo impostato Tengu mask, Vader mask e Gas mask come oggetti speciali: vogliamo che appaiano meno del 16%. Usando 0,2, in pratica stiamo moltiplicando 16% * 20% = ~3%.
Naturalmente, poiché ora la probabilità di tre oggetti è più bassa, la probabilità che gli altri oggetti appaiano è più alta.
In questo modo è possibile manipolare non solo le probabilità per le classi, ma anche per i singoli oggetti.
Combinazioni da escludere
L'ultimo pezzo del puzzle è opzionale, ma comunque utile se un artista ha creato combinazioni complesse tra livelli. In EXCLUDE_COMBINATIONS è possibile specificare le combinazioni da escludere dalla collezione NFT.
Perché farlo? Un caso sarebbe quello di evitare conflitti e glitch tra livelli incompatibili. Abbiamo escluso la maschera Tengu e altri oggetti a causa del loro aspetto insieme. Non ci piacevano, quindi abbiamo deciso di consentire all'utente di controllare anche questo aspetto.
Rimando nuovamente al video di YouTube che illustra tutte queste funzionalità nel dettaglio.
Come creare una collezione di NFT
Ora che abbiamo impostato tutto nel file config.py, siamo pronti per creare la nostra raccolta NFT.
Mettiamo la cartella Layers nella root del progetto e assicuriamoci che le convenzioni di denominazione utilizzate siano coerenti ovunque.
Ora raggiungiamo ./src/run.py e impostiamo il numero di elementi che compongono la raccolta.
Il terminale dovrebbe popolarsi con la sequenza genetica dei nostri NFT.
Se una sequenza genetica non è valida (ad esempio se la combinazione si trova in EXCLUDE_COMBINATIONS), il software distruggerà la sequenza e ne creerà una nuova.
Una volta completata, troveremo la cartella collection che conterrà le immagini e metadati, insieme ad alcuni report sulla rarità di oggetti e classi.
E questo è tutto! Ora siamo pronti per pubblicare la nostra collezione NFT su Solana!
Se avete trovato utile questo software, condividete i vostri pensieri nei commenti qui sotto o mandatemi un messaggio. Se siete degli sviluppatori, sentitevi liberi di contribuire su Github.
Commenti dalla community"
https://www.diariodiunanalista.it/posts/come-funziona-un-modello-di-autocorrezione/,"In questo post vedremo gli step essenziali per sviluppare un semplice modello di autocorrezione. Non mi soffermerò sull'implementazione, ma sulla logica e sulle particolarità di modelli del genere.
Vediamo modelli di autocorrezione soprattutto nei nostri smartphone, mentre scriviamo su Whatsapp, Facebook Messenger o altre applicazioni di messaggistica. Essi pervadono la nostra quotidianità, ma come ogni tecnologia che lavora ""dietro le quinte"", tendiamo a prendere consapevolezza della sua esistenza solo quando quest'ultime sbagliano e ci trasmettono frustrazione e ansia nell'utilizzo. Accade spesso infatti che il nostro autocorrettore venga accusato di aver predetto erroneamente la nostra intenzione.
Oltre al funzionamento, vedremo come mai modelli del genere racchiudano difficoltà importanti che spiegano il motivo dei loro errori.
Step 1: Identificare la parola scritta male
Una parola viene identificata come typo se quest'ultima non è presente nel vocabolario del software. Il vocabolario ha la forma di un insieme con elementi unici.
Step 2: Trovare stringhe che sono distanti n modifiche
n sta per il numero di caratteri da modificare per trasformare la stringa osservata in una stringa target. La stringa osservata, nel nostro caso, è il typo, mentre la stringa target è il potenziale candidato per il suggerimento. Ci sono quattro tipi di manipolazioni che modificano la nostra stringa in un potenziale suggerimento sensato:
- Inserimento (aggiungo una lettera): ""cia"" --> ""ciao""
- Rimozione (rimuovo una lettera): ""ciaop"" --> ""ciao""
- Scambio (scambio una lettera con un'altra): ""caio"" --> ""ciao""
- Sostituzione (aggiungo una lettera alla posizione k, rimuovendo la lettera originaria alla posizione k): ""ciso"" --> ""ciao""
Com'è possibile notare, ognuna di queste manipolazioni trasforma una stringa errata in una stringa di senso compiuto. Ovviamente il computer non ha idea di quale sia una parola si senso compiuto o meno, quindi vengono generati m elementi per ognuna delle manipolazioni nell'esempio. Questo crea una lista molto lunga di potenziali candidati che devono essere quindi filtrati per rimuovere le parole senza senso.
Step 3: Filtrare i candidati
Usando lo step 1, troviamo l'intersezione tra il nostro vocabolario di termini e le stringhe manipolate nello step 2. Questo ci assicura che tutti le stringhe siano di senso compiuto.
Step 4: Calcolare le probabilità per i suggerimenti
L'ultimo step è quello di selezionare il candidato che ha la probabilità più alta di occorrere in quel determinato contesto. Questo è il modo in cui un modello ""base"" di autocorrect sa fornire un suggerimento valido per il nostro errore.
La formula matematica per calcolare le probabilità che una parola possa occorrere nel nostro corpus è la seguente
\[ P(parola) = \frac{C(parola)}{V} \]
la probabilità P che una parola appaia nel corpus è uguale a quante volte vediamo quella parola nel nostro vocabolario fratto la lunghezza del nostro vocabolario V Prendiamo l'esempio seguente
|Parola
|Conteggio
|Io
|10
|sono
|7
|bravo
|1
|con
|4
|Python
|2
|e
|6
|R
|1
Ponendo che il nostro corpus è formato da 165 termini, vediamo come la parola Python abbia una probabilità di occorrere di \[ \frac{2}{165} = 0.012 \]
Abbiamo ora disponibile una distribuzione di probabilità delle parole nel nostro corpus. Il nostro sistema di autocorrect andrà quindi a selezionare il candidato che ha la probabilità di capitare nel nostro corpus più alta. Molto semplice.
Conclusione
Abbiamo visto la logica e come implementare un semplice algoritmo di autocorrezione. Questo modello mentale può fungere da base per creare modelli più complessi, che non si basano semplicemente sulla probabilità di occorrenza dei termini, ma anche sul contesto e sul dizionario che ha manualmente creato l'utente attraverso le varie correzioni da lui inserite durante la scrittura.
Commenti dalla community"
https://www.diariodiunanalista.it/posts/come-identificare-anomalie-nei-tuoi-dati/,"Nella analisi dei dati, identificare anomalie (outlier detection) è un tema cruciale che può fornire parecchie informazioni importanti.
In questo articolo ci concentreremo sui metodi e tecniche utilizzati per identificare le anomalie nei dati. In particolare, esploreremo tecniche di visualizzazione dei dati e l'utilizzo di statistiche descrittive e test statistici.
La definizione di anomalia
Una anomalia, spesso riferita come outlier dall’inglese, è un valore che si discosta significativamente dagli altri valori del dataset. Questo discostamento può essere numerico o anche categoriale.
Ad esempio, un discostamento numerico è quando abbiamo un valore molto più grande o molto più piccolo rispetto alla maggior parte degli altri valori all'interno del dataset.
Un discostamento categoriale, invece, si verifica quando abbiamo delle etichette note come ""altro"" o ""sconosciuto"" che rappresentano una proporzione molto più alta rispetto alle altre etichette all'interno del dataset.
Può essere causata da errori di misurazione, errori di input, errori di trascrizione o semplicemente da dati che non seguono il normale andamento del dataset.
In alcuni casi, le anomalie possono essere indicative di problemi più ampi nel dataset o nel processo che ha prodotto i dati e possono offrire informazioni importanti alle persone che hanno sviluppato il processo di raccolta dati.
Come identificare le anomalie in un set di dati
Ci sono diverse tecniche che puoi utilizzare per identificare le anomalie nei tuoi dati. Ecco quelle che toccheremo in questo articolo
- visualizzazione dei dati: che permette di identificare anomalie guardando la distribuzione dei dati facendo uso di grafici utili a tale scopo
- utilizzo di statistiche descrittive, come la differenza interquartilica
- utilizzo di score z
- utilizzo di tecniche di clustering: che permette di individuare gruppi di dati simili e di identificare eventuali dati ""isolati"" o ""non classificabili""
ognuno di questi metodi è valido per identificare le anomalie, e vanno scelti in base ai nostri dati. Vediamone uno per uno.
Visualizzazione dei dati
Una delle tecniche più comuni per trovare anomalie è attraverso l’analisi esplorativa dei dati e in particolare con la visualizzazione dei dati.
Utilizzando Python, puoi utilizzare librerie come Matplotlib o Seaborn per visualizzare i dati in modo tale da poter individuare facilmente eventuali anomalie.
Ad esempio, è possibile creare un istogramma o un boxplot per visualizzare la distribuzione dei tuoi dati e individuare eventuali valori che si discostano significativamente dalla media.
L’anatomia del boxplot può essere compresa da questo post su Kaggle.
Utilizzo di statistiche descrittive
Un altro metodo per individuare le anomalie è l’utilizzo di statistiche descrittive. Ad esempio, la differenza interquartilica (IQR) può essere utilizzata per identificare i valori che si discostano significativamente dalla media.
La differenza interquartilica (IQR) è definita come la differenza tra il terzo quartile (Q3) e il primo quartile (Q1) del dataset. Gli outlier sono definiti come i valori al di fuori dell'intervallo IQR moltiplicato per un coefficiente tipicamente di 1,5.
Il boxplot precedentemente trattato è proprio un metodo che utilizza tale metrica descrittiva per identificare le anomalie.
Un esempio in Python per identificare gli outlier utilizzando la differenza interquartilica è il seguente:
import numpy as np
def find_outliers_IQR(data, threshold=1.5):
# Calcola il primo e il terzo quartile
Q1, Q3 = np.percentile(data, [25, 75])
# Calcola l'IQR
IQR = Q3 - Q1
# Calcola la soglia inferiore e superiore
lower_bound = Q1 - (threshold * IQR)
upper_bound = Q3 + (threshold * IQR)
# Identifica gli outlier
outliers = [x for x in data if x < lower_bound or x > upper_bound]
return outliers
Questo metodo calcola il primo e il terzo quartile del dataset, poi calcola l'IQR e la soglia inferiore e superiore. Infine, identifica gli outlier come quei valori che sono al di fuori della soglia inferiore e superiore.
Questa comoda funzione può essere utilizzata per identificare gli outlier in un dataset e può essere aggiunta al tuo toolkit di funzioni util in pressoché qualsiasi progetto.
Utilizzo di score z
Un altro modo per individuare le anomalie è attraverso dei punteggi z. Gli score z misurano quanto un valore si discosta dalla media in termini di deviazioni standard.
La formula per convertire i dati in punteggi z è la seguente:
\[ z = \frac{x - \mu}{\sigma} \]
dove \( x \) è il valore originale, \( \mu \) è la media del dataset e \( \sigma \) è la deviazione standard del dataset. Il punteggio z indica quanti deviazioni standard il valore originale è distante dalla media. Un valore di punteggio z superiore a 3 (o inferiore a -3) è generalmente considerato un outlier.
Questo metodo è particolarmente utile quando si lavora con grandi dataset e quando si vuole identificare anomalie in modo oggettivo e riproducibile.
In
sklearn in Python, la conversione in punteggi z può essere fatta così
from sklearn.preprocessing import StandardScaler
def find_outliers_zscore(data, threshold=3):
# Standardizza i dati
scaler = StandardScaler()
standardized = scaler.fit_transform(data.reshape(-1, 1))
# Identifica gli outlier
outliers = [data[i] for i, x in enumerate(standardized) if x < -threshold or x > threshold]
return outliers
Utilizzo di tecniche di clustering
Infine, le tecniche di clustering possono essere utilizzate per individuare eventuali dati ""isolati"" o ""non classificabili"". Questo può essere utile quando si lavora con dataset molto grandi e complessi, dove la visualizzazione dei dati non è sufficiente per individuare le anomalie.
In questo caso, un'opzione è di utilizzare l'algoritmo DBSCAN (Density-Based Spatial Clustering of Applications with Noise), che è un algoritmo di clustering che può identificare gruppi di dati in base alla loro densità e individuare eventuali punti che non appartengono a nessun cluster. Questi punti sono considerati come anomalie.
L'algoritmo DBSCAN può essere implementato sempre con
sklearn di Python e può essere utilizzato per identificare anomalie in un set di dati.
Prendiamo ad esempio questo dataset visualizzato
L’applicazione di DBSCAN fornisce questa visualizzazione
Il codice per creare questi grafici è il seguente
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import DBSCAN
def generate_data_with_outliers(n_samples=100, noise=0.05, outlier_fraction=0.05, random_state=42):
# Genera dati casuali
X = np.concatenate([np.random.normal(0.5, 0.1, size=(n_samples//2, 2)),
np.random.normal(1.5, 0.1, size=(n_samples//2, 2))], axis=0)
# Aggiungi outlier
n_outliers = int(outlier_fraction * n_samples)
outliers = np.random.RandomState(seed=random_state).rand(n_outliers, 2) * 3 - 1.5
X = np.concatenate((X, outliers), axis=0)
# Aggiungi rumore ai dati per renderli verosimili
X = X + np.random.randn(n_samples + n_outliers, 2) * noise
return X
# Genera dati con outlier
X = generate_data_with_outliers(outlier_fraction=0.2)
# Applica DBSCAN per trovare cluster e outlier
dbscan = DBSCAN(eps=0.2, min_samples=5)
dbscan.fit(X)
# Ottieni gli indici dei punti outlier
outlier_indices = np.where(dbscan.labels_ == -1)[0]
# Visualizza dati e outlier
plt.scatter(X[:, 0], X[:, 1], c=dbscan.labels_, cmap=""viridis"")
plt.scatter(X[outlier_indices, 0], X[outlier_indices, 1], c=""red"", label=""Outliers"", marker=""x"")
plt.xticks([])
plt.yticks([])
plt.legend()
plt.show()
Questo metodo crea un oggetto
DBSCAN con i parametri
eps e
min_samples e lo adatta ai dati. Quindi identifica gli outlier come quei valori che non appartengono a nessun cluster, cioè quelli che sono etichettati come -1.
Questa è solo una delle molte tecniche di clustering che si possono utilizzare per identificare le anomalie. Ad esempio, un metodo basato sul deep learning si basa sugli autoencoder particolari reti neurali che sfruttano una rappresentazione compressa del dato per identificare caratteristiche distintive nei dati in input.
Conclusione
In questo articolo abbiamo visto diverse tecniche che possono essere utilizzate per identificare le anomalie nei dati.
Abbiamo parlato di visualizzazione dei dati, di utilizzo di statistiche descrittive e di score z, e di tecniche di clustering.
Ognuna di queste tecniche è valida e va scelta in base al tipo di dati che si sta analizzando. L'importante è ricordare che l'individuazione delle anomalie può fornire informazioni importanti per migliorare i processi di raccolta dati e per prendere decisioni migliori in base ai risultati ottenuti.
Commenti dalla community"
https://www.diariodiunanalista.it/posts/come-impostare-un-ambiente-di-sviluppo-per-il-data-science-e-machine-learning/,"Impostare un ambiente virtuale non ha niente di emozionante. Ma proprio nulla.
Può sembrare un argomento banale, noioso.
Ma ti informo di una cosa.
La maggior parte dei neofiti della programmazione non imposta (o sbaglia nel farlo) un ambiente virtuale.
Questo porta con sé notevole svantaggi, come errori di compatibilità tra librerie, disorganizzazione tra librerie e progetti e errori di cui non si capisce la natura in maniera intelligibile.
Evitare questi problemi con le semplici operazioni qui descritte sarà molto di aiuto e ti permetterà di essere più produttivo.
- insegnarti come e perché impostare un ambiente di sviluppo sia essenziale per essere organizzati e programmare in serenità
- insegnarti ad usare conda o Poetry per la gestione di ambienti e pacchetti
Iniziamo!
Cosa è un ambiente virtuale?
Un breve incipit se hai appena iniziato il tuo percorso nella data science e nella programmazione in Python.
Un ambiente virtuale è un ambiente di sviluppo che si comporta come contenitore del nostro attuale progetto.
Pur avendo la possibilità di installare qualsivoglia libreria a livello di sistema, avere un ambiente virtuale permette di installare tali librerie e pacchetti per progetti specifici.
Questo permette di avere un sistema ordinato e facilmente navigabile, senza rischi di incompatibilità o malfunzionamenti a livello globale.
Dedicare un ambiente virtuale ad un progetto è prassi usuale, e andrebbe sempre fatto per i motivi sopracitati.
Nel tempo si è instaurata la tradizione nell'ambito della data science e machine learning di usare Anaconda per la gestione degli ambienti virtuali.
Insieme a quest'ultimo mostrerò anche come funziona Poetry, un'altra libreria molto interessante che ha lo stesso obiettivo di Anaconda, ma con una attenzione alla definizione della struttura del codice e della gestione delle imcompatibilità.
Gestione degli ambienti virtuali con Anaconda
In Python esistono diverse opzioni per la gestione degli ambienti virtuali. Una delle più famose è sicuramente Anaconda, che un software che aiuta (soprattutto i data scientist) a configurare e gestire in maniera efficiente i nostri ambienti di sviluppo.
In uno degli esempi utilizzerò Miniconda per configurare Python sul sistema. È un installer lightweight di conda, un sistema open source di gestione di ambienti di sviluppo orientato al data science e disponibile per Linux, OSX e Windows.
La scelta di optare per Miniconda e non per Anaconda è perché quest'ultimo porta con se numerosi pacchetti che secondo me che non useremo mai se non in sporadici casi e quindi avere una installazione di piccole dimensioni è più sensato.
Come installare Miniconda su OSX
Userò l'installer per Python 3.9. Al tempo di scrittura di questo articolo il file che scaricheremo dal sito ufficiale avrà un nome del genere: Miniconda3-py39_23.5.2-0-MacOSX-arm64.sh
A seconda dell'architettura del nostro sistema possiamo scegliere tra la versione base e la versione per chipset M1 ARM.
Una volta scaricato il file, apriamo il terminale e scriviamo
$ cd Downloads
$ sh Miniconda3-py39_23.5.2-0-MacOSX-arm64.sh
e seguiamo le istruzioni su schermo. Possiamo controllare abbiamo installato correttamente il software se scriviamo
conda nel nostro terminale.
A questo punto abbiamo installato Miniconda sul nostro sistema Mac OSX e siamo pronti a creare un ambiente virtuale.
Creare un ambiente virtuale per la data science con Anaconda o Miniconda
Una volta installato Anaconda o Miniconda e validato il loro corretto funzionamento usando il comando
conda è possibile creare un nuovo ambiente di sviluppo così:
$ conda create -n [nome_del_mio_ambiente]
Questo comando creerà un ambiente di sviluppo virtuale chiamato nome_del_mio_ambiente nella directory di installazione. Per attivare l'ambiente virtuale basta eseguire il comando
$ conda activate [nome_del_mio_ambiente]
A questo punto abbiamo finito. Il nostro ambiente virtuale è pronto per lo sviluppo del nostro applicativo. Vediamo ora come installare o disinstallare librerie e pacchetti nel nostro ambiente virtuale appena attivato.
Come installare pacchetti e librerie nell'ambiente virtuale di Anaconda
Ora è il momento di aggiungere strumenti alla nostra cassetta degli attrezzi. Per aggiungere un pacchetto o libreria Python, basta usare il comando
$ conda / pip install nome_libreria
Come possiamo vedere è possibile usare il comando pip per installare pacchetti anche all'interno di conda. Infatti, se usiamo il comando
$ conda list / pip list
conda o pip, rispettivamente, ci mostreranno i pacchetti installati attraverso il loro comando.
Consiglio di usare il comando pip install rispetto a
conda install perché alcuni pacchetti non sono disponibili nel repository di conda. I motivi possono essere diversi, ma usare pip non ha alcun svantaggio e permette di scaricare e installare pacchetti efficientemente.
Conda Cheat Sheet - ogni comando a portata
Condivido con voi il cheat sheet ufficiale messo a disposizione da Anaconda. Vi permetterà di risparmiare moltissimo tempo nelle vostre ricerche e di ottimizzare il lavoro.
Gestione degli ambienti virtuali con Poetry
Sto usando Poetry sempre di più perché permette di gestire le dipendenze in una maniera strutturata e soprattutto replicabile.
Uno dei punti di forza di Poetry è il suo lockfile. È un file che viene generato da Poetry ogni volta che si aggiunge, cambia o rimuove una dipendenza dal progetto.
In pratica permette di replicare in maniera deterministica un ambiente virtuale e aggiornarlo senza introdurre problema di incompatibilità.
Se sei un neofita questa può non sembrare una grande feature, ma ti garantisco che lo è.
Tantissime librerie dipendono da particolari versioni di librerie cardine, come ad esempio Numpy.
Quando il Numpy di turno rilascia un nuovo aggiornamento, ecco che potrebbe nascere una incompatibilità che potrebbe rendere il nostro progetto problematico.
La soluzione è bloccare la versione della dipendenza ad una specifica versione, che è quella specificata durante l'installazione.
Poetry fa tutto questo per noi, e molto altro. Vediamo come iniziare con Poetry con degli esempi.
Come installare Poetry
Alla pagina della documentazione è possibile trovare diversi link da inserire in terminale in base al nostro sistema operativo.
Usando OSX, è sufficiente lanciare il comando
curl -sSL https://install.python-poetry.org | python3 -
Questo installerà Poetry sul nostro sistema e per testarlo sarà sufficiente scrivere
poetry in terminale.
poetry non genera alcun risultato nel terminale, segui gli step presenti qui sotto la sezione ""Add Poetry to your PATH""
Quando la risposta del terminale è visibile, siamo pronti a creare un ambiente virtuale con Poetry.
Creare un ambiente virtuale per la data science con Poetry
A differenza di Anaconda, Poetry è in grado di creare direttamente la cartella del nostro progetto e popolarla con dei file utili.
$ poetry new [nome progetto]
La bellezza di Poetry è visibile qui: creerà una struttura intellegibile e pronta all'uso per il progetto.
nome-del-mio-progetto
├── pyproject.toml
├── README.md
├── nome-del-mio-progetto
│ └── __init__.py
└── tests
└── __init__.py
Spostiamoci nella cartella facendo
$ cd [nome progetto]
Ovviamente se la struttura proposta da Poetry non è gradita o vogliamo cambiarla, siamo liberi di farlo a patto che non tocchiamo i file
pyproject.toml e
poetry.lock. Quest'ultimo verrà creato non appena installeremo una qualsiasi dipendenza al progetto.
Come installare pacchetti e librerie nell'ambiente virtuale di Poetry
Per installare qualsiasi pacchetto al progetto basta lanciare il seguente comando quando siamo presenti all'interno della cartella oppure attivato l'ambiente virtuale con il comando
poetry shell
$ poetry add pandas numpy
Questo comanda Poetry di installare le librerie di Pandas e Numpy, alla loro ultima versione disponibile, e di avvertirci qualora ci fossero problemi di compatibilità.
Ora se andiamo a guardare il file
pyproject.toml vediamo come questo stia tenendo traccia delle dipendenze e delle loro versioni, e altri metadati.
pyproject.toml si comporta un po' come il file
requirements.txt, ma è obbligatorio per il corretto funzionamento di Poetry
Poetry è veramente facile da usare - ho avuto un grande boost di produttività da quando ho iniziato ad usarlo, ed è la mia prima scelta oggi in termini di software di gestione dell'ambiente virtuale.
Poetry permette di fare questo e molto altro. Per andare oltre questa introduzione ti consiglio di leggere attentamente la documentazione presente sul sito.
Conclusioni
Con questo articolo hai imparato quali sono alcune delle soluzioni più comuni per la gestione di ambienti virtuali e dipendenze in Python per la data science, machine learning e progetti anche generali.
Hai imparato a
- installare Anaconda e Poetry
- creare ambienti virtuali e installare pacchetti
Ricorda: non sottovalutare dedicare il tuo tempo a imparare competenze gestionali per i tuoi progetti informatici.
Se eseguite bene, queste operazioni semplificano la vita e aumentano la produttività.
Alla prossima,
Andrea
Commenti dalla community"
https://www.diariodiunanalista.it/posts/come-partizionare-i-dati-di-training-in-k-fold/,"La cross-validazione è la prima tecnica da utilizzare per evitare overfitting e data leakage quando vogliamo addestrare un modello predittivo sui nostri dati.
La sua funzione è essenziale in quanto permette di testare funzioni e logiche sui nostri dati in modo sicuro - senza che questi processi vadano a ""sporcare"" i nostri dati di validazione.
Se vogliamo fare preprocessing, feature engineering o altre trasformazioni, bisogna sempre prima ripartire i nostri dati correttamente.
Questo assicura che i nostri dati di validazione siano effettivamente rappresentativi del nostro training set e (possibilmente) anche dei dati nel mondo reale che ancora non possediamo.
Se quindi riusciamo a creare un dataset e a ripartire correttamente i nostri dati, allora possiamo essere relativamente sicuri che i risultati che osserviamo dal training siano effettivamente usabili e bias-free.
Ripartire i dati prima di applicare la cross-validazione
Come ho scritto nell'articolo sulla cross-validazione e come metterla in pratica in Python, uno degli step essenziali è stratificare la variabile target rispetto alle fold (porzioni di dataset) per evitare che uno sbilanciamento nelle classi da predire possa influenzare negativamente sulle performance di un modello.
Nel caso di una classificazione binaria, ad esempio, è possibile che l'80% delle classi siano positive e solo il 20% siano negative. Addestrare un modello senza tener conto di questo sbilanciamento potrebbe portare a dei risultati poco attendibili.
Esistono tecniche di bilanciamento del dato, ma non le tratteremo in questo articolo. Invece, spiegherò come usare Python, Pandas e Sklearn per creare un dataset con una colonna in più che indicherà a che fold appartiene una riga del nostro dataset.
Per ogni sample del dataset, indicheremo a quale fold esso appartiene in modo che possiamo testare il nostro algoritmo su ogni fold, valutando i nostri algoritmi porzione per porzione.
Vediamo come applicare questo procedimento in Python.
Iniziamo importando le librerie essenziali
import pandas as pd
from sklearn import model_selection
Ponendo che i nostri dati di training si trovino in un file al percorso
./data/dataset_train.csv, importiamolo con Pandas e creiamo una colonna chiama fold che inizializziamo con il valore di
-1. Questo perché le nostre fold avranno valore da 0 in avanti.
# importiamo il dataset di addestramento
df = pd.read_csv(""./data/dataset_train.csv"")
# definiamo una nuova colonna chiamata 'fold'
df[""fold""] = -1
Tipicamente il dataset viene mescolato - applichiamo questo ragionamento usando
.sample. Useremo il parametro
frac=1 per dire a Pandas di prendere tutto il dataset e campionarlo in maniera casuale. Questo, di fatto, mescolerà ogni riga del dataset in maniera semplice ed efficace.
df = df.sample(frac=1).reset_index(drop=True)
Ora che abbiamo mescolato il dataset, possiamo definire la nostra variabile target per passarla a
StratifiedKFold per stratificare il dataset. Useremo un numero arbitrario di 5 per le nostre fold.
Creato l'oggetto, iteriamo attraverso ogni riga del dataset e applichiamo la porzione generata da Sklearn nella colonna che prima aveva il valore costante di -1.
Fatto questo, salviamo il dataset.
y = df[""target""].values
# inizializziamo l'oggetto per la stratificazione kfold con 5 porzioni
kf = model_selection.StratifiedKFold(n_splits=5)
# popoliamo la colonna fold
for fold, (train_idx, valid_idx) in enumerate(kf.split(X=df, y=y)):
df.loc[valid_idx, ""fold""] = fold
# salviamo il dataset
df.to_csv(""./data/dataset_train_folds.csv"")
Ora abbiamo un dataset che, oltre alle colonne che lo caratterizzavano, possiede una colonna aggiuntiva che indica la porzione per effettuare validazione.
Vediamo l'intero script.
import pandas as pd
from sklearn import model_selection
if __name__ == ""__main__"":
df = pd.read_csv(""./data/dataset_train.csv"")
# definiamo una nuova colonna chiamata 'fold'
df[""fold""] = -1
# mescoliamo il dataset
df = df.sample(frac=1).reset_index(drop=True)
# dichiariamo la variabile target
y = df[""target""].values
# inizializziamo l'oggetto per la stratificazione kfold con 5 porzioni
kf = model_selection.StratifiedKFold(n_splits=5)
# popoliamo la colonna fold
for fold, (train_idx, valid_idx) in enumerate(kf.split(X=df, y=y)):
df.loc[valid_idx, ""fold""] = fold
# salviamo il dataset
df.to_csv(""./data/dataset_train_folds.csv"")
Ora possiamo lanciare questo script e semplicemente cambiando il percorso del dataset su disco sarà possibile creare una sua copia con porzioni di validazione.
Questo approccio è estendibile completamente, ed è consigliabile farlo per praticamente qualsiasi problema di machine learning.
Commenti dalla community"
https://www.diariodiunanalista.it/posts/come-prendere-appunti-nella-data-science/,"Prendere appunti è spesso una attività trascurata durante lo sviluppo di un progetto di data science o analytics, soprattutto dai principianti.
Un errore che ho fatto io stesso nel primo periodo della mia carriera è stato quello di saltare subito alla fase di modellazione, prestando poca attenzione alle fasi precedenti, ma essenziali, della pipeline di machine learning.
In noi data scientist spesso si scatena un’ondata di adrenalina quando iniziamo a modellare i dati (almeno questo succede al sottoscritto).
Ma le fasi di modellazione e previsione possono risentire negativamente di una scarsa strutturazione del progetto, ipotesi o preprocessing sbagliati.
Questo di solito è causato da una moltitudine di motivi:
- sessioni di brainstorming incomplete (o addirittura assenti)
- dati errati (garbage in, garbage out; spazzatura dentro, spazzatura fuori)
- cattive pratiche di programmazione
e altro ancora.
Ti linko un articolo inerente a come strutturare un progetto di machine learning, dove do spunti in merito al file README.md, che uso per prendere appunti.
In questo articolo imparerai:
- una metodologia per strutturare i tuoi appunti in maniera logica e consistente
- come migliorare la tua produttività attraverso l'attività di note-taking
- come tale struttura impatterà la comunicazione del processo
Praticamente in qualsiasi contesto, prendere appunti equivale ad avere una sessione di brainstorming con se stessi e aiuta a chiarire il nostro pensiero.
Prendere appunti dà una struttura progressiva e chiarezza alle varie fasi del progetto e ci aiuta a capire cose che non sarebbero così chiare se non fossero state scritte in primo luogo.
I principianti si precipitano nelle cose perché sono pieni di energia e vogliono mettersi alla prova: questo è il vero motivo per cui commettono così tanti errori.
Vanno di fretta e si dimenticano di scrivere i loro pensieri su carta.
Se sei un principiante, troverai molti vantaggi dal rallentare e concentrarti sul prendere appunti correttamente.
Se sei un professionista esperto, capirai perché sto dedicando un intero articolo a questo argomento.
Spero che ti divertirai a leggerlo :)
Perché scrivere appunti in primo luogo?
Scrivere su questo blog e su Medium ha consolidato ancora di più per me questa convinzione. Essere in grado di scaricare i propri pensieri su carta è veramente un superpotere, e per di più molto sottovalutato.
Perché? Perché scrivere le cose ci fa pensare meglio. Ci permette di affinare i pensieri che funzionano e di sbarazzarti di quelli che non funzionano.
Vediamo come questo influisca sul risultato dei nostri progetti. Se prendiamo appunti correttamente possiamo
- Comprendere il problema a un livello più profondo rispetto allo scrivere codice direttamente a rifarci ai commenti
- Inquadrare il problema da un'angolazione diversa, portandoci a risultati inaspettati
- Scrivere le cose porta a migliori capacità di comunicazione verbale. Ne avremo bisogno quando presenteremo agli stakeholder
- Avere una mappa mentale da seguire e raggiungere il l'obiettivo: ogni volta che ci sentiamo distratti o fuori focus, pos siamo sempre tornare indietro e leggere i nostri appunti.
A breve ti presenterò un processo interessante per prendere appunti in modo efficiente, aumentando chiarezza verso il problema e la tua velocità di esecuzione.
Il template per prendere appunti
Ecco il template che ti menzionavo.
Con questo approccio, che potrai liberamente espandere e trasformare, potrai beneficiare di una metodologia replicabile e scalabile a qualsiasi progetto, anche non inerente alla data science e al machine learning.
Iniziamo dividendo il nostro file in più sezioni:
- informazioni che abbiamo ora (ingredienti)
- cosa intendiamo fare con i dati che abbiamo (ricetta)
- risultati previsti (visione)
- considerazioni e commenti (interpretazione)
L'ordine è fisso, ma se il progetto richiede uno sforzo creativo ulteriore trovo utile inserire prima considerazioni e commenti e poi il resto.
Ingredienti
Qui mettiamo giù ciò che già abbiamo.
Non si tratta solo di dati: stiamo parlando di informazioni che circondano i dati, il contesto, restrizioni o le limitazioni impostate dal cliente e così via.
Ad ogni informazione dedico un paio di righe per descrivere di cosa si tratta e gli ipotetici utilizzi. Questo mi aiuta in diversi modi - elencando e descrivendo gli elementi ho uno scenario di partenza più chiaro. So cosa ho, so cosa posso e cosa non posso fare.
Questa sezione viene solitamente popolata all'inizio del progetto, prima della scrittura di codice. Se vengono visualizzate nuove informazioni durante lo sviluppo, possiamo aggiungerle in un secondo momento.
Ricetta
Qui è dove stabilisco cosa intendo fare con gli ingredienti che ho. Di solito scrivo di una ipotetica una sequenza di azioni, un po' come un diagramma di flusso o un elenco enumerato.
Questi passaggi non sono scolpiti in pietra: se ci rendiamo conto che dobbiamo modificare la direzione o concentrarci su qualche altro aspetto del progetto per garantire un risultato migliore, possiamo aggiornare questi passaggi e consolidare il ragionamento descrivendo perché ciò è accaduto.
Se lavori in gruppo e prevedi di distribuire i tuoi appunti, assicurati di chiarire i dettagli che per altri potrebbero essere ignoti.
Risultati previsti
È molto utile delineare ciò che ci si aspetta di vedere dal nostro modello/analisi prima di iniziare a programmare.
Questo ci aiuterà a vedere i problemi in anticipo se ce ne fossero. Possiamo anche elencare una serie di output che soddisfano le esigenze del cliente.
Considerazioni e commenti
Puro brainstorming e flusso di coscienza. Poniamo domande, elaboriamo nuove idee e utilizziamo qualsiasi strumento di cui abbiamo bisogno per descrivere ciò che abbiamo in mente, anche se non si collega ancora al progetto.
Forse vogliamo ampliare la nostra analisi con una serie di idee che sembrano interessanti? Scriviamolo in dettaglio.
Vogliamo chiedere al cliente e chiarire cosa significa una determinata variabile? Scriviamolo come TODO e chiamiamo il cliente. Ho reso l'idea.
Come questo processo aiuta la comunicazione
Quando avrai preso consapevolezza con questo metodo vedrai che la tua chiarezza verso il problema da risolvere sarà maggiore.
Questo ti permetterà di rispondere a domande precise da parte degli stakeholder o dai membri del tuo team.
Migliore sarà la tua comprensione del problema, maggiore sarà la tua efficienza nel trovare soluzioni risolutive.
Consiglio anche di consolidare i propri pensieri in una presentazione PowerPoint da mostrare al tuo capo o al tuo team. Farai una bella figura e ti metterai in condizione di raccogliere angoli nuovi sul problema che vuoi risolvere.
Conclusione
Se non hai mai preso appunti prima di iniziare un progetto, ti consiglio di farlo. Vedrai un enorme miglioramento nei tuoi ragionamenti e di conseguenza nel tuo output.
Questo processo si applica anche ad altri campi (ma forse con processi diverso), non solo alla data science e analytics.
Sentiti libero di sperimentare con la tua struttura: questo processo funziona per me, ma ciò che funziona per te potrebbe essere leggermente diverso.
Come nota finale, ecco il video che mi ha ispirato anni fa a sviluppare questo processo. George Hotz è un famoso hacker, imprenditore e a dirla tutta un genio della programmazione.
Il modo in cui scorre il suo pensiero e come organizza i suoi progetti è affascinante. Dai una occhiata anche gli altri suoi video, te lo consiglio!
Commenti dalla community"
https://www.diariodiunanalista.it/posts/come-preparare-i-tuoi-dati-per-il-machine-learning/,"Addestrare dei modelli predittivi richiede che i nostri dati siano in un formato adeguato. Non possiamo fornire il nostro file .csv ad un modello e aspettarci che questo impari correttamente a generalizzare.
In questo articolo, vedremo come preparare i dati per il machine learning, partendo dalla pipeline di preparazione dei dati fino alla divisione in set di addestramento, validazione e di test.
Pipeline di preparazione dei dati
Una pipeline è una serie di passaggi che vengono eseguiti per preparare i dati per l'elaborazione da parte del modello di machine learning. La pipeline può variare in base al tipo di dati che si ha a disposizione, ma di solito include i seguenti passaggi:
- Raccolta dei dati: il primo passo è raccogliere i dati che si vogliono utilizzare per addestrare il modello di machine learning. I dati possono provenire da diverse fonti, come ad esempio file CSV, database, API, sensori IoT e altre.
- Pulizia dei dati: una volta raccolti i dati, è importante pulirli eliminando i dati mancanti, correggendo gli errori e rimuovendo i dati duplicati o non rilevanti. La pulizia dei dati contribuisce a migliorare la qualità dei dati utilizzati per addestrare il modello di machine learning.
- Preprocessing dei dati: il preprocessing dei dati include la normalizzazione dei dati, la standardizzazione dei dati, la codifica delle variabili categoriche e la gestione degli outlier. Questi passaggi aiutano a preparare i dati in modo che possano essere elaborati dal modello di machine learning.
- Trasformazione dei dati: la trasformazione dei dati include la riduzione della dimensionalità, la selezione delle features e la creazione di nuove features. Questi passaggi contribuiscono a ridurre il rumore nei dati e a migliorare la capacità del modello di machine learning di fare previsioni accurate.
Questa permette di specificare proprio degli step di processamento dei dati per facilitare la scrittura di codice e ridurre l'occorrenza di errori.
Se vuoi leggere delle pipeline di Scikit-Learn, leggi l'articolo qui in basso
Raccolta dei dati
È importante raccogliere i dati da fonti affidabili e pertinenti al problema che si vuole risolvere.
Ad esempio, se si vuole creare un modello di machine learning per la predizione del prezzo delle case, è necessario raccogliere dati relativi a fattori quali la posizione della casa, il numero di stanze, la presenza di giardino, la vicinanza ai mezzi di trasporto pubblico. Queste informazioni, come informano noi umani del valore di un immobile, informeranno anche il modello predittivo.
È importante scegliere la fonte più adatta al problema e alla disponibilità dei dati. Ad esempio, se si vuole creare un modello di machine learning per la rilevazione di anomalie in un sistema IoT, è necessario raccogliere i dati dai sensori presenti nel sistema.
Altre fonti di dati possono essere
- API pubbliche e private
- Fornitori diretti
- Sondaggi
Inoltre, è importante valutare la qualità dei dati raccolti. I dati possono contenere errori, duplicati o mancanze che possono influire negativamente sulla qualità del modello di machine learning.
Pertanto, è necessario eseguire una pulizia dei dati e controllare che non ci siano dati mancanti, duplicati o errati. La pulizia dei dati può contribuire a migliorare la qualità del modello di machine learning e a ottenere previsioni più accurate.
Qui un articolo che si focalizza proprio sul processo di creazione di un dataset
Preprocessing dei dati
In generale, il preprocessing dei dati include la normalizzazione o standardizzazione dei dati, la codifica delle variabili categoriali e la gestione degli outlier.
La normalizzazione / standardizzazione dei dati viene utilizzata per ridurre la scala dei dati in modo che siano comparabili tra loro. Molti modelli di machine learning, come il K-nearest neighbors e le reti neurali richiedono che il dato sia normalizzato o standardizzato per performare bene.
Invito il lettore interessato a leggere questa risorsa in inglese che spiega le differenze tra standardizzazione e normalizzazione per saperne di più.
La codifica delle variabili categoriali (detto anche encoding in inglese) viene utilizzata quando si hanno variabili che non sono numeriche, come ad esempio il sesso o il colore degli occhi. La codifica delle variabili categoriali trasforma queste variabili in numeri per poter essere utilizzate dal modello di machine learning.
La gestione degli outlier viene utilizzata per gestire i dati che si discostano molto dal resto dei dati. Questi dati possono influenzare negativamente il modello di machine learning, pertanto vanno gestiti in modo adeguato.
Trasformazione dei dati
La trasformazione dei dati può includere la riduzione della dimensionalità, la selezione delle features e la creazione di nuove features.
La riduzione della dimensionalità viene utilizzata per ridurre il numero di features dei dati. Questo passaggio può essere utile quando si hanno molti dati ma si hanno limitazioni di risorse, come ad esempio il tempo di elaborazione del modello di machine learning. Una delle tecniche più utilizzata è la PCA (Principal Component Analysis).
La selezione delle features viene utilizzata per selezionare le features più importanti dei dati. Questo passaggio può essere utile quando si hanno molte features ma si vuole utilizzare solo un sottoinsieme di esse per addestrare il modello di machine learning.
La creazione di nuove features viene utilizzata per creare nuove features dai dati esistenti. Questo passaggio può essere utile quando si vogliono creare features che non erano presenti nei dati originali ma che possono essere utili per addestrare il modello di machine learning.
Divisione in set di addestramento, validazione e test
La divisione in set di addestramento, validazione e test è un passaggio importante nella preparazione dei dati per il machine learning.
- Set di addestramento (train set): il train set viene utilizzato per addestrare il modello di machine learning. Esso contiene i dati che il modello userà per apprendere le relazioni utili alla predizione
- Set di validazione (validation set): il validation set viene utilizzato per valutare le prestazioni del modello di machine learning durante l'addestramento e per testare i suoi iperparametri
- Set di test (test set): il test set viene utilizzato per valutare le prestazioni del modello di machine learning dopo l'addestramento.
La divisione in train, validation e test set è importante perché consente di valutare le prestazioni del modello di machine learning in modo accurato.
Se si utilizzano tutti i dati per addestrare il modello di machine learning e poi si valutano le prestazioni del modello sui dati stessi, si rischia di ottenere un'immagine distorta delle prestazioni del modello.
Questa consiste nella ripartizione del set di addestramento in porzioni, che vengono usate per valutare il modello in maniera iterativa.
Se vuoi leggere della cross-validazione e come applicarla nella tua codebase, leggi l'articolo qui in basso
Conclusione
In questo articolo abbiamo esaminato i passaggi necessari per preparare i dati per il machine learning.
Abbiamo visto come la raccolta dei dati da fonti affidabili e pertinenti al problema, insieme alla pulizia dei dati, siano fondamentali per garantire la qualità dei dati utilizzati per addestrare il modello di machine learning.
Abbiamo inoltre esaminato il preprocessing dei dati, che include la normalizzazione e la standardizzazione dei dati, la codifica delle variabili categoriche e la gestione degli outlier, nonché la trasformazione dei dati, che può includere la riduzione della dimensionalità, la selezione delle features e la creazione di nuove features.
Infine, abbiamo visto l'importanza della divisione dei dati in train, validation e test set per valutare le prestazioni del modello di machine learning in modo accurato.
La preparazione dei dati per il machine learning richiede un'attenzione particolare ai dettagli, ma svolgere correttamente i passaggi della pipeline di preparazione dei dati può fare la differenza tra un modello di machine learning che funziona e uno che non funziona.
Commenti dalla community"
https://www.diariodiunanalista.it/posts/come-scraperare-un-blog-e-raccogliere-i-suoi-articoli/,"Creare un dataset è una delle prime e più importanti fasi di un progetto di data analytics e machine learning. Come ho già parlato nel mio articolo dove spiego dell'importanza di creare un dataset partendo da zero, qui mi concentro sul condividere con voi lettori una metodica semplice ed efficace per popolare un corpus di dati testuali usando come sorgente dati dei blog online.
La motivazione da parte mia che mi ha convinto a scrivere questo pezzo è che il dato di tipo testuale è sicuramente il più prevalente nel mondo online ed è sicuramente una competenza valida quella di poter attingere a questa pool di dati. In questo vi condividerò come.
Voglio sottolineare la metodica qui presente non è invasiva e in generale non dovrebbe esserci il rischio (se non modificate il codice, ovviamente) di incorrere in penalità.
Ricordatevi di scraperare sempre responsabilmente. Se un sito web dichiara nero su bianco che non vuole essere scraperato, non fatelo. Inoltre, questa metodica è indicata per recuperare articoli presenti nell'HTML - questo significa che se il contenuto è generato via Javascript allora questo non sarà recuperabile. In questo caso bisogna emulare un browser con uno strumento come Playwright o Selenium.
Iniziamo.
Come funziona il software?
Il primo step è quello di recuperare i link dal nostro sito target in modo da poter inviare delle richieste a quei link e recuperare il corpo dell'articolo. Il flusso di dati può essere immaginato così:
- Recuperiamo la lista di URL che vogliamo scraperare
- Per ogni URL facciamo una richiesta GET per recuperare il corpo dell'articolo dall'HTML
- Salviamo il dato in un dizionario Python con due chiavi, URL e articolo
- Salviamo il dizionario in una lista
- Trasformiamo la lista in un dataframe Pandas
Andiamo subito nel punto 1.
Come recuperare una lista di URL che vogliamo scraperare
Per trovare la lista di URL useremo la sitemap.xml, un file presente nella maggior parte dei siti web (gli editoriali ne hanno veramente bisogno dal punto di vista SEO) che ha l'obiettivo di indicare ai motori di ricerca il contenuto del nostro sito web, frequenza di aggiornamento e categorie. Essa spesso si trova in www.sitoweb.it/sitemap.xml. Se così non fosse, possiamo usare il file robots.txt, solitamente allo stesso indirizzo web, che ci indicherà la posizione della sitemap.
Raggiungendo la sitemap vediamo che si presenta così
Quello che vediamo è un file in formato .xml (extensible markup language) che in una gerarchia ad albero comunica con le sue foglie le specifiche di un dato elemento. Ad esempio, il nodo di una URL è tipicamente il nodo loc. Questo è quello più importante perché comunica ai motori di ricerca proprio l'inserimento di nuove URL nel nostro sito.
Va anche specificato che spesso esiste un file chiamato sitemap_index.xml, che invece di puntare alle URL dei singoli articoli / pagine, contiene una lista di altre sitemap che a loro volta puntano a URL specifiche. Ad esempio, potrebbe esserci una sitemap dedicata alle pagine statiche, una alle categorie e una ai post.
Perché partire dalla sitemap?
È una domanda legittima. Essenzialmente abbiamo due strade:
- Atterriamo con il nostro scraper sulla homepage e seguiamo tutte le URL interne al sito (quelle che puntano a risorse / pagine nello stesso sito)
- Sfruttiamo la sitemap che già contiene le URL rilevanti.
La 2. sicuramente sembra la più efficace. Ci sono tuttavia alcuni punti da tenere a mente: nella sitemap potrebbero non esserci tutti gli articoli che in realtà esistono sul sito.
Ricordiamoci che la sitemap è un file controllato dal proprietario del sito web e lui può decidere se rendere una URL pubblica nella sitemap o meno (ci sono anche altre impostazioni che rendono una pagina su un sito irraggiungibile per un motore di ricerca, la sitemap non è una di queste).
Per la maggior parte dei casi, una sitemap di un blog sarà sufficiente per permetterci di creare un corpus di dati testuali per le nostre analisi.
Gli strumenti
Useremo una libreria chiamata Trafilatura sia per recuperare le URL dalla sitemap che per raccogliere i nostri articoli. Questa è una potente libreria che serve a diverse cose, tra le quali localizzare una sitemap all'interno di un sito senza doverla esplicitamente dichiarare, seguire ogni link interno per mappare il sito (qualora la sitemap non fosse disponibile), scaricare ed estrarre il contenuto di una pagina. Trafilatura fa questo e altro e vi consiglio di leggere la documentazione per scoprire tutte le sue potenzialità.
Ad accompagnare Trafilatura c'è Pandas - i dati raccolti verranno inseriti in un DataFrame per un facile utilizzo.
Come optional c'è PyMongo, nel caso volessimo salvare i nostri dati in un database non relazionale.
L'algoritmo
Iniziamo a scrivere del codice.
from trafilatura.sitemaps import sitemap_search
def get_urls_from_sitemap(resource_url: str) -> list:
""""""
Questa funzione recupera una lista di URL da una sitemap con Trafilatura
""""""
urls = sitemap_search(resource_url)
return urls
Importiamo sitemap_search da Trafilatura e scriviamo una funzione che data una URL base di un sito web (come la homepage ad esempio) essa restituisca tutte le URL presenti in sitemap.
Avendo tutte le URL presenti in sitemap per un sito, possiamo ora iniziare a creare il nostro dataset. Trafilatura scarica e estrae i testi così
from trafilatura import fetch_url, extract
def extract_article(url: str) -> dict:
""""""
Estrae un articolo da una URL con Trafilatura
""""""
downloaded = fetch_url(url)
article = extract(downloaded, favor_precision=True)
return article
Con
favor_precision=True chiediamo alla libreria di usare metodi più stringenti per quanto riguarda l'identificazione del corpo centrale dell'articolo. Poiché essa applica delle euristiche, a volte è può sbagliarsi e tirar dentro del contenuto non appartenente a quello centrale, ad esempio intestazioni presenti nella sidebar. Con l'argomento
favor_precision chiediamo una maggiore precisione a scapito di tempi di elaborazione un pochino più lunghi.
Ora mettiamo tutto insieme in una funzione che chiameremo
create_dataset(). Questa funzione riceve come argomento una lista di siti web dalla quale vogliamo prendere gli articoli. Useremo https://www.diariodiunanalista.it e https://fragrancejourney.it, che è un piccolo blog italiano nella nicchia delle profumazioni.
import pandas as pd
from tqdm import tqdm
import time
def create_dataset(list_of_websites: list) -> pd.DataFrame:
""""""
Funzione che crea un DataFrame Pandas di URL e articoli.
""""""
data = []
# usiamo tqdm per creare una progress bar
for website in tqdm(list_of_websites, desc=""Websites""):
# popoliamo urls con tutte le URL del sito web usando la funzione precedente
urls = get_urls_from_sitemap(website)
# creiamo un dizionario python che diventerà poi il nostro dataframe
for url in tqdm(urls, desc=""URLs""):
d = {
'url': url,
""article"": extract_article(url)
}
data.append(d)
# ricordiamoci di rispettare il sito web! Non spammiamo
time.sleep(0.5)
df = pd.DataFrame(data)
# rimuoviamo duplicati se esitono
df = df.drop_duplicates()
# rimuoviamo righe vuote se esistono
df = df.dropna()
return df
Ho inserito un
time.sleep(0.5) così da non pesare eccessivamente sul server del sito web. Provate ad alzare questo timer se incorrete in errori di timeout o simili.
Lanciamo la funzione e vedremo il nostro corpus costruirsi piano piano. La durata del processo dipende da quanti articoli vogliamo recuperare.
Esportiamo il nostro dataset in formato .csv e abbiamo finito! Abbiamo un corpus di dati testuali pronto per essere sottoposto a pre-processing e ad analisi.
I risultati
Diamo una occhiata al nostro dataset
Ora è possibile applicare diverse metodiche analitiche su questo corpus. Sicuramente dovrà essere sottoposto a pre-processing, come rimozioni delle stopword, dei caratteri speciali e della trasformazione in lemmi.
Una applicazione diretta può essere di fare clustering con TF-IDF sui nostri testi. Tutto dipende dal nostro obiettivo. La creatività è il nostro unico limite.
Template del codice
Ecco il codice completo in formato copia-incolla
import time
import pandas as pd
from tqdm import tqdm
from trafilatura.sitemaps import sitemap_search
from trafilatura import fetch_url, extract
def get_urls_from_sitemap(resource_url: str) -> list:
""""""
Funzione che crea un DataFrame Pandas di URL e articoli.
""""""
urls = sitemap_search(resource_url)
return urls
def extract_article(url: str) -> dict:
""""""
Estrae un articolo da una URL con Trafilatura
""""""
downloaded = fetch_url(url)
article = extract(downloaded, favor_precision=True)
return article
def create_dataset(list_of_websites: list) -> pd.DataFrame:
""""""
Funzione che crea un DataFrame Pandas di URL e articoli.
""""""
data = []
for website in tqdm(list_of_websites, desc=""Websites""):
urls = get_urls_from_sitemap(website)
for url in tqdm(urls, desc=""URLs""):
d = {
'url': url,
""article"": extract_article(url)
}
data.append(d)
time.sleep(0.5)
df = pd.DataFrame(data)
df = df.drop_duplicates()
df = df.dropna()
return df
if __name__ == ""__main__"":
list_of_websites = [
""https://www.diariodiunanalista.it/"",
""https://www.fragrancejourney.it/""
]
df = create_dataset(list_of_websites)
df.to_csv(""dataset.csv"", index=False)
Commenti dalla community"
https://www.diariodiunanalista.it/posts/come-strutturare-un-progetto-di-machine-learning/,"Spesso i junior del data science si concentrano sul funzionamento di librerie come Scikit-Learn, Numpy e Pandas. Parecchi MOOC presenti online premono molto su concetti che girano intorno a questi ultimi, tralasciando la componente gestionale di un progetto di data science.
Per quanto un junior possa saperne di algoritmi, librerie e programmazione in generale, il successo di un progetto è anche legato alla sua struttura.
Una struttura confusionaria può impattare significativamente sulle performance dell'analista, che deve orientarsi continuamente tra il mare-magnum di file e TO-DOs. Questo è ancora più enfatizzato se nel progetto sono coinvolte più persone.
In questo articolo vi condividerò il mio boilerplate per la strutturazione di progetti di data science. Questa organizzazione prende spunto dal lavoro di Abhishek Thakur (@abhi1thakur), noto Grandmaster di Kaggle, dal suo libro Approaching Almost Any Machine Learning Problem (lettura molto consigliata).
Mentre molti data scientist utilizzano Jupyter per lavorare ai loro progetti, noi useremo un IDE come VSCode oppure PyCharm. Jupyter e i suoi notebook verranno usati solamente per l'analisi esplorativa e la generazione di grafici. In questo modo possiamo focalizzarci sul creare un template che definiamo plug & play in quanto con piccole modifiche potrà essere adattato a qualsiasi problema di data science.
Struttura del progetto
Vediamo la struttura dei file e delle cartelle. Creiamo una cartella con il nome del nostro progetto - nel nostro caso la chiameremo progetto. In progetto, avremo questa struttura:
La cartella input conterrà i file di sorgente dati e materiale relativo al dataset del progetto.
La cartella src (source in inglese) conterrà invece tutti i file di lavoro vero e proprio.
- train.py: lo script che si preoccuperà di addestrare il modello scelto da tune_model.py. Restituirà un modello salvato su disco in formato pickle.
- predict.py: è lo script da chiamare per effettuare una predizione con il modello generato da train.py
- model_selection.py: in questo script inseriremo il codice che andrà a darci un riferimento per il modello più performante per il nostro dataset.
- tune_model.py: servirà a passare il risultato di model_selection.py alla pipeline di ottimizzazione degli iperparametri
- utils.py: conterrà tutte le nostre funzioni helper che ci serviranno durante il progetto
La cartella models conterrà i modelli che salveremo restituiti da train.py in formato pickle.
Notebooks sarà il nostro ambiente di esplorazione dove inseriremo tutti i nostri notebook .ipynb.
README.md invece sarà il nostro ""manuale di istruzioni"" - qui inseriremo la documentazione necessaria per comunicare il comportamento del nostro software.
Come approcciarsi all'utilizzo del boilerplate
Tipicamente inizio un progetto di data science / ML creando il file README.md. È di fondamentale importanza usare quest'ultimo per documentare ogni step del nostro processo.
Nelle fasi iniziali lo uso come raccoglitore di idee, partendo da un brainstorming, e poi pian piano vado a scremarlo, giungendo alla fine ad avere una documentazione semplice ma dritta al punto.
In parallelo o subito dopo passo ai notebook. Qui esploro il dato e cerco di capire se ci sono particolari condizioni a cui fare attenzione (valori mancanti, pattern particolari, presenza di outlier, e così via).
All'interno del notebook stesso mi dedico sempre delle celle tra i vari snippet di codice per commentare o creare del markdown esplicativo di quello che sto facendo. Inizio a riempire il file utils.py con le varie funzioni che so che mi aiuteranno a livello di script.
Quando sento di aver preso confidenza con il dataset passo a model_selection.py per testare baseline e trovare il modello più performante (leggere questo articolo per la metodologia di scelta del modello).
tune_model.py è strettamente legato a model_selection.py, in quanto il modello selezionato viene fornito direttamente ad una pipeline di ottimizzazione degli iperparametri, spesso effettuata con GridSearchCV oppure con Optuna.
Da qui poi è semplice - train.py si occupa di addestrare il modello ottimizzato e di valutarne la performance, predict.py viene invece chiamato per fare inferenze.
Con questo boilerplate avrete un punto di partenza per strutturare i vostri progetti di data science. Sentitevi liberi di espandere e di modificare la logica di questa struttura per adeguarla alle vostre esigenze.
Commenti dalla community"
https://www.diariodiunanalista.it/posts/come-tokenizzare-e-fare-padding-di-sequenze-in-tensorflow/,"In questo articolo vedremo come usare un corpus di dati e estrarre token e sequenze con padding da usare per addestrare modelli di deep learning attraverso Tensorflow.
Ho già toccato l'argomento in un post precedente dove parlavo di come convertire testi per in tensori per il deep learning, ma in questo caso il focus sarà su come formattare correttamente le sequenze di token per Tensorflow.
Questa metodologia è indispensabile per fornire ai nostri modelli sequenze di token di lunghezza uniforme (padding). Vediamo come.
Il Dataset
Useremo il dataset fornito da Sklearn, 20newsgroups, per avere rapido accesso ad un corpus di dati testuali. A scopo dimostrativo, userò solo un campione di 10 testi.
import numpy as np
from sklearn.datasets import fetch_20newsgroups
# categorie dalle quali prenderemo i nostri dati
categories = [
'comp.graphics',
'comp.os.ms-windows.misc',
'rec.sport.baseball',
'rec.sport.hockey',
'alt.atheism',
'soc.religion.christian',
]
# primi 10 elementi
dataset = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, remove=('headers', 'footers', 'quotes'))
corpus = [item for item in dataset['data'][:10]]
corpus
Su questi testi non applicheremo preprocessing, in quanto la tokenizzazione di Tensorflow rimuove automaticamente la punteggiatura per noi.
Cosa è la tokenizzazione?
In gergo, tokenizzare vuol dire ridurre una frase nei simboli (in inglese, token, per l'appunto) che la formano.
Quindi se abbiamo una frase del tipo ""Ciao, mi chiamo Andrea."" la sua versione tokenizzata sarà semplicemente [""Ciao"", "","", ""mi"", ""chiamo"", ""Andrea"", "".""]. Da notare come la tokenizzazione includa di default la punteggiatura.
Applicare la tokenizzazione è il primo passo per convertire le nostre parole in valori numerici processabili da un modello di machine learning.
Tipicamente è sufficiente applicare .split() su una stringa in Python per effettuare una tokenizzazione semplice. Ci sono però diverse metodologie di tokenizzazione che possono essere applicate. Tensorflow offre una API molto interessante per che permette di customizzare proprio tale logica. La vedremo tra poco.
Cosa è il padding?
Una volta tokenizzata una frase, Tensorflow restituisce valori numerici associati ad ogni token. Questa è chiamata tipicamente word_index, ed è un dizionario formato da parola e indice {word: index}. Ogni parola incontrata viene numerata e tale numero viene usato per identificare quella parola.
Un modello di deep learning vorrà spesso un input di dimensione uniforme. Questo significa che frasi di lunghezza diverse saranno problematiche per il nostro modello. Qui è dove entra il padding in gioco.
Prendiamo due frasi e le loro sequenze di indici (escludendo la punteggiatura):
- Ciao, mi chiamo Andrea: [43, 3, 56, 6]
- Ciao, sono un analista e uso Tensorflow per i miei progetti di deep learning: [43, 11, 9, 34, 2, 22, 15, 4, 5, 8, 19, 10, 26, 27]
Notiamo come la prima sia più corta (4 elementi) della seconda (14 elementi). Se dessimo in pasto al nostro modello le sequenze in questo modo, questo restituirebbe degli errori. Vanno quindi normalizzate le sequenze in modo che abbiano la stessa lunghezza.
Applicare padding (in italiano possiamo tradurlo con riempimento, imbottitura) su una sequenza di significa usare un valore numerico predefinito (solitamente 0) per portare le sequenze più corte alla stessa lunghezza della sequenza dalla lunghezza massima. Quindi avremo questo:
- Ciao, mi chiamo Andrea: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 43, 3, 56, 6]
- Ciao, sono un analista e uso Tensorflow per i miei progetti di deep learning: [43, 11, 9, 34, 2, 22, 15, 4, 5, 8, 19, 10, 26, 27]
Ora entrambe le sequenze hanno la stessa lunghezza. Possiamo decidere come fare padding, se inserire gli zeri prima o dopo la sequenza, direttamente con pad_sequences di Tensorflow.
Applicazione della tokenizzazione e padding
Applichiamo ora con il codice qui sotto la tokenizzazione e il padding al nostro corpus di dati dopo aver estratto le frasi da esso.
# creiamo una lista vuota che conterrà le nostre frasi
sentences = []
# iteriamo nel nostro corpus
for text in corpus:
# splittiamo le frasi sul punto
splitted_text = text.split(""."")
# inseriamo nella lista ogni frase estratta,
for sentence in splitted_text:
sentences.append(sentence)
# importiamo Tokenizer e pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
# inizializziamo il tokenizer con un out_of_vocabulary token
tokenizer = Tokenizer(oov_token=""<OOV>"")
# generiamo il dizionario parola - indice andando a fare fit sul tokenizer
tokenizer.fit_on_texts(sentences)
# generiamo e facciamo padding sulle sequenze
sequences = tokenizer.texts_to_sequences(sentences)
padded = pad_sequences(sequences, padding='post')
Da notare come nel codice sia presente post come modalità di padding. Questo significa che gli zeri vengono inseriti post-sequenza, quindi allungando il vettore dopo il valore tokenizzato e non prima.
Ed ecco come vengono applicati tokenizzazione e padding sui testi per fornirli ad una rete neurale su Tensorflow. Se avete domande o dubbi lasciate un commento nel box in fondo alla pagina. Alla prossima!
Commenti dalla community"
https://www.diariodiunanalista.it/posts/cosa-e-data-leakage/,"Il data leakage è senza alcun dubbio la bestia nera e feroce che preda sul data scientist, giovane o senior che sia.
È quel fenomeno che può colpire proprio tutti - anche professionisti con anni di esperienza nel settore.
Insieme all'over/underfitting rappresenta la causa principale di fallimento di progetti di machine learning che vanno in produzione.
Ma perché il data leakage miete così tante vittime?
Perché anche dopo molti esperimenti e valutazioni nella fase di sviluppo, i nostri modelli possono fallire in maniera spettacolare in uno scenario di produzione.
Evitare il data leakage non è cosa semplice. Ora capirai perché leggendo il continuo dell'articolo 👇
Esempi di data leakage
Siamo degli sviluppatori di IA applicata e veniamo assunti da una azienda che fabbrica giocattoli per bambini in serie.
Il nostro compito è quello di creare un modello di machine learning per identificare se un giocattolo sarà oggetto di richiesta di rimborso entro 3 giorni dalla sua vendita.
Otteniamo i dati dalla fabbrica, sotto forma di immagini che catturano il giocattolo prima dell'inscatolamento.
Utilizziamo queste immagini per addestrare il nostro modello che performa molto bene in cross validazione e sul test set.
Consegnamo il modello e per il primo mese il cliente riporta solo il 5% di richieste di rimborso di giocattoli difettati.
Al secondo mese ci prepariamo per il retraining del modello. La fabbrica ci manda altre fotografie, che andiamo ad usare per espandere il dataset di addestramento iniziale.
Di nuovo, il modello performa bene in fase di cross-validazione e test.
Stavolta però riceviamo una comunicazione che i clienti stanno facendo richiesta e che di tali richieste, il 90% fanno riferimento ad un giocattolo difettato.
Iniziamo a guardare le foto....e notiamo che le foto inviate dal cliente nell'ultima batch mostrano i giocattoli oggetto di rimborso durante il primo mese.
Le nuove fotografie fornite dalla fabbrica includono involontariamente l'informazione riguardante la richiesta di rimborso entro 3 giorni dalla vendita del giocattolo.
In pratica il cliente ci ha mandato immagini selezionate dopo che la richiesta di rimborso è stata effettuata, quindi catturano caratteristiche specifiche dei giocattoli che sono stati restituiti. Questo può includere danni visibili o difetti evidenti che sono stati rilevati dal cliente e che hanno portato alla richiesta di rimborso.
Di conseguenza capiamo che il modello è molto abile a identificare specifici difetti, mostrando quindi una alta performance in fase di sviluppo ma non di produzione.
Tocca chiamare il cliente, spiegargli la situazione e pulire il dataset di addestramento.
Ti piace quello che stai leggendo?
Iscriviti al blog per rimanere sempre aggiornato sui contenuti di settore. No pubblicità. No spam.
Solo contenuto utile per la tua carriera nel ML e DS.
Puoi rimuovere l'iscrizione quando vuoi seguendo il link nella email.
Cause comuni di data leakage
Vediamo ora alcune delle cause più frequenti che portano al data leakage.
1. Sequenze temporali divise casualmente invece che sul tempo
Normalmente ci viene insegnato che dividere i dati set di addestramento, validazione e test in modo casuale sia la scelta corretta.
Molto spesso però i nostri dati possono essere correlati su base temporale, il che significa che la variabile temporale influenza la distribuzione delle nostre etichette.
Prendiamo ad esempio una serie temporale che viene dal mercato azionario.
Se dividessimo in modo casuale tale dataset, il data leakage si manifesterebbe al 100%.
Questo perché dati di giorni successivi verrebbero casualmente mischiati al dataset di addestramento. Il nostro modello sarebbe esposto alle etichette ""corrette"" senza doverle apprendere.
È un po' come se un ragazzo a scuola avesse le risposte corrette al test che sta facendo. Prestazioni alte, ma conoscenza molto bassa.
Per evitare il problema, è utile dividere i dati in base al tempo: ad esempio se abbiamo dati per un mese, addestriamo il modello sui primi 20 giorni, e testiamo sui restanti 10, in maniera sequenziale.
2. Trasformare i dati prima di dividerli
Questa è una delle cause più comuni tra i neofiti della data science.
Ecco come si manifesta questo errore
from sklearn import model_selection
from sklearn import preprocessing
# ...
scaler = preprocessing.StandardScaler()
data = scaler.fit_transform(data)
train, test = model_selection.train_test_split(data, test_size=0.2, random_state=42)
# ...
L'errore è quello di scalare tutti i dati prima di dividerli in set di addestramento e test.
In questo caso, l'oggetto
scaler richiede di conoscere la media e la deviazione standard del dataset alla quale è applicato.
Fornendogli tutto il dataset, questo avrà immagazzinato e usato anche informazioni dal test set, che andranno a spostare la media e la deviazione standard.
Per risolvere questo problema, dividi sempre i tuoi dati prima di applicare una trasformazione come lo scaling.
3. Riempire i dati mancanti con informazioni provenienti dal test set
Un po' simile al punto sopra, ma da un altro angolo.
Un metodo comune per imputare i dati mancanti di una colonna è quella di riempire le celle con la media o mediana di tutti i dati presenti nella colonna.
Se la media viene calcolata anche sui valori che appartengono al test set, stiamo generando leakage nel training set.
Anche qui, dividiamo i nostri dati prima di applicare l'imputazione.
4. Mancata rimozione dei duplicati
Se abbiamo dei record duplicati nel nostro dataset, c'è il rischio che alcuni di questi possano apparire sia in training che in test set.
Da qui, il leakage. Il nostro modello sarà chiaramente abile nel predire il valore di tali duplicati, riducendo l'errore di predizione considerevolmente (ma in maniera errata).
Per evitare questo errore, occorre rimuovere i duplicati prima di effettuare il train-test split.
Se per caso facciamo oversampling, cioè creare dei duplicati artificiali per addestrare un modello su un dataset sbilanciato, allora occorre farlo dopo aver splitatto i dati.
5. Processo di generazione dei dati sbagliato
Come l'esempio di cui sopra della fabbrica di giocattoli.
A volte il leakage proviene da come vengono generati e consegnati i dati su cui dobbiamo addestrare il modello.
Non c'è modo per evitare questo scenario. Dobbiamo solo essere vigili, fare domande e non dare nulla per scontato, soprattutto quando non siamo in controllo del processo di generazione dei dati proprio come nel caso della fabbrica di giocattoli.
In generale, il consiglio è sempre di controllare i dati se questi non vengono processati / consegnati da un team di data science
Conclusioni
Hai imparato cosa è il data leakage e perché è così difficile da gestire, anche per data scientist esperti.
Le sue insidie si nascondono nei dettagli, che possono essere facilmente ignorati durante la scrittura di codice.
Gli esempi che ti ho mostrato dovrebbero aiutarti a valutare se il tuo progetto è vittima di leakage.
Di solito, se noti performance molto alte in fase di sviluppo, controlla sempre per il leakage.
Non vogliamo mai che il nostro modello fallisca in produzione!
Alla prossima,
Andrea
Commenti dalla community"
https://www.diariodiunanalista.it/posts/cosa-e-il-machine-learning/,"Spiegare cosa sia il machine learning è relativamente semplice, ma il discorso va tarato in base all'interlocutore. Alcuni termini possono essere interpretati in modo diverso dipendentemente dal contesto, quindi è giusto ricercare un vocabolario quanto più generale possibile.
Il mio discorso sarà tarato per i neofiti del campo: persone che sanno poco di analisi quantitativa ma che vogliono iniziare ad approcciarsi alla materia per diversi motivi. In particolare, spero che questo articolo possa aiutare giovani figure ad orientarsi nello spazio universitario e lavorativo.
Che cos'è il Machine Learning?
Il termine machine learning si traduce in italiano apprendimento automatico ed è un ramo dell'intelligenza artificiale, che a sua volta è una branchia dell'informatica.
Nel machine learning si utilizzano i dati numerici per addestrare computer a completare dei compiti specifici. Il risultato è un algoritmo che a sua volta utilizza un modello del fenomeno per trovare la soluzione ad un problema. Il termine addestrare è fondamentale ed è l'attività che caratterizza maggiormente il campo. Successivamente vedremo nel dettaglio cosa significa.
La differenza tra un software che applica una regola specifica per risolvere un compito (ad esempio, se una frase contiene la parola ""casa"", creare una categoria ""immobile"" in un foglio di calcolo Excel) e un algoritmo di machine learning è che quest'ultimo non deve essere esplicitamente programmato per risolverlo correttamente.
Un algoritmo di machine learning utilizza un modello per inferire matematicamente delle regole nei dati tale che quest'ultimo sia in grado di trovare una soluzione generica al problema.
Diamo quindi una definizione generale:
Il machine learning è un ramo dell'intelligenza artificiale che permette a software di utilizzare dati numerici per trovare soluzioni a specifici compiti senza essere esplicitamente programmati per farlo.
Queste soluzioni possono essere più o meno accurate, e difficilmente si arriva a giungere performance paragonabili a quelle umane. Questo non deve però destare preoccupazioni: la maggior parte dei problemi affrontati col machine learning offre performance che soddisfano i casi d'uso. Ci basti pensare alle raccomandazioni dei prodotti su Amazon oppure all'identificazione di spam di Gmail. Vedremo degli esempi a breve.
Che si intende per algoritmo, modello e performance?
Chiariamo alcuni concetti prima di continuare.
Un algoritmo non è altro che una serie di istruzioni seguite da un computer. È sicuramente una parola molto inflazionata al momento (algoritmo di Facebook, algoritmo di Twitter e così via), ma di fatto è un concetto molto semplice.
Un modello è un software che viene inserito nell'algoritmo e che ci serve per trovare la soluzione al nostro problema. Poiché spesso non conosciamo la vera soluzione, queste vengono chiamate predizioni.
Prima di essere utilizzato per risolvere problemi importanti, un modello viene sottoposto ad una serie di test che ne valutano la performance. Questa può essere calcolata solo se possediamo un set di dati che ci permette di confrontare la realtà con la predizione del modello. Se la performance ci soddisfa allora useremo il modello, altrimenti decidere di addestreremo nuovamente oppure di usarne un altro.
Cosa significa ""addestrare un modello""?
Come abbiamo menzionato, ad un modello vengono forniti dati numerici per trovare delle soluzioni generalizzabili. L'atto di mostrare questi dati al modello e di permettere a quest'ultimo di apprendere si chiama training (addestramento).
Durante il training, l'algoritmo di apprendimento cerca di imparare i pattern che legano i dati insieme partendo da certe ipotesi. Ad esempio, gli algoritmi probabilistici fondano il loro funzionamento proprio nel dedurre le probabilità che un evento accada in presenza di certi dati.
L'addestramento è controllato attraverso gli iperparametri, che ci permettono di regolare/tarare come il modello interpreta i dati e molto altro. Ogni modello ha i suoi iperparametri che variano in base alla logica di funzionamento (ce ne sono veramente tanti). Uno degli aspetti più importanti del lavoro di un data scientist è proprio trovare quali siano gli iperparametri giusti per un determinato modello. Spesso è una vera e propria ricerca che può durare anche molto tempo.
Una volta tarato il modello e addestrato, occorre calcolare le performance per comprendere se siamo soddisfatti o meno delle sue predizioni. Se lo siamo, la fase di training si considera conclusa e si procede con le seguenti fasi di sviluppo.
Un modello addestrato si comporta come qualsiasi altro software: riceve input e restituisce output. In input ci saranno i dati del fenomeno, in output invece le predizioni.
Tipi di apprendimento automatico
Il machine learning può essere diviso fondamentalmente in tre aree: apprendimento supervisionato, non supervisionato e per rinforzo. Vediamo come l'addestramento di questi algoritmi differisce proprio in base alla sottocategoria alla quale appartiene.
Apprendimento supervisionato
Gli algoritmi che si basano sull'apprendimento supervisionato hanno bisogno di essere addestrati su dati che al loro interno contengono la risposta esatta al problema, in modo tale da comprendere quale sia la relazione tra quest'ultima ed il fenomeno.
Ad esempio, un dataset per un task supervisionato potrebbe essere quello che contiene i dati di proprietà immobiliari e il loro prezzo. Se volessimo predire il prezzo di un immobile, l'algoritmo dovrebbe essere addestrato a comprendere l'associazione tra caratteristiche della casa, come numero di camere, metri calpestabili e altro e proprio il prezzo.
In gergo diciamo che le caratteristiche di un fenomeno facciano parte del feature set (denotato con \( X \) , variabile aleatoria indipendente). La variabile da predire è la variabile dipendente (perché dipende dalle caratteristiche, per l'appunto), denotata tipicamente con \( y \).
Riassumiamo con una frase cosa è l'apprendimento supervisionato.
L'apprendimento supervisionato è una sottocategoria di apprendimento automatico che racchiude algoritmi che hanno bisogno di dati in forma X e y. X è l'insieme delle caratteristiche del fenomeno, y è l'osservazione che vogliamo predire.
Un algoritmo supervisionato apprende la relazione tra X e y ed è in grado di predire una nuova y data una X non appartenente al set di addestramento.
L'ultima parte della definizione potrebbe essere un po' complicata, quindi cerchiamo di capire meglio cosa si intende X non appartenente al set di addestramento.
L'obiettivo di un algoritmo di machine learning supervisionato è quello di predire qualcosa date delle caratteristiche di un fenomeno. Durante il training, un modello predittivo impara le relazioni tra queste e poi viene valutato.
Usando una metafora,
un modello predittivo non è che un bambino a scuola che studia per il suo piccolo test. Durante le esercitazioni (training), il bambino ha accesso alle risposte corrette ed è quindi in grado di perfezionare il suo apprendimento. Al test finale, il bambino sarà sottoposto a delle domande senza avere accesso alla soluzione corretta.
X (domande al test finale) non fa parte del set di addestramento (domande di esercitazione), e quindi il bambino (modello predittivo) dovrà trovare la soluzione (y) più precisa possibile in base all'apprendimento al quale è stato sottoposto.
Fanno parte dell'apprendimento supervisionato gli algoritmi di regressione (predizione di un valore numerico) e classificazione (predizione di una categoria).
Apprendimento non supervisionato
In questo caso i nostri algoritmi non necessitano di avere accesso alla risposta corretta nel nostro dataset, e quindi necessitano solo di un feature set X.
Com'è possibile? Beh perché la logica di questi algoritmi è completamente diversa. Non tutti i modelli di machine learning devono comportarsi come il bambino nella metafora. Infatti, gli algoritmi di apprendimento non supervisionato cercano di scoprire pattern nascosti nei dati per raggruppare, separare o manipolare i dati in qualche modo.
La bellezza di questi algoritmi è che non necessitano dell'intervento umano per fare il loro lavoro. Basta passare ad uno di questi i nostri dati e il gioco è fatto.
Fanno parte dell'apprendimento non supervisionato gli algoritmi di clustering (raggruppamento), anomaly e signal detection (identificazione di anomalie e di segnale) e quelli di riduzione della dimensionalità.
Ci sono anche algoritmi che funzionano in parte in maniera supervisionata e parte in maniera non supervisionata, ma questi sono tipicamente algoritmi ad-hoc creati da grandi aziende o da team molto grandi.
Apprendimento per rinforzo
L'apprendimento per rinforzo, chiamato anche reinforcement learning, viene spesso considerato un argomento ostico e separato dai due menzionati. In realtà il concetto è molto semplice: programmiamo un software, chiamato agente, per apprendere come completare un certo compito (task) in uno specifico ambiente e offriamo ricompense o punizioni in base a come performa.
La logica è di premiare l'agente quando raggiunge il risultato oppure punirlo quando fa qualcosa che lo allontana dal raggiungimento dello stesso. In maniera ciclica, il modello impara le regole per massimizzare la funzione di fitness, cioè quella funzione matematica che definisce la ricompensa, e minimizza l'errore, cioè la punizione. Questo permette un apprendimento simile a quello umano: quello basato sul trial and error.
Sentdex è un esponente del machine learning su YouTube. Consiglio altamente di seguire il suo canale e di guardare questa playlist dove programma un algoritmo di RF a giocare una partita di Starcraft II.
Un altro video particolarmente efficace a comunicare la potenza del reinforcement learning è questo
Esempi di applicazione del machine learning
Per consolidare la nostra interpretazione di cosa sia il machine learning, vediamo alcuni esempi pratici di come viene usato nella vita di tutti i giorni. Ognuno di questi casi d'uso è stato messo in atto da un team di data science, e se scegliete di lavorare nel campo anche voi potreste lavorare su progetti del genere!
Identificazione anomalie
Avete presente quando una banca nota una transazione anomala e vi chiede il permesso più volte per essere sicuri che siete voi effettivamente ad autorizzarla? Ecco. Dietro questi meccanismi ci sono algoritmi di anomaly detection che utilizzando i dati comportamentali riescono a cogliere comportamenti che divergono da quelli usuali. Estremamente utili per bloccare una transazione non autorizzata nel contesto bancario, altrettanto utile durante il monitoraggio di fenomeni naturali, come con i terremoti e uragani.
Classificazione di immagini e di testi
VI siete mai chiesti come fa l'iPhone a sfruttare il nostro volto per funzionalità come FaceID? Anche se non è l'unico meccanismo coinvolto, Apple utilizza il machine learning per classificare le immagini che provengono dalla nostra fotocamera. In questo caso, la classificazione è binaria: presenza o meno di un volto riconosciuto nell'immagine.
Gmail ha uno degli algoritmi anti-spam più famosi. Basta aprire il nostro account email e vedere la casella dello spam piena di email dannose. Come fa Google a sapere che una email è potenzialmente dannosa e piazzarla nella casella spam senza il nostro intervento? Attraverso algoritmi di machine learning precisamente addestrati e tarati per classificare il contenuto testuale delle nostre email.
Come già menzionato, le performance non di un modello non devono essere perfette per fornire valore concreto agli utenti. Infatti a volte potremmo trovarci a dover piazzare manualmente delle email spam nella casella appropriata.
Previsione dei prezzi
Il machine learning permette di prevedere valori numerici, come il prezzo di qualcosa. Potrebbe sembrare una magia, ma nel settore immobiliare le aziende utilizzano algoritmi di machine learning per prevedere il prezzo di immobili e di conseguenza affinare le strategie di compravendita e ottenere un vantaggio competitivo.
Contenuti e UX personalizzata
Alcuni siti web come Amazon tracciano i nostri comportamenti di acquisto e di interazione. Questo permette loro di addestrare modello di machine learning in grado di comprendere cosa vogliamo comprare e mettere davanti a noi quelle possibili scelte.
Amazon infatti ci propone spesso prodotti che ""potrebbero piacerci"" in base ad algoritmi di raccomandazione e di clustering. Implementare soluzioni del genere non è semplice ma offrire una esperienza del genere ai nostri clienti potrebbe veramente fare la differenza.
Condivido qui un video che elenca altre applicazioni del machine learning
Come si lavora nel campo del machine learning?
Per lavorare nel campo del machine learning occorre avere conoscenze di informatica, matematica e statistica. Tanto più specifiche sono queste conoscenze, tanto più saranno le nostre possibilità di trovare un lavoro ben pagato e soddisfacente. Infatti, il data scientist, che è la figura principale che è coinvolta in questo campo, lavora proprio all'intersezione di queste tre discipline.
Un data scientist fa il suo lavoro principalmente scrivendo codice, solitamente in Python o R. Per questo motivo deve possedere buone conoscenze di logiche di sviluppo software, strutture dati e algoritmi.
Le aree della matematica più importanti sono sicuramente quelle dell'algebra lineare, che permette al data scientist di sfruttare le proprietà e operazioni su matrici, dell'analisi matematica, con lo studio di funzione e della loro ottimizzazione e del calcolo della probabilità.
Può sembrare molto complesso essere un data scientist, ma avere conoscenza specifica dell'industria dove si vuole lavorare è ancora più importante.
Infatti non serve conoscere a menadito tutte le nozioni delle aree menzionate, ma basta conoscere le basi e aiutare l'azienda o il cliente a cogliere le opportunità più redditizie nel settore dove opera.
Esistono oggi corsi di studi universitari che preparano giovani studenti a lavorare nel settore del data science. I loro curriculum sono variegati, ma tutti coprono le queste discipline.
Per gli autodidatti invece, esistono corsi online estremamente validi per iniziare e consolidare le conoscenze necessarie per lavorare nel settore.
A tema machine learning puro, Coursera offre i corsi più impattevoli sulla carriera di un giovane studente che vuole iniziare il percorso.
Non posso non condividere il corso di Andrew Ng su Coursera di introduzione al machine learning. È sicuramente una delle prime tappe da smarcare prima di intraprendere il viaggio profondo nel mondo dei dati.
Commenti dalla community"
https://www.diariodiunanalista.it/posts/cosa-e-la-cross-validazione-nel-machine-learning/,"Il concetto di cross-validazione estende direttamente da quello dell'overfitting, trattato nel mio precedente articolo.
La cross-validazione è una delle tecniche più efficaci proprio per evitare l'overfitting e per comprendere bene le performance di un modello predittivo.
Quando ho parlato di overfitting, ho diviso i miei dati in training e test set. Il training set ci è servito a ad addestrare il modello, il test set a valutare le sue performance. Ma questa metodica è sbagliata e non va applicata in casi reali. Questo perché possiamo indurre overfitting nel test set se addestriamo a lungo il nostro modello finché non troviamo la configurazione corretta.
Questo concetto prende il nome di data leakage ed è uno dei problemi più comuni e gravi che si possano fare nel campo. Infatti, se addestrassimo il nostro modello a performare bene sul test set, questo poi sarebbe ""adeguato"" solo per quel test set.
Che si intende per configurazione? Ogni modello è caratterizzato da una serie di iperparametri. Segue definizione
un iperparametro del modello è una configurazione esterna ad esso il cui valore non può essere stimato dai dati. Modificare un iperparametro modifica di conseguenza il comportamento del modello sui nostri dati e può migliorare o peggiorare le nostre performance.
Ad esempio un albero
tree.DecisionTreeClassifier di Sklearn, ha
max_depth come iperparametro che gestisce la profondità dell'albero. Modificare questo iperparametro modifica le performance del modello, in bene o in peggio. Non possiamo sapere il valore ideale di max_depth se non sperimentando. Oltre a max_depth, l'albero decisionale possiede molti altri iperparametri.
Quando selezioniamo il modello da usare sul nostro dataset, dobbiamo poi comprendere quali sono le configurazioni di iperparametri migliori. Questa attività si chiama tuning degli iperparametri.
Trovata la configurazione migliore, si porta il modello migliore, con la configurazione migliore, nel mondo ""reale"" - vale a dire il test set che è formato da dati che il modello non ha mai visto prima.
Al fine di testare la configurazione senza passare direttamente al test set, introduciamo un terzo set di dati, chiamato di validazione.
Il flusso generale è questo:
- Addestriamo il modello sul training set
- Testiamo le performance della configurazione attuale sul set di validazione
- Se e solo se siamo soddisfatti delle performance sul set di validazione, allora testiamo sul test set.
Ma perché complicarci la vita inserendo un ulteriore set per valutare le performance? Perché non usare la classica divisione training-test set?
Il motivo è semplice ma è estremamente importante.
Fare machine learning è un processo iterativo.
Per iterativo si intende che un modello può e dev'essere valutato più volte con configurazioni diverse per capire quale è la condizione più performante. Il set di validazione ci permette proprio di testare diverse configurazioni e selezionare quella migliore per il nostro scenario, senza il rischio di overfitting.
Ma c'è un problema. Dividendo il nostro dataset in tre parti andiamo a ridurre il numero di esempi disponibili al nostro modello per l'addestramento. Inoltre, poiché la divisione usuale è 50-30-20, i risultati del modello potrebbero dipendere casualmente da come i dati sono stati distribuiti nei vari set.
La cross-validazione risolve questo problema, andando a rimuovere il set di validazione dall'equazione e preservando il numero di esempi disponibili al modello per l'apprendimento.
Cosa è la cross-validazione?
La cross-validazione è uno dei concetti più importanti nel machine learning. Questo perché ci permette di creare un modello in grado di generalizzare, cioè in grado di creare predizioni coerenti anche su dati non appartenenti al set di addestramento.
Un modello in grado di generalizzare è un modello utile, potente.
Fare cross-validazione significa dividere i nostri dati di addestramento in diverse porzioni e testare il nostro modello su parti alcune di queste parti. Il test set continua ad essere usato per la valutazione finale, mentre le performance del modello vengono valutate sulle porzioni generate dalla cross-validazione. Questa metodica viene chiamata K-Fold, che vedremo meglio in dettaglio tra poco.
In basso una immagine che riassume questo detto finora.
Cross-validazione K-Fold
La cross-validazione può essere fatta in modi diversi, ma ogni metodo è adatto ad uno scenario diverso. In questo articolo vedremo la cross-validazione K-Fold, che è sicuramente la tecnica di cross-validazione più popolare. Altri varianti popolari sono la cross-validazione stratificata e quella basata su gruppi.
Il training set viene diviso in k-fold (leggiamo ""porzioni"") e il modello viene addestrato su
k-1 porzioni. La porzione rimanente viene usata per valutare il modello.
Il tutto avviene nel cosiddetto cross-validation loop - il ciclo di cross-validazione. Vediamo una immagine presa da Scikit-learn.org che mostra questo concetto chiaramente
Dopo aver interato attraverso ogni split, avremo come risultato finale la media delle performance. Questo aumenta la validità delle performance, in quanto un modello ""nuovo"" viene addestrato su ogni porzione del dataset di addestramento. Avremo quindi un punteggio finale che riassume la performance del modello in tanti step di validazione - una metodica molto affidabile rispetto alla performance in una singola iterazione!
Andiamo a scomporre il processo:
1. Randomizzare ogni riga del dataset
2. Dividere il dataset in k porzioni
3. Per ogni gruppo
1. Creare una porzione di test
2. Allocare il restante all'addestramento
3. Addestrare il modello e valutarlo sui menzionati set
4. Salvare la performance
4. Valutare le performance generali prendendo la media dei punteggi alla fine del processo
Il valore di k è tipicamente 5 o 10, ma si può usare la regola di Sturges per stabilire un numero più preciso di split
\[ numero\_di\_split = 1 + \log_2{(N)} \]
Il loop di cross-validazione
Ho poco fa menzionato il loop di cross-validazione. Andiamo più in profondità in questo concetto che è fondamentale ma spesso trascurato dai giovani analisti.
Fare cross-validazione, di per sé, è già molto utile. Ma in alcuni casi è necessario andare oltre e testare nuove idee e ipotesi per migliorare ulteriormente il proprio modello.
Tutto questo va fatto all'interno del ciclo di cross-validazione, che è il punto 3 del flusso citato sopra.
Ogni esperimento dev'essere effettuato all'interno del loop di cross-validazione.
Poiché la cross-validazione ci permette di addestrare e testare il modello più volte e tirare le somme alla fine con una media, bisogna inserire tutte le logiche che vanno a modificare il comportamento del modello proprio all'interno della cross-validazione. Non fare ciò rende impossibile misurare l'impatto delle nostre ipotesi.
Vediamo ora degli esempi.
Implementare la cross-validazione K-Fold in Python
Usando Sklearn, vediamo un template per applicare la cross-validazione in Python. Useremo Sklearn per generare un dataset fantoccio per un task di classificazione e useremo la accuratezza e il ROC-AUC score per valutare il nostro modello.
Alla fine otterremo un punteggio ROC-AUC medio attraverso tutti gli split.
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestClassifier
from sklearn import datasets
from sklearn import metrics
# creiamo un dataset per un task di classificazione
X, y = datasets.make_classification(n_samples=2000, n_features=20, n_classes=2, random_state=42)
# creiamo l'oggetto KFold applicando la regola di Sturges
sturges = int(1 + np.log(len(X)))
kf = KFold(n_splits=sturges, shuffle=True, random_state=42)
fold = 0
aucs = []
# QUESTO È IL LOOP DI CROSS-VALIDAZIONE!
for train_idx, val_idx, in kf.split(X, y):
# l'oggetto kf genera gli indici e i valori per le rispettive X e y, creando il set di validazione su cui testare il modello nello split.
X_tr = X[train_idx]
y_tr = y[train_idx]
X_val = X[val_idx]
y_val = y[val_idx]
# ----
# applicare qui le ipotesi che vogliamo testare
# ...
# ----
# qui addestriamo il modello
clf = RandomForestClassifier(n_estimators=100)
clf.fit(X_tr, y_tr)
# creiamo le predizioni e salviamo lo score nella lista aucs
pred = clf.predict(X_val)
pred_prob = clf.predict_proba(X_val)[:, 1]
acc_score = metrics.accuracy_score(y_val, pred)
auc_score = metrics.roc_auc_score(y_val, pred_prob)
print(f""======= Fold {fold} ========"")
print(
f""Accuracy on the validation set is {acc_score:0.4f} and AUC is {auc_score:0.4f}""
)
# aggiorniamo il valore di fold così possiamo stampare il progresso
fold += 1
aucs.append(auc_score)
general_auc_score = np.mean(aucs)
print(f'\nOur out of fold AUC score is {general_auc_score:0.4f}')
I risultati finali potrebbero cambiare data la natura stocastica (leggere casuale) di alcuni processi.
Conclusione
La cross-validazione è il primo, essenziale step da considerare quando si fa machine learning.
Ricordate sempre: se vogliamo fare feature engineering, aggiungere logiche o testare altre ipotesi - splittate sempre prima i dati con KFold e applicate tali logiche nel loop di cross-validazione.
Se abbiamo un buon framework di cross-validazione con dati di validazione rappresentativi della realtà e di quelli di addestramento, allora possiamo creare buoni modelli di machine learning, altamente generalizzabili.
Commenti dalla community"
https://www.diariodiunanalista.it/posts/cosa-e-la-grid-search-e-come-applicarla-in-python-con-sklearn/,"Il machine learning è una branca dell'intelligenza artificiale che si occupa di creare modelli in grado di effettuare previsioni su dati non visti in precedenza.
Uno dei primi passi per costruire un modello di machine learning è la scelta della giusta architettura e dei parametri del modello.
Questi parametri, chiamati iperparametri, sono settati a priori e non vengono appresi durante il processo di training. I valori degli iperparametri possono avere un impatto significativo sulle prestazioni del modello.
Sta allo sviluppatore trovare un set di iperparametri che massimizzano le performance del modello tale che questo possa generalizzare correttamente su dati non visti.
Tuttavia, trovare la combinazione ottimale di valori può essere una sfida, soprattutto quando ci sono molti iperparametri da considerare. In questi casi, la grid search (ricerca a griglia) può essere una soluzione efficace per risolvere il problema.
La grid search è una tecnica utilizzata nel machine learning per trovare i migliori iperparametri di un modello e consiste nel definire una griglia di valori per ogni iperparametro e testare tutte le possibili combinazioni di valori.
In questo modo è possibile trovare i valori di iperparametri che massimizzano le prestazioni del modello.
Nella pipeline di machine learning, applicare la grid search fa parte della fase chiamata hyperparameter tuning.
- Cosa è la grid search e come funziona
- Come usare Sklearn per applicare la grid search
Iniziamo!
Ti piace quello che stai leggendo?
Iscriviti al blog per rimanere sempre aggiornato sui contenuti di settore. No pubblicità. No spam.
Solo contenuto utile per la tua carriera nel ML e DS.
Puoi rimuovere l'iscrizione quando vuoi seguendo il link nella email.
Come funziona la grid search
Partiamo da un esempio per comprendere come funziona la grid search. Useremo la famosa libreria sklearn in Python per testare la logica di una grid search su un albero decisionale.
Andando sulla documentazione ufficiale di Sklearn per l'oggetto
DecisionTreeClassifier troviamo una lista di iperparametri che questo accetta e che noi possiamo scegliere.
Questi e molti altri sono disponibili da visionare al link della documentazione.
Vediamo come l'iperparametro
criterion possa assumere diversi valori (""gini"", ""entropy"", etc.). La scelta di uno di questi valori potrà influenzare le prestazioni del modello.
Il parametro
max_depth è spesso oggetto di grid search poiché influenza molto il comportamento dell'albero decisionale. Questo rappresenta la profondità massima di estensione dell'albero prima di giungere ad una previsione. Stesso discorso per
min_samples_split, che definisice quanti campioni occorrono per creare un nodo nella struttura dell'albero.
La griglia potrebbe essere la seguente:
parametri = {
'max_depth': [2, 3, 4, 5, 10, 20],
'min_samples_split': [2, 3, 4, 5, 10, 20]
}
La grid search testerebbe tutte le possibili combinazioni di valori nella griglia, ad esempio:
max_depth = 2, min_samples_split = 2
max_depth = 2, min_samples_split = 3
max_depth = 2, min_samples_split = 4
...
max_depth = 5, min_samples_split = 4
max_depth = 5, min_samples_split = 5
...
Per ogni combinazione di valori, il modello viene addestrato e valutato utilizzando una metrica di valutazione, come l'accuratezza o l'F1-score. Alla fine della grid search, si selezionano i valori di iperparametri che massimizzano la metrica di valutazione.
Ti piace quello che stai leggendo?
Iscriviti al blog per rimanere sempre aggiornato sui contenuti di settore. No pubblicità. No spam.
Solo contenuto utile per la tua carriera nel ML e DS.
Puoi rimuovere l'iscrizione quando vuoi seguendo il link nella email.
Come applicare la grid search in Python con Sklearn
In Python, la grid search può essere facilmente applicata proprio grazie a
Sklearn. Sklearn fornisce la classe
GridSearchCV che implementa la grid search.
Ecco un esempio di come utilizzare la classe GridSearchCV per ottimizzare gli iperparametri di un albero di decisione in Python sfruttando il dataset Iris:
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.datasets import load_iris
iris = load_iris()
X = iris.data
y = iris.target
param_grid = {'max_depth': [2, 3, 4, 5, 10, 20],
'min_samples_split': [2, 3, 4, 5, 10, 20]}
model = DecisionTreeClassifier()
grid_search = GridSearchCV(model, param_grid, cv=5)
grid_search.fit(X, y)
print(""Migliori iperparametri:"", grid_search.best_params_)
print(""Miglior score:"", grid_search.best_score_)
La griglia di valori per gli iperparametri
max_depth e
min_samples_split è definita nel dizionario
param_grid. La classe
GridSearchCV viene inizializzata con l'albero decisionale, la griglia di valori degli iperparametri e il numero di fold per la cross-validation (
cv=5). Infine, il metodo
fit viene chiamato per eseguire la grid search.
Il risultato della grid search è stampato a schermo utilizzando le proprietà
best_params_ e
best_score_. La proprietà
best_params_ restituisce i valori degli iperparametri che massimizzano il punteggio di validazione incrociata (
best_score_).
L'oggetto
grid_search può essere ora usato come un modello a se stante per un nuovo addestramento e validazione.
Tuttavia, è importante notare che la grid search può diventare molto lenta con molti iperparametri da esplorare e che potrebbero esserci approcci migliori in tal caso.
Inoltre, la grid search può portare alla selezione di modelli troppo complessi, che tendono a sovra-adattarsi ai dati di training.
Un modello che si sovra-adatta ai dati di addestramento non è in grado di generalizzare corretamente su dati non visti e diventa quindi inutile in un contesto di applicazione reale.
Leggi di più sull'overfitting visitando l'articolo qui
Pertanto, è consigliabile utilizzare la grid search con cautela e combinare la sua applicazione con altre tecniche di tuning degli iperparametri, come la ricerca casuale o la bayesian optimization.
Conclusioni
La grid search è una tecnica potente per ottimizzare gli iperparametri dei modelli di machine learning e può essere particolarmente utile quando si hanno molti iperparametri da considerare.
Tuttavia, è importante considerare che la grid search può essere computazionalmente costosa se ci sono molti iperparametri da esplorare.
Inoltre, la grid search può portare alla selezione di modelli troppo complessi, che tendono a sovra-adattarsi ai dati di training.
Commenti dalla community"
https://www.diariodiunanalista.it/posts/costruire-il-proprio-dataset-da-zero-vantaggi-approccio-e-strumenti/,"Se cerchiamo how to become data scientists su Google troviamo una marea di risorse che ci esortano ad imparare le basi dell'algebra lineare e dell'analisi matematica.
La motivazione che ci viene data è che queste saranno la base della nostra comprensione degli algoritmi di machine e deep learning che andremo a studiare più avanti. Sono tasselli propedeutici e, a parer mio, le risorse che dicono questo hanno ragione. Non se ne esce.
Ho scoperto però che seppur necessari alla crescita e alla comprensione della materia come data scientist, esse non sono sufficienti. Ci sono moltissimi analisti incredibilmente abili nel loro lavoro, ma parecchi arrancano a trovare lavoro e a farsi valere.
Scrivo questo post nel gennaio 2022 ma sono abbastanza sicuro che questa affermazione rimarrà vera anche nel futuro. Basta andare a leggere qualsiasi discussione su Kaggle (ad esempio qui), Reddit (in /r/machinelearning oppure learnmachinelearning) oppure in un canale Discord che tratti l'argomento.
Questi individui non sono bravi - sono più che bravi. Eppure hanno difficoltà ad interfacciarsi con la realtà. Una realtà che richiede conoscenze che vanno al di là data science o dell'ingegneria informatica. Non voglio peccare di arroganza nel dire che io conosco la verità e che queste persone dovrebbero seguirmi. Non è così. Anche io ho sbattuto la testa contro questo palo e spesso la sbatto ancora oggi dopo 6 anni di esperienza in questo lavoro.
Non è stato facile, ma ho pian piano sviluppato degli approcci che si sono rivelati utili per aumentare il mio valore all'interno di un contesto lavorativo e non. Il consiglio che mi sento di condividere oggi è uno che non si sente spesso nei tanti corsi di data science che ci sono online, vale a dire quello di creare un dataset partendo da zero per risolvere un problema che ci interessa.
Iniziamo.
Quali sono i vantaggi di creare il proprio dataset da zero?
Ci sono diversi motivi che mi spingono a consigliare questo approccio. Questi spaziano da quelli pragmatici a quelli più personali. Ecco una panoramica:
- ci assumiamo la responsabilità completa del progetto, da inizio a fine (accountability)
- siamo proprietari a tutti gli effetti del materiale che usiamo (ownership)
- sviluppiamo una comprensione per il problema e i nostri dati molto profonda (specific knowledge)
- il nostro problema non è risolvibile attraverso gli open dataset conosciuti (leverage)
I temi di accountability, ownership, knowledge e leverage sono, a mio avviso, centrali allo sviluppo personale e professionale. Esploriamoli uno ad uno per dare un senso alla attività di creare un dataset da zero.
Responsabilità (Accountability)
Creare un dataset da zero ci pone in una situazione di responsabilità completa - ogni errore o bias nei dati è riconducibile a noi e solo a noi.
È quindi molto importante essere abili nel reperire il dato in maniera corretta, rispettando la validità e la coerenza dei sistemi di misurazione (se usiamo sensori fisici) o scrivendo codice pulito e strutturato se il dato è reperito online (ad esempio con dei testi).
Possesso (Ownership)
Quando ci mettiamo in gioco e dedichiamo del tempo a fare il lavoro sporco di scraping, creare sondaggi o condurre interviste, sviluppiamo disciplina, pazienza e abilità nelle attività menzionate.
Questo non accadrebbe se il dataset usato fosse pubblico, poiché altri ricercatori hanno fatto in passato questo lavoro per noi. Avere possesso del nostro dato ci permette anche di condurre con intenzione e scopo un eventuale team a disposizione per lo studio.
Conoscenza specifica (Specific knowledge)
Il percorso sarà ricco di sfide che affronteremo da soli o in compagnia. In ogni caso, possiamo essere sicuri che lo termineremo con più conoscenze specifiche e generali riguardo il problema. Se affrontiamo più volte questo percorso, possiamo anche diventare degli esperti in materia (la cosiddetta nicchia).
Leva (Leverage)
Avere leva significa che siamo in grado di fare o offrire qualcosa che non tutti possono. Avere leva ci dà valore. Se abbiamo molta leva, siamo in grado di offrire soluzioni che servono agli altri e che quest'ultimi sono in grado di pagare bene.
Quando costruiamo un dataset da zero per un problema che un dataset pubblico non copre adeguatamente, stiamo creando un asset con del valore. Questo valore è più o meno prezioso in base all'ambizione del progetto e al prodotto finale.
In sostanza, come per quasi tutti i progetti di sviluppo, quello che se ne ricava è quasi sempre importante per lo sviluppo personale e professionale.
È l'atteggiamento imprenditoriale che emerge - se il progetto è nostro a tutti i livelli menzionati allora dobbiamo essere in grado di sacrificare parecchio per dargli l'opportunità di vedere la luce. Nel data science, questo sacrificio spesso inizia proprio con la creazione di un dataset di addestramento.
Approccio
Poniamo che vogliamo modellare il nostro stato di salute quando siamo al PC. L'ipotesi è che ci sono dei momenti della giornata dove il nostro corpo si affatica di più e questo affaticamento può essere dovuto alla routine lavorativa oppure a fluttuazioni fisiologiche.
L'approccio giusto per chi vuole studiare questo fenomeno è mettere giù un piccolo disegno sperimentale. Infatti, la fase di modellazione viene dopo quella sperimentale a tutti gli effetti. Non si modella qualcosa di cui non si conosce nulla, e l'unico modo per conoscere qualcosa in maniera accurata è usare il metodo sperimentale.
L'esempio che ho menzionato è interessate - vediamo come applicare il metodo sperimentale per un progetto del genere. Ecco gli step
- Definizione dell'ipotesi
Una ricerca si basa quasi sempre su una o più ipotesi sperimentali. A volte può essere solo esplorativa, ma la maggior parte hanno delle affermazioni fatte dai ricercatori che le sostiene. Nel nostro caso abbiamo l'abbiamo menzionata poc'anzi: ci sono dei momenti della giornata in cui si è più stanchi e questi momenti sono dovuti a ragioni di stress lavorativo o fluttuazioni fisiologiche.
Il nostro obiettivo sarà quello di falsificare questa affermazione (principio della falsicabilità di Popper) andando a trovare prove a sostegno dell'opposto. Se queste prove sono importanti (statisticamente) allora siamo in grado di rifiutare l'ipotesi nulla (quella che descrive il mondo prima dell'esperimento) e accettare una ipotesi alternativa (quella su cui abbiamo raccolto abbastanza prove nell'esperimento).
- Preparazione
La fase preparatoria ci permette di organizzare il flusso di attività e di reperire i vari strumenti che ci occorrono. Nel nostro caso dovremmo raccogliere i dati durante la giornata.
Compreremo sensori da applicare al corpo per tracciare battito cardiaco, pressione sanguigna e saturazione del sangue. Inoltre, useremo dei software per tracciare le nostre attività al PC. Dobbiamo anche tener conto dell'orario della giornata: l'intuizione iniziale è quella che l'indice di ogni attività sarà un timestamp che indica l'orario preciso.
- Raccolta dati e esperimento
In questa fase cerchiamo di osservare o causare l'effetto che falsifica la nostra ipotesi. Nel nostro caso, gli indicatori espressi dai sensori indossati e dai software usati ci informeranno di quello che accade realmente.
Magari abbiamo un crollo di energie solo per quando parliamo con un gruppo specifico di persone a lavoro, oppure quando facciamo una attività che non ci piace molto. Ogni evento che prende luogo sarà misurato e memorizzato con l'apparato tecnico predisposto nel punto precedente.
- Analisi dei risultati
Questa è la fase finale del processo (a meno che non si pubblica un paper al riguardo, in quel caso l'approccio si conclude con la fase di presentazione). Qui andremo ad applicare un numero ampio di tecniche di analisi dati per validare e chiarare i risultati della ricerca.
È solo in seguito all'analisi dei risultati del nostro studio che possiamo dire di aver compreso il problema che avevamo di fronte all'inizio. Aggiungo che questo è l'unico momento dove eticamente siamo giustificati a modellare il fenomeno osservato.
Utilizzo la parola eticamente perché, sebbene si possa fare modeling senza l'approccio descritto, è solo attraverso questo percorso che l'autore della ricerca ottiene quello che desidera sin dall'inizio: una maggiore comprensione del fenomeno.
Ogni altro tentativo è veicolato da obiettivi che esulano da una comprensione più profonda, come ad esempio fare bella figura verso qualcuno (il cosiddetto status game) oppure risolvere il problema di qualcun altro che a sua volta non ha l'esigenza di comprendere più profondamente quello di cui è interessato.
Voglio specificare che non c'è niente di male a non voler perseguire un obiettivo puramente conoscitivo. Tutti giochiamo lo status game, e a tutti fa piacere fare un buon lavoro ed essere pagati bene. Non è questo il punto. Il punto è che, se dovessi consigliare a mio figlio come condurre un progetto personale in un campo come questo, gli direi queste parole. Non c'è niente di male nel non farlo...ma sono dell'idea che se lo si fa se ne esce come delle persone migliori, professionalmente e personalmente.
Strumenti utili ad un analista per creare un dataset da zero
Stilerò una lista di metodiche che ho usato nel corso della mia carriera per raccogliere i dati per il mio progetto. Sviluppare abilità nell'applicare queste tecniche è utile a qualsiasi analista che vuole avere la possibilità di lavorare autonomamente sul suo progetto.
Web scraping
la regina delle tecniche digitali per il reperimento del dato. Oggigiorno non c'è nulla che non possa essere scraperato se si è abili abbastanza. Consiste nel raccogliere i dati che ci interessano da siti web o altre risorse online. Il dato dev'essere visibile da un web browser e dev'essere concepito per essere visionato da altri utenti.
Ci sono delle regole, però: non scraperare siti che esplicitamente chiedono di non essere scraperati - non è etico - e non scraperare assiduamente (ergo, non floodare il server). Quando scraperiamo non stiamo facendo altro che mandare richieste al server per ricevere il dato. Se lo facciamo troppo velocemente potremmo danneggiare il server in questione.
La barriera più imponente è rappresentata dai social media, ma ci sono comunque dei metodi validi che è possibile trovare online.
Gli strumenti che possiamo usare in Python sono
- per piccoli progetti --> combinazione di BeautifulSoup + Requests (qui un ottimo template per iniziare)
- per grossi progetti --> Scrapy
- per qualsiasi esigenza di rendering JavaScript --> Playwright
Sondaggi
Spesso dimentichiamo che i sondaggi sono delle tecniche per la raccolta dati molto potenti. Ci sono siti web come Pollfish che permettono di inviare sondaggi e interviste su larga scala attraverso il web e con profilazione molto precisa.
È vero che otteniamo spesso dati più qualitativi attraverso di essi, ma per un data scientist un dato qualitativo non è altro che un dato simile a qualsiasi altro. Possiamo sempre utilizzarlo per la nostra ricerca.
Sensori fisici
L'utilizzo dei sensori fisici è molto cresciuto nell'ultimo decennio grazie alla crescita del settore IoT e dei wearable devices come Apple Watch e simili. Oggi possiamo facilmente mettere su una strumentazione fisica per la nostra raccolta dati, sia per le cose che per le persone.
Script
Se possediamo un sito web e siamo abili in Javascript o Php, possiamo scrivere degli script di tracciamento che, con il consenso dell'utente, possono raccogliere dati sul suo comportamento di utilizzo. Un po' come fanno Google Analytics e HotJar.
Aggiungo che raccogliere questo tipo di informazione può ricadere nella regolamentazione GDPR, quindi dobbiamo essere cauti e usare i dati responsabilmente.
In conclusione, devo menzionare che è anche possibile comprare dati, che differiscono in prezzo in base alla loro qualità. Spesso si tratta di dati difficilmente reperibili da un singolo individuo (come dati che provengono da zone profonde dello spazio oppure dagli abissi marini) e sono molto difficili da mettere insieme proprio a causa del fenomeno che descrivono.
Questa strada è spesso usata dai data scientist per avere una mole di dati più grande da unire ad un dataset già sviluppato da loro.
Commenti dalla community"
https://www.diariodiunanalista.it/posts/data-science-data-engineering-data-analytics-ma-qual-e-la-differenza/,"In questo articolo voglio presentare al lettore una visione olistica di quello che per me è processo di data science.
Tale processo viene spesso menzionato in materiale educativo di settore, come corsi online e manuali ed è il punto di partenza del professionista che vuole muovere i suoi primi passi nel campo dell'analisi dei dati. Questo perché il processo di data science è molto simile al metodo scientifico di Galileo che si studia a scuola.
Diventa quindi un aggancio dalla semplice interpretazione che permette a chi si muove nel campo di comprendere e spiegare efficacemente ogni step del progetto che sta osservando.
Questo processo spesso include le seguenti fasi:
- inquadramento del problema
- raccolta del dato
- preprocessing e trasformazioni
- esplorazione
- modellazione e interpretazione dei risultati
- go-live
Quando il data scientist lavora da solo, ogni aspetto di questo processo è sotto la sua completa responsabilità.
Infatti, la figura del data scientist è molto ricercata proprio per il suo skill set variegato, che copre molteplici campi di competenza, sia quelli più orientati all'ingegneria del software, come creazione di programmi per reperire i dati dal web, che l'aspetto più statistico, che si esprime nella fase di preprocessing e esplorazione.
Quando invece il data scientist è inserito in un team, ecco che il suo focus viene diretto a fasi specifiche del processo di data science. Questo perché figure come il data engineer e il data analyst (ma anche altre che qui non menziono) vengono impiegate per gestire con altrettanto focus le loro rispettive aree di expertise.
Declinazione fase - ruolo nel processo di data science
Per comprendere al meglio i ruoli e le attività delle figure menzionate può essere interessante associare ogni fase del processo di data science ad un ruolo.
1. Inquadramento del problema
Figure necessarie: tutte
Figure optional: nessuna
Questa è una fase di brainstorming, dove si parla del problema e di come affrontarlo per migliorare in qualche modo il processo che è afflitto da tale problema.
Ogni ruolo è in grado di contribuire in maniera importante alla definizione e messa a fuoco del problema, cosa lo causa e come è composto in modo da analizzare un piano risolutivo.
Solitamente durante questa fase di kick-off del progetto ci si riunisce più volte in team e si discute di cosa si vuole fare, risolvere e ottenere alla fine del progetto. I membri chiave del progetto prendono appunti, fanno domande e guidano la conversazione per gli individui che non sono direttamente coinvolte nello sviluppo.
Gli strumenti usati coinvolgono diagrammi di flusso, di Gantt e creazione di documentazione preliminare per offrire al team esteso una visione di come il progetto sarà costituito dal punto di vista gestionale tecnico.
Insieme alle figure che lavorano nel mondo dei dati, in questa fase sono coinvolti anche stakeholder e membri di altri team che possono contribuire alla conversazione.
2. Raccolta del dato
Figure necessarie: Data engineer
Figure optional: Data scientist
Nella fase di raccolta dati il data engineer è essenziale. A dispetto di quanti molti pensano, un data scientist da solo non è in grado di creare impalcature di raccolta dati su larga scala.
Sebbene il data scientist abbia conoscenze anche in questo campo, le specifiche di tale lavoro sono responsabilità del data engineer poiché non si parla solo di scraping, richieste HTTP a API o altro, ma di mettere insieme tutte queste fonti e creare sistemi di memorizzazione del dato efficienti.
Un data engineer è esperto dei sistemi di archiviazione e processamento nel cloud, come Amazon Web Services (AWS) o Azure di Microsoft. È in grado di scegliere la piattaforma adatta per massimizzare l'efficienza sia in termini di costi che di tempo. Sentiamo spesso i termini data lake e warehouse - questi non sono altro che database con scopi diversi posizionati, solitamente, nel cloud.
Servizi del genere sono tipicamente usati per progetti di livello enterprise o comunque molto grandi e strutturati e quindi un data engineer diventa molto importante proprio in contesti del genere.
In pratica, il data engineer trova modo di lavorare soprattutto in un contesto legato al big data, con un numero molto elevato di terabyte processati ogni giorno dai sistemi messi in piedi dal team di software e data engineering.
Come figura optional troviamo il data scientist, che può affiancare il DE nella conversazione che andrà a trattare la struttura del dato in database, poiché saranno i data scientist e analyst a lavorare con quel dato una volta raccolto e trasformato.
Il database utilizzato, la schematizzazione del dato in modelli e molto altro è quindi in mano al data engineer che si occuperà, in seguito alla raccolta, anche della sua trasformazione.
3. Preprocessing e trasformazioni
Figure necessarie: Data engineer
Figure optional: Data scientist
Anche qui un data engineer è essenziale per strutturare pipeline di preprocessing del dato. Il bisogno di un data engineer è che di nuovo gli strumenti nel cloud diventano essenziali per lavorare con un grosso volume di dati in maniera efficiente.
Fuori dal contesto dei (quasi) big data, un data scientist può trovare modo di far funzionare le cose, ma comunque in modo meno efficiente di un data engineer.
Un data engineer potrebbe scegliere di usare Apache Airflow invece di NiFi per fare scheduling dei task oppure per gestire il flusso in entrata e in uscita dei dati.
Questi sono solo due strumenti che permettono al professionista di strutturare e applicare l'ETL (extract, load, transform) sui dati.
Una delle grandi contribuzioni di un data engineer è proprio quella di poter guidare il team nella scelta della strumentazione adatta.
Anche qui il data scientist può affiancare il data engineer per contribuire agli step di preprocessing.
4. Esplorazione
Figure necessarie: Data scientist, data analyst
Figure optional: Data engineer
In base alla grandezza del progetto, la fase di esplorazione e comprensione del dato viene fatta dagli esperti di analytics (che di solito sono i data scientist / analyst) oppure da questi e anche il team di data engineering.
Lavorando strettamente con database di ogni tipo, i data engineer sono molto abili con SQL. Questo li rende molto utili anche nella fase di esplorazione del dato, non solo nella fase di memorizzazione.
Nella fase di analisi esplorativa del dato, il professionista interroga dati usabili (che lo sono tipicamente solo in seguito alla fase di preprocessing) al fine di trovare insight, anomalie e pattern utili. Questa fase è essenziale per capire se il dato è veramente usabile per il destinatario (che può essere un utente umano o un modello di machine learning) e per comprendere i next step.
Questo viene fatto attraverso query SQL, visualizzazione e report per i membri del team e stakeholder.
Ad un analista servono solo conoscenze di SQL e di data visualization per coprire completamente la fase di esplorazione del dato. Tutto il resto è conoscenza che si costruisce sopra quei due mattoni fondamentali.
La fase esplorativa si conclude quando si è solitamente superata una validazione da parte del team responsabile dei dati e dagli stakeholder. Se il professionista lavora da solo, allora la fase esplorativa si conclude con un report finale che dettaglia gli insight trovati oppure con una fase di modellazione del dato.
5. Modellazione e interpretazione dei risultati
Figure necessarie: Data scientist
Figure optional: Data analyst
Un data scientist si trova a suo agio durante la fase di modeling più di qualsiasi altro professionista con un ruolo nel mondo dei dati. Questa è la sua specialità perché permette di applicare tutte le hard skill di questa figura, dalle competenze di statistica a quelle di sperimentazione e ottimizzazione.
In questa fase il team di data science si riunisce per valutare il feature set e inquadrare il problema in una ottica di machine learning. Si comprende se è un problema che può essere affrontato con un approccio supervisionato, oppure se c'è bisogno di generare nuovi dati attraverso il feature engineering per arricchire il modello.
Tale modello, quando addestrato correttamente sui dati a disposizione, potrà fornire importanti benefici al business, quali automazione di una serie di task intensivi e inefficienti se portati avanti da un umano (generazione di testi, categorizzazione di contenuto, etc.) oppure predire un evento futuro con un errore possibilmente piccolo.
Un data scientist è un abilitatore dell'automazione
La fase di modellazione è caratterizzata anche dalla costruzione di impalcature di sperimentazione. Infatti, raramente un modello funziona alla prima iterazione, e richiede invece molti step di correzione e fine-tuning per raggiungere performance apprezzabili.
Questo sistema dev'essere mantenuto e gestito dai data scientist, che tracciano e analizzano i risultati dei vari esperimenti. Strumenti come MLFlow e Neptune.ai vengono usati per questo tipo di compito.
Il data analyst offre supporto al scientist, che può contribuire all'analisi delle performance di vari modelli e nella documentazione.
In un contesto enterprise, i modelli predittivi vengono testati usando tecniche come AB test con un gruppo ristretto di utenti prima di essere rilasciati ufficialmente.
6. Go-live
Figure necessarie: Data scientist
Figure optional: Data engineer
La fase di go-live, detta anche deployment, coinvolge principalmente il data scientist con competenze di ML Ops (machine learning operations). Queste competenze riguardano sistemi e infrastrutture per gestire diversi modelli predittivi e tracciare le performance nel tempo.
L'obiettivo principale del professionista che ha responsabilità in questa fase è quello di assicurarsi che i modelli deployati siano in linea con le performance attese e non mostrino drift: quando qualcosa nel mondo cambia che il modello non può conoscere e che quindi impatta negativamente sulle performance.
La preoccupazione principale del data scientist coinvolto nel processo di go-live è quella di tracciare le performance dei modelli e che questi non si spostino significativamente dalle performance mostrate in un setting sperimentale.
Insieme agli strumenti di tracciamento delle performance che ho menzionato prima, qui un data scientist, in collaborazione anche con dei software developer, può scegliere come servire il modello al pubblico e creare quindi API o microservizi collegati tra loro.
La libreria in Python più versatile al momento per configurare delle API è FastAPI.
Il data engineer può essere coinvolto per aiutare il data scientist a configurare i sistemi nel cloud.
Visione olistica del processo di data science
Se valutiamo il coinvolgimento di queste figure professionali nel processo di data science, vediamo come tutte contribuiscano in maniera fondamentale allo svolgersi di ogni fase, aumentando l'efficienza di ogni processo.
Il processo di data science è quindi naturalmente inclusivo di molteplici figure professionali, soprattutto se esteso ad un contesto enterprise.
È vero che se il professionista lavora da solo o in un piccolo team, allora questi possono focalizzarsi su processi più snelli e lavorare su di essi in prima persona senza un supporto da parte dello specialista. Va notato però che i processi in questione soffriranno dell'assenza di queste figure.
Qui langue un pericolo per lo stakeholder inesperto del campo:
progetti grandi consegnati a team piccoli possono soffrire sugli aspetti più fondamentali (soprattutto step 1 e 2 del processo).
Lavorare in solitudine o in team piccoli su progetti molto grandi come software as a service (SAS) a tema analytics può essere una sfida importante, dove alcune scelte fondamentali possono essere soggetta a compromessi a causa di competenze non specializzate.
Ad esempio, un data scientist esperto in modeling avrà ottimi risultati nell'addestrare un modello predittivo, ma potrebbe perdersi nella scelta dell'infrastruttura cloud ideale dove deployare il suo modello.
Lo stakeholder che è interessato a innescare un progetto basato sui dati dovrebbe essere consapevole del processo di data science e di come questo possa includere figure professionali specializzate che possono fare la differenza.
Se i dati esistono già nel mondo e sono usabili, ad esempio un dataset di Kaggle, il progetto potrebbe non richiedere figure come il data engineer o il scientist poiché il focus sarebbe puramente analitico.
Va considerato anche il destinatario del risultato finale: un modello o una dashboard deployati per uso interno possono essere caratterizzati da processi molto più snelli e meno stringenti rispetto agli stessi invece consegnati ad un cliente che paga una fee mensile.
Conclusione
Vedere il processo di data science come un ciclo di fasi che naturalmente coinvolgono tutte le figure specializzate in ambito dati è a mio avviso essenziale - permette a chi fa questa valutazione di comprendere profondamente i vantaggi e le limitazioni di cui beneficia o soffre il progetto, e da consapevolezza maggiore dell'output finale.
Sono dell'idea che una conversazione costante tra membri di team diverso, ma appartenente allo stesso campo (leggere data engineering e science) possa beneficiare enormemente il progetto, in modi impossibili da prevedere ab origine.
Il lupo solitario invece dovrebbe circondarsi di colleghi virtuali quanto più possibile: consiglio infatti di ingaggiare persone sui social e parlare con loro direttamente per migliorare le performance su un task complesso.
Infine, lo stakeholder principale, che sia un CTO oppure un imprenditore con una ambizione di sfondare in ambito data, dovrebbe prima di tutto informarsi sul processo di data science al fine di pianificare adeguatamente il progetto lato business.
L'inclusione o meno di una di queste figure può impattare notevolmente sul business, e questa non dovrebbe essere una sorpresa, ma bensì una condizione precedentemente pensata e compresa profondamente.
Commenti dalla community"
https://www.diariodiunanalista.it/posts/early-stopping-in-tensorflow/,"In questo articolo spiegherò come controllare il training di una rete neurale in TensorFlow attraverso l'utilizzo di callback.
Una callback è una funzione che viene chiamata ripetutamente durante un processo (ad esempio l'addestramento di una rete neurale) e che generalmente serve per validare o correggere determinati comportamenti.
Nel machine learning, possiamo usare delle callback per definire cosa succede prima, durante o alla fine di un'epoca.
Questo è utile soprattutto per fare logging delle performance oppure per interrompere il training se la nostra metrica di performance raggiunge una certa soglia. Questo meccanismo si chiama early stopping.
Ad esempio, se si impostano 1000 epoche e la precisione desiderata è già stata raggiunta all'epoca 200, l'addestramento si interromperà automaticamente. Vediamo come questo viene implementato in TensorFlow e Python.
Perché usare l'early stopping?
L'early stopping è uno dei metodi di regolarizzazione per reti neurali più comuni e efficaci.
Infatti, grazie ad esso, possiamo evitare overfitting e underfitting dei nostri dati. Senza andare nel dettaglio in questo articolo, quando il nostro modello overfitta i nostri dati, allora quel modello non è in grado di generalizzare la mondo reale poiché troppo sensibile ai dati di training.
In maniera complementare, un modello che underfitta sarà troppo generico e non sarà in grado mappare adeguatamente il nostro input al nostro output.
Il lettore interessato all'argomento può espandere la sua conoscenza su questi due fenomeni leggendo un articolo specifico sull'overfitting.
Gettiamo le basi andando a importare il dataset
fashion_mnist da TensorFlow. Useremo questo dataset per spiegare come funzionano le callback.
import tensorflow as tf
# Importiamo il dataset dall'API di tensorflow
fmnist = tf.keras.datasets.fashion_mnist
# Carichiamo il dataset
(x_train, y_train),(x_test, y_test) = fmnist.load_data()
# Normalizziamo le immagini
x_train, x_test = x_train / 255.0, x_test / 255.0
Ecco come appare il nostro dataset
La classe EarlyStopping
Il secondo step è la creazione della classe dedicata all'early stopping. In questo caso creeremo una classe che erediterà da
tf
.
keras
.
callbacks
.
Callback e permetterà di fermare il training al raggiungimento del 95% di accuracy. La callback userà la funzione
on_epoch_end per fermare il training se la condizione è soddisfatta andando a guardare i log erogati dal modello di TensorFlow.
In pratica qui non facciamo altro che accedere al metodo on_epoch_end che è ereditato da
tf.keras.callbacks.Callback e fare override il suo comportamento andando ad inserire la condizione che farà interrompere il training.
Continuiamo con l'implementazione del codice del nostro modello.
class EarlyStopping(tf.keras.callbacks.Callback):
def on_epoch_end(self, epoch, logs={}):
'''
Interrompe l'addestramento al raggiungimento del 95% di accuracy
'''
# Controlliamo l'accuracy
if(logs.get('accuracy') > 0.95):
# Fermiamo il training se la condizione è soddisfatta
print(""\nSoglia di accuracy raggiunta. Training interrotto!"")
self.model.stop_training = True
# Creiamo un oggetto della nostra classe e assegnamolo ad una variabile
early_stopping = EarlyStopping()
Classificazione con deep neural network
Useremo una rete neurale con diversi strati per classificare gli indumenti del dataset. L'approccio migliore sarebbe quello di usare una rete neurale convoluzionale, ma per questo esempio una deep neural network andrà più che bene.
# Creiamo un modello sequenziale con tre strati
model = tf.keras.models.Sequential([
tf.keras.layers.Flatten(input_shape=(28, 28)), # appiattiamo l'input
tf.keras.layers.Dense(512, activation=""relu""), # creiamo uno strato denso di 512 neuroni con attivazione ReLU
# creiamo lo strato di output 10 neuroni, con attivazione Softmax per mappare
# ogni tipo di indumento nel dataset
tf.keras.layers.Dense(10, activation=""softmax"")
])
# Compiliamo il modello con Adam e settiamo le metriche canoniche per un task di classificazione
model.compile(optimizer=""adam"",
loss='sparse_categorical_crossentropy',
metrics=['accuracy'])
Ora siamo pronti ad addestrare il modello. Per passare la callback, basta inserire il nostro oggetto nella lista da fornire all'argomento
callbacks nella funzione
.fit() del modello.
# Addestriamo il modello con la nostra callback!
model.fit(x_train, y_train, epochs=10, callbacks=[early_stopping])
Ecco come si imposta una callback per controllare il training di una rete neurale.
Codice completo
import tensorflow as tf
# Importiamo il dataset dall'API di tensorflow
fmnist = tf.keras.datasets.fashion_mnist
# Carichiamo il dataset
(x_train, y_train),(x_test, y_test) = fmnist.load_data()
# Normalizziamo le immagini
x_train, x_test = x_train / 255.0, x_test / 255.0
class EarlyStopping(tf.keras.callbacks.Callback):
def on_epoch_end(self, epoch, logs={}):
'''
Interrompe l'addestramento al raggiungimento del 95% di accuracy
'''
# Controlliamo l'accuracy
if(logs.get('accuracy') > 0.95):
# Fermiamo il training se la condizione è soddisfatta
print(""\nSoglia di accuracy raggiunta. Training interrotto!"")
self.model.stop_training = True
# Creiamo un oggetto della nostra classe e assegnamolo ad una variabile
early_stopping = EarlyStopping()
# Creiamo un modello sequenziale con tre strati
model = tf.keras.models.Sequential([
tf.keras.layers.Flatten(input_shape=(28, 28)), # appiattiamo l'input
tf.keras.layers.Dense(512, activation=""relu""), # creiamo uno strato denso di 512 neuroni con attivazione ReLU
# creiamo lo strato di output 10 neuroni, con attivazione Softmax per mappare
# ogni tipo di indumento nel dataset
tf.keras.layers.Dense(10, activation=""softmax"")
])
# Compiliamo il modello con Adam e settiamo le metriche canoniche per un task di classificazione
model.compile(optimizer=""adam"",
loss='sparse_categorical_crossentropy',
metrics=['accuracy'])
# Addestriamo il modello con la nostra callback!
model.fit(x_train, y_train, epochs=10, callbacks=[early_stopping])
Commenti dalla community"
https://www.diariodiunanalista.it/posts/estrazione-keyword-un-benchmark-di-7-algoritmi-in-python/,"Di recente mi sono trovato a lavorare su attività di estrazione di parole chiave. L'obiettivo era trovare un algoritmo in modo efficiente, bilanciando la qualità dell'estrazione e il tempo di esecuzione, poiché il mio corpus di dati stava aumentando raggiungendo rapidamente milioni di righe. Uno dei KPI era estrarre parole chiave che avevano sempre senso da sole anche contesto.
Ciò mi ha portato a testare e sperimentare diversi meccanismi di estrazione conosciuti nell'ambiente. Eccomi condividere con voi il mio piccolo viaggio.
Le Librerie
Ho utilizzato le seguenti librerie per condurre lo studio
- NLTK, per aiutarmi nelle fasi di preprocessing e per alcune funzioni di helper
- RAKE
- YAKE
- PKE
- KeyBERT
- Spacy
Sono stati utilizzati anche Pandas e Matplotlib, insieme ad altre librerie fondamentali.
Struttura dell'Esperimento
Il modo in cui funziona il benchmark è il seguente
Per prima cosa importeremo il set di dati che contiene i nostri dati testuali. Creeremo quindi funzioni separate che applicano la logica di estrazione
__nome_algoritmo__(str: testo) → [keyword1, keyword2, …, keywordn]
Quindi creeremo una funzione che applica il meccanismo di estrazione all'intero corpus.
extract_keywords_from_corpus(algoritmo, corpus) → {algoritmo, keyword_del_corpus, tempo_impiegato}
Spacy ci aiuterà a definire un oggetto matcher che restituirà True o False se una parola chiave corrisponde a un modello sintattico che abbia senso per il nostro compito.
Infine raggrupperemo tutto in una funzione che emette il nostro rapporto finale.
Il Dataset
Il dataset contiene dei testi brevi presi online da diverse risorse, in lingua inglese. Ecco un esempio
['To follow up from my previous questions. . Here is the result!\n',
'European mead competitions?\nI’d love some feedback on my mead, but entering the Mazer Cup isn’t an option for me, since shipping alcohol to the USA from Europe is illegal. (I know I probably wouldn’t get caught/prosecuted, but any kind of official record of an issue could screw up my upcoming citizenship application and I’m not willing to risk that).\n\nAre there any European mead comps out there? Or at least large beer comps that accept entries in the mead categories and are likely to have experienced mead judges?',
'Orange Rosemary Booch\n',
'Well folks, finally happened. Went on vacation and came home to mold.\n',
'I’m opening a gelato shop in London on Friday so we’ve been up non-stop practicing flavors - here’s one of our most recent attempts!\n',
""Does anyone have resources for creating shelf stable hot sauce? Ferment and then water or pressure can?\nI have dozens of fresh peppers I want to use to make hot sauce, but the eventual goal is to customize a recipe and send it to my buddies across the States. I believe canning would be the best way to do this, but I'm not finding a lot of details on it. Any advice?"",
'what is the practical difference between a wine filter and a water filter?\nwondering if you could use either',
'What is the best custard base?\nDoes someone have a recipe that tastes similar to Culver’s frozen custard?',
'Mold?\n']
Per lo più sono articoli legati al cibo. Prenderemo un campione di 2000 documenti per testare i nostri algoritmi. Non andremo ancora a fare preprocessing sui nostri testi perché alcuni algoritmi basano i loro risultati sulla presenza di stopword e punteggiatura.
Gli Algoritmi
Definiamo le funzioni di estrazione keyword
# inizializziamo BERT fuori dalle funzioni
bert = KeyBERT()
# 1. RAKE
def rake_extractor(text):
r = Rake()
r.extract_keywords_from_text(text)
return r.get_ranked_phrases()[:5]
# 2. YAKE
def yake_extractor(text):
keywords = yake.KeywordExtractor(lan=""en"", n=3, windowsSize=3, top=5).extract_keywords(text)
results = []
for scored_keywords in keywords:
for keyword in scored_keywords:
if isinstance(keyword, str):
results.append(keyword)
return results
# 3. PositionRank
def position_rank_extractor(text):
# definiamo i pattern grammaticali da considerare
pos = {'NOUN', 'PROPN', 'ADJ', 'ADV'}
extractor = pke.unsupervised.PositionRank()
extractor.load_document(text, language='en')
extractor.candidate_selection(pos=pos, maximum_word_number=5)
extractor.candidate_weighting(window=3, pos=pos)
# selezioniamo le top 5 keyword
keyphrases = extractor.get_n_best(n=5)
results = []
for scored_keywords in keyphrases:
for keyword in scored_keywords:
if isinstance(keyword, str):
results.append(keyword)
return results
# 4. SingleRank
def single_rank_extractor(text):
pos = {'NOUN', 'PROPN', 'ADJ', 'ADV'}
extractor = pke.unsupervised.SingleRank()
extractor.load_document(text, language='en')
extractor.candidate_selection(pos=pos)
extractor.candidate_weighting(window=3, pos=pos)
keyphrases = extractor.get_n_best(n=5)
results = []
for scored_keywords in keyphrases:
for keyword in scored_keywords:
if isinstance(keyword, str):
results.append(keyword)
return results
# 5. MultipartiteRank
def multipartite_rank_extractor(text):
extractor = pke.unsupervised.MultipartiteRank()
extractor.load_document(text, language='en')
pos = {'NOUN', 'PROPN', 'ADJ', 'ADV'}
extractor.candidate_selection(pos=pos)
extractor.candidate_weighting(alpha=1.1, threshold=0.74, method='average')
keyphrases = extractor.get_n_best(n=5)
results = []
for scored_keywords in keyphrases:
for keyword in scored_keywords:
if isinstance(keyword, str):
results.append(keyword)
return results
# 6. TopicRank
def topic_rank_extractor(text):
extractor = pke.unsupervised.TopicRank()
extractor.load_document(text, language='en')
pos = {'NOUN', 'PROPN', 'ADJ', 'ADV'}
extractor.candidate_selection(pos=pos)
extractor.candidate_weighting()
keyphrases = extractor.get_n_best(n=5)
results = []
for scored_keywords in keyphrases:
for keyword in scored_keywords:
if isinstance(keyword, str):
results.append(keyword)
return results
# 7. KeyBERT
def keybert_extractor(text):
keywords = bert.extract_keywords(text, keyphrase_ngram_range=(3, 5), stop_words=""english"", top_n=5)
results = []
for scored_keywords in keywords:
for keyword in scored_keywords:
if isinstance(keyword, str):
results.append(keyword)
return results
Ogni estrattore prende come argomento il testo da cui vogliamo estrarre le parole chiave e restituisce un elenco di parole chiave, dalla migliore alla peggiore in base alla loro tecnica estrazione.
Nota: per qualche motivo, non ho potuto inizializzare tutti gli oggetti estrattore al di fuori delle funzioni. TopicRank e MultiPartiteRank generavano errori ogni volta che lo facevo. Questo inficia un po' sulle performance, ma il benchmark può essere fatto comunque.
Stiamo già limitando alcuni dei modelli grammaticali accettati passando pos = {'NOUN', 'PROPN', 'ADJ', 'ADV'} (nome, nomi propri, aggettivi e avverbi) — questo, insieme a Spacy, assicurerà che quasi tutte le parole chiave siano sensate da un prospettiva del linguaggio umano.
Vogliamo anche che le parole chiave siano almeno trigrammi, solo per avere parole chiave più specifiche. Invito a controllare la documentazione delle librerie per approfondire i parametri e come funzionano.
Estrarre le keyword dall'intero corpus
Definiamo ora una funzione che applicherà un singolo estrattore all'intero corpus insieme ad alcune informazioni utili.
def extract_keywords_from_corpus(extractor, corpus):
""""""Questa funzione usa un estrattore per estrarre le keyword da una lista di documenti""""""
extractor_name = extractor.__name__.replace(""_extractor"", """")
logging.info(f""Starting keyword extraction with {extractor_name}"")
corpus_kws = {}
start = time.time()
# logging.info(f""Timer initiated."") <-- rimuovere il commento per loggare quando inizia il timer
for idx, text in tqdm(enumerate(corpus), desc=""Extracting keywords from corpus...""):
corpus_kws[idx] = extractor(text)
end = time.time()
# logging.info(f""Timer stopped."") <-- rimuovere il commento per loggare quando si ferma il timer
elapsed = time.strftime(""%H:%M:%S"", time.gmtime(end - start))
logging.info(f""Time elapsed: {elapsed}"")
return {""algorithm"": extractor.__name__,
""corpus_kws"": corpus_kws,
""elapsed_time"": elapsed}
Tutto ciò che fa questa funzione è popolare un dizionario con i dati provenienti dall'estrattore passati come argomento e una serie di informazioni utili come il tempo impiegato per eseguire l'attività.
È qui che ci assicuriamo che le parole chiave restituite dagli estrattori abbiano sempre (quasi?) un senso. Ad esempio,
Vediamo come le prime tre parole chiave abbiano senso fuori da qualsiasi contesto. Hanno un significato e sono del tutto sensate. Qualcosa quando usciamo, non ha senso perché abbiamo bisogno di più informazioni per comprendere il significato di quel blocco di dati. Vogliamo evitare questo.
Spacy ci aiuta con l'oggetto Matcher. Definiremo una funzione di corrispondenza che accetta una parola chiave e restituisce True o False se i modelli definiti corrispondono.
def match(keyword):
""""""Questa funzione controlla se un elenco di parole chiave corrisponde a un determinato modello grammaticale""""""
patterns = [
[{'POS': 'PROPN'}, {'POS': 'VERB'}, {'POS': 'VERB'}],
[{'POS': 'NOUN'}, {'POS': 'VERB'}, {'POS': 'NOUN'}],
[{'POS': 'VERB'}, {'POS': 'NOUN'}],
[{'POS': 'ADJ'}, {'POS': 'ADJ'}, {'POS': 'NOUN'}],
[{'POS': 'NOUN'}, {'POS': 'VERB'}],
[{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}],
[{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'NOUN'}],
[{'POS': 'ADJ'}, {'POS': 'NOUN'}],
[{'POS': 'ADJ'}, {'POS': 'NOUN'}, {'POS': 'NOUN'}, {'POS': 'NOUN'}],
[{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'ADV'}, {'POS': 'PROPN'}],
[{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'VERB'}],
[{'POS': 'PROPN'}, {'POS': 'PROPN'}],
[{'POS': 'NOUN'}, {'POS': 'NOUN'}],
[{'POS': 'ADJ'}, {'POS': 'PROPN'}],
[{'POS': 'PROPN'}, {'POS': 'ADP'}, {'POS': 'PROPN'}],
[{'POS': 'PROPN'}, {'POS': 'ADJ'}, {'POS': 'NOUN'}],
[{'POS': 'PROPN'}, {'POS': 'VERB'}, {'POS': 'NOUN'}],
[{'POS': 'NOUN'}, {'POS': 'ADP'}, {'POS': 'NOUN'}],
[{'POS': 'PROPN'}, {'POS': 'NOUN'}, {'POS': 'PROPN'}],
[{'POS': 'VERB'}, {'POS': 'ADV'}],
[{'POS': 'PROPN'}, {'POS': 'NOUN'}],
]
matcher = Matcher(nlp.vocab)
matcher.add(""pos-matcher"", patterns)
# creiamo l'oggetto spacy
doc = nlp(keyword)
# iteriamo tra i match
matches = matcher(doc)
# se match non è vuoto vuol dire che ha trovato almeno un elemento che segue le regole grammaticali
if len(matches) > 0:
return True
return False
La Funzione di Benchmark
Abbiamo quasi finito. Questo è l'ultimo passaggio prima di avviare lo script e raccogliere i risultati.
Definiremo una funzione di benchmark che prende come argomenti il nostro corpus e valore booleano per mescolare o meno i nostri dati. Per ogni estrattore, chiama la funzione extract_keywords_from_corpus, che restituisce un dizionario contenente il risultato di quell'estrattore. Memorizziamo quel valore in un elenco.
Per ogni algoritmo della lista, calcoliamo
- numero medio di parole chiave estratte
- numero medio di parole chiave corrispondenti
- calcolare un punteggio che tenga conto del numero medio di corrispondenze trovate diviso per il tempo impiegato per eseguire l'operazione
- salviamo tutti i nostri dati in un DataFrame Pandas e li esportiamo in .csv.
def benchmark(corpus, shuffle=True):
""""""Questa funzione esegue il benchmark per gli algoritmi di estrazione delle parole chiave""""""
logging.info(""Starting benchmark...\n"")
# Randomizziamo l'ordine del corpus
if shuffle:
random.shuffle(corpus)
# estrazione delle keyword
results = []
extractors = [
rake_extractor,
yake_extractor,
topic_rank_extractor,
position_rank_extractor,
single_rank_extractor,
multipartite_rank_extractor,
keybert_extractor,
]
for extractor in extractors:
result = extract_keywords_from_corpus(extractor, corpus)
results.append(result)
# calcolare il numero medio di parole chiave estratte
for result in results:
len_of_kw_list = []
for kws in result[""corpus_kws""].values():
len_of_kw_list.append(len(kws))
result[""avg_keywords_per_document""] = np.mean(len_of_kw_list)
# match con i pattern grammaticali
for result in results:
for idx, kws in result[""corpus_kws""].items():
match_results = []
for kw in kws:
match_results.append(match(kw))
result[""corpus_kws""][idx] = match_results
# calcolare il numero medio di parole chiave che matchano
for result in results:
len_of_matching_kws_list = []
for idx, kws in result[""corpus_kws""].items():
len_of_matching_kws_list.append(len([kw for kw in kws if kw]))
result[""avg_matched_keywords_per_document""] = np.mean(len_of_matching_kws_list)
# calcolare la percentuale media di parole chiave che matchano, decimali arrotondati a 2
result[""avg_percentage_matched_keywords""] = round(result[""avg_matched_keywords_per_document""] / result[""avg_keywords_per_document""], 2)
# creare un punteggio basato sulla percentuale media di parole chiave che matchano divisa per il tempo trascorso (in secondi)
for result in results:
elapsed_seconds = get_sec(result[""elapsed_time""]) + 0.1
# pesare il punteggio in base al tempo trascorso
result[""performance_score""] = round(result[""avg_matched_keywords_per_document""] / elapsed_seconds, 2)
# cancellare corpus_kw
for result in results:
del result[""corpus_kws""]
# creazione dataframe
df = pd.DataFrame(results)
df.to_csv(""results.csv"", index=False)
logging.info(""Benchmark finished. Results saved to results.csv"")
return df
def get_sec(time_str):
""""""Calcolare i secondi""""""
h, m, s = time_str.split(':')
return int(h) * 3600 + int(m) * 60 + int(s)
Risultati
Per lanciare il benchmark basta fare
# su un laptop di 4 anni ha impiegato circa 3 ore
results = benchmark(texts[:2000], shuffle=True)
Ed ecco i risultati
e un grafico a barre con il punteggio delle prestazioni
Rake vince su tutti gli altri algoritmi di molto secondo la formula del punteggio che è (avg_matched_keywords_per_document / time_elapsed_in_seconds). Il fatto che Rake elabori 2000 documenti in 2 secondi è impressionante, e anche se la precisione non è così alta come Yake o KeyBERT, il fattore tempo lo fa vincere sugli altri.
Se dovessimo considerare solo l'accuratezza, calcolata come il rapporto tra avg_matched_keywords_per_document e avg_keywords_per_document, otteniamo questi risultati
Rake si comporta abbastanza bene anche dal punto di vista della accuracy. Ha senso avere un punteggio di prestazioni così elevato dato il breve tempo necessario per eseguire l'estrazione.
Considerazioni finali
Se non consideriamo iltempo nell'equazione, KeyBERT avrebbe sicuramente preso il posto del vincitore come l'algoritmo più accurato in grado di estrarre parole chiave sensate.
Lo scopo di questo progetto era trovare il miglior algoritmo in termini di efficienza. Per questo compito, Rake sembra prendere quel posto.
In conclusione, se abbiamo bisogno di precisione rispetto a qualsiasi altra cosa, KeyBERT è la soluzione, altrimenti Rake o Yake. Userei Yake nei casi in cui non ho obiettivi particolari e desidero solo una soluzione equilibrata.
Riferimenti
Campos, R., Mangaravite, V., Pasquali, A., Jatowt, A., Jorge, A., Nunes, C. and Jatowt, A. (2020). YAKE! Keyword Extraction from Single Documents using Multiple Local Features. In Information Sciences Journal. Elsevier, Vol 509, pp 257–289. pdf
Campos R., Mangaravite V., Pasquali A., Jorge A.M., Nunes C., and Jatowt A. (2018). A Text Feature Based Automatic Keyword Extraction Method for Single Documents. In: Pasi G., Piwowarski B., Azzopardi L., Hanbury A. (eds). Advances in Information Retrieval. ECIR 2018 (Grenoble, France. March 26–29). Lecture Notes in Computer Science, vol 10772, pp. 684–691. pdf
Campos R., Mangaravite V., Pasquali A., Jorge A.M., Nunes C., and Jatowt A. (2018). YAKE! Collection-independent Automatic Keyword Extractor. In: Pasi G., Piwowarski B., Azzopardi L., Hanbury A. (eds). Advances in Information Retrieval. ECIR 2018 (Grenoble, France. March 26–29). Lecture Notes in Computer Science, vol 10772, pp. 806–810.
Csurfer. (n.d.). CSURFER/Rake-nltk: Python implementation of the rapid automatic keyword extraction algorithm using NLTK. Retrieved November 25, 2021, from https://github.com/csurfer/rake-nltk
Liaad. (n.d.). Liaad/Yake: Single-document unsupervised keyword extraction. Retrieved November 25, 2021, from https://github.com/LIAAD/yake
Boudinfl. (n.d.). BOUDINFL/pke: Python keyphrase extraction module. Retrieved November 25, 2021, from https://github.com/boudinfl/pke
MaartenGr. (n.d.). MAARTENGR/Keybert: Minimal keyword extraction with bert. Retrieved November 25, 2021, from https://github.com/MaartenGr/KeyBERT
Explosion. (n.d.). Explosion/spacy: 💫 industrial-strength natural language processing (NLP) in Python. Retrieved November 25, 2021, from https://github.com/explosion/spaCy
Commenti dalla community"
https://www.diariodiunanalista.it/posts/guida-alla-gestione-delle-variabili-categoriali/,"Gestire le variabili categoriali in un progetto di data science o machine learning non è compito facile. Questo tipo di lavoro richiede conoscenza profonda del campo di applicazione e una ampia conoscenza delle molteplici metodologie disponibili.
Per questo motivo, l’articolo presente si concentrerà sullo spiegare i seguenti concetti
- cosa sono le variabili categoriali e come dividerle nelle tipologie diverse nelle quali sono possono presentare
- come convertirle in valore numerico in base al loro tipo
- strumenti e tecnologie per la loro gestione usando principalmente Sklearn
La corretta gestione delle variabili categoriali può migliorare notevolmente il risultato del nostro modello predittivo o della nostra analisi. Infatti, la maggior parte delle informazioni rilevanti all’apprendimento e alla comprensione dei dati potrebbe essere contenuta proprio nelle variabili categoriali.
Ci basti pensare a dei dati tabellari, divisi per la variabile
sesso oppure per un certo
colore. Queste divisioni, in base al numero di categorie, possono far emergere notevoli differenze tra i gruppi e che possono informare l’analista o l’algoritmo di apprendimento.
Iniziamo col definire cosa sono e come possono presentarsi.
Definizione di variabile categoriale
Le variabili categoriali sono un tipo di variabile utilizzato in statistica e data science per rappresentare dati qualitativi o nominali. Queste variabili possono essere definite come una classe o una categoria di dati che non possono essere quantificati in modo continuo, ma solo in modo discreto.
Ad esempio, un esempio di variabile categoriale potrebbe essere il colore degli occhi di una persona, che può essere blu, verde o marrone.
La maggior parte dei modelli di apprendimento non funzionano con dati in formato categoriale. Occorre quindi sapere come convertire queste informazioni in formato numerico, in modo tale che siano preservate le informazioni.
Le variabili categoriali possono essere classificate in due tipi:
- Nominali
- Ordinali
Le variabili nominali sono variabili che non sono vincolate da un ordine preciso. Il sesso, il colore o dei marchi sono degli esempi di variabili nominali poiché non sono ordinabili.
Le variabili ordinali sono invece variabili categoriali divise in livelli ordinabili logicamente. Una colonna in un dataset formata da livelli come Primo, Secondo e Terzo può essere considerata una variabile categoriale ordinale.
È possibile andare più in profondità nella suddivisione delle variabili categoriali, considerando le variabili binarie e cicliche.
Una variabile binaria è semplice da comprendere: è una variabile categoriale che può assumere solo due valori.
Una variabile ciclica invece è caratterizzata da una ripetizione dei suoi valori. Ad esempio, i giorni della settimana sono ciclici, così come lo sono anche le stagioni.
Come trasformare le variabili categoriali
Ora che abbiamo definito cosa siano le variabili categoriali e come possono presentarsi, affrontiamo il discorso della loro trasformazione usando un esempio pratico - un dataset Kaggle chiamato cat-in-the-dat.
Il dataset
Questo è un dataset open source alla base di una competizione introduttiva proprio alla gestione e modellazione delle variabili categoriali, chiamata Categorical Feature Encoding Challenge II. È possibile scaricare il dato direttamente dal link qui in basso.
La particolarità di questo dataset è che contiene solo dati categoriali. Diventa quindi il caso d’uso perfetto per questa guida. Include variabili nominali, ordinali, cicliche e binarie.
Vedremo delle tecniche per trasformare ogni variabile in un formato usabile da un modello di apprendimento.
Il dataset si presenta così
Dato che la variabile target può assumere solo due valori, questo è un compito di classificazione binaria. Useremo la metrica AUC per valutare il nostro modello.
Ora andremo ad applicare delle tecniche di gestione delle variabili categoriali usando il dataset menzionato.
1. Label Encoding (mappatura ad un numero arbitrario)
La tecnica più semplice che c’è per convertire una categoria in un formato usabile è quella di assegnare ogni categoria ad un numero arbitrario.
Prendiamo ad esempio la colonna
ord_2 che contiene questi valori.
array(['Hot', 'Warm', 'Freezing', 'Lava Hot', 'Cold', 'Boiling Hot', nan],
dtype=object)
La mappatura potrebbe avvenire in questo modo usando Python e Pandas:
df_train = train.copy()
mapping = {
""Cold"": 0,
""Hot"": 1,
""Lava Hot"": 2,
""Boiling Hot"": 3,
""Freezing"": 4,
""Warm"": 5
}
df_train[""ord_2""].map(mapping)
>>
0 1.0
1 5.0
2 4.0
3 2.0
4 0.0
...
599995 4.0
599996 3.0
599997 4.0
599998 5.0
599999 3.0
Name: ord_2, Length: 600000, dtype: float64
Questa metodica ha un problema però: occorre dichiarare manualmente la mappatura. Per un numero limitato di categorie non è un problema, ma per un numero elevato potrebbe esserlo.
Per questo useremo Scikit-Learn e l’oggetto
LabelEncoder per ottenere lo stesso risultato in maniera più flessibile.
from sklearn import preprocessing
# gestiamo i valori vuoti
df_train[""ord_2""].fillna(""NONE"", inplace=True)
# inizializzamo l'encoder di sklearn
le = preprocessing.LabelEncoder()
# fit + transform
df_train[""ord_2""] = le.fit_transform(df_train[""ord_2""])
df_train[""ord_2""]
>>
0 3
1 6
2 2
3 4
4 1
..
599995 2
599996 0
599997 2
599998 6
599999 0
Name: ord_2, Length: 600000, dtype: int64
La mappatura è controllata da Sklearn. Possiamo visualizzarla in questo modo:
mapping = {label: index for index, label in enumerate(le.classes_)}
mapping
>>
{'Boiling Hot': 0,
'Cold': 1,
'Freezing': 2,
'Hot': 3,
'Lava Hot': 4,
'NONE': 5,
'Warm': 6}
È da notare il
.fillna(""NONE"") nello snippet di code di cui sopra. Di fatto, il label encoder di Sklearn non gestisce i valori vuoti e darà errore durante la sua applicazione se ne trova.
Una delle cose più importanti da tenere a mente per la corretta gestione delle variabili categoriali è quella di gestire sempre i valori vuoti presenti. Infatti, la maggior parte delle tecniche più rilevanti non funziona con valori mancanti.
Il label encoder mappa numeri arbitrari ad ogni categoria presente nella colonna, senza una dichiarazione esplicita della mappatura. Questo è comodo, ma introduce una problematica per alcuni modelli predittivi: introduce la necessità di scalare il dato se la colonna non è quella di target.
Infatti, spesso i neofiti del machine learning chiedono quale sia la differenza tra label encoder e one hot encoder, che vedremo tra poco. Il label encoder, per design, dovrebbe essere applicato alle label, cioè alla variabile target che vogliamo predire e non alle altre colonne.
Detto ciò, alcuni modelli anche molto rilevanti nel campo funzionano bene anche con un encoding di questo tipo. Sto parlando di modelli ad albero, tra cui spiccano XGBoost e LightGBM.
Quindi sentiamoci liberi di usare label encoder se decidiamo di usare modelli ad albero, ma per gli altri casi, dobbiamo usare il one hot encoding.
2. One Hot Encoding
Come ho già menzionato nel mio articolo riguardo le rappresentazioni vettoriali nel machine learning, il one hot encoding è una tecnica di vettorizzazione (cioè conversione di un testo in numero) molto comune e famosa.
Funziona così: per ogni categoria presente, si crea una matrice quadrata i cui unici valori possibili sono 0 e 1. Questa matrice informa il modello che tra tutte le possibili categorie, questa riga osservata ha il valore denotato dall’1.
Un esempio:
|Freezing
|0
|0
|0
|0
|0
|1
|Warm
|0
|0
|0
|0
|1
|0
|Cold
|0
|0
|0
|1
|0
|0
|Boiling Hot
|0
|0
|1
|0
|0
|0
|Hot
|0
|1
|0
|0
|0
|0
|Lava Hot
|1
|0
|0
|0
|0
|0
La matrice è di dimensione n_categorie. Questo è una informazione molto utile, perché il one hot encoding tipicamente richiede una rappresentazione sparsa del dato convertito. Cosa significa? Significa che per grandi numeri di categorie, la matrice potrebbe assumere dimensioni altrettanto grandi. Essendo popolata solo da valori di 0 e 1 e poiché solo una delle posizioni può essere popolata da un 1, questo rende la rappresentazione one hot molto ridondante e pesante.
Una matrice sparsa risolve questo problema - vengono salvate solamente le posizioni degli 1, mentre i valori uguali a 0 non vengono salvati. Questo snellisce il problema menzionato e permette di salvare una matrice enorme di informazioni in pochissimo spazio.
Vediamo come si presenta una tale matrice in Python, applicando nuovamente il codice di prima
from sklearn import preprocessing
# gestiamo i valori vuoti
df_train[""ord_2""].fillna(""NONE"", inplace=True)
# inizializzamo l'encoder di sklearn
ohe = preprocessing.OneHotEncoder()
# fit + transform
ohe.fit_transform(df_train[""ord_2""].values.reshape(-1, 1))
>>
<600000x7 sparse matrix of type '<class 'numpy.float64'>'
with 600000 stored elements in Compressed Sparse Row format>
Python restituisce un oggetto di default, e non una lista di valori. Per avere tale lista, bisogna usare
.toarray()
ohe.fit_transform(df_train[""ord_2""].values.reshape(-1, 1)).toarray()
>>
array([[0., 0., 0., ..., 0., 0., 0.],
[0., 0., 0., ..., 0., 0., 1.],
[0., 0., 1., ..., 0., 0., 0.],
...,
[0., 0., 1., ..., 0., 0., 0.],
[0., 0., 0., ..., 0., 0., 1.],
[1., 0., 0., ..., 0., 0., 0.]])
Non preoccupatevi se non comprendete a pieno il concetto: a breve vedremo come applicare il label e one hot encoder al dataset per addestrare un modello predittivo.
3. Trasformazioni e aggregazioni
Un possibile metodo di conversione da formato categoriale a numerico è quello di effettuare una trasformazione o una aggregazione sulla variabile.
Facendo un raggruppamento con
.groupby è possibile usare il conteggio dei valori presenti nella colonna come output della trasformazione.
df_train.groupby([""ord_2""])[""id""].count()
>>
ord_2
Boiling Hot 84790
Cold 97822
Freezing 142726
Hot 67508
Lava Hot 64840
Warm 124239
Name: id, dtype: int64
usando
.transform() possiamo sostituire questi numeri alla corrispettiva cella
df_train.groupby([""ord_2""])[""id""].transform(""count"")
>>
0 67508.0
1 124239.0
2 142726.0
3 64840.0
4 97822.0
...
599995 142726.0
599996 84790.0
599997 142726.0
599998 124239.0
599999 84790.0
Name: id, Length: 600000, dtype: float64
È possibile applicare questa logica anche con altre operazioni matematiche - va testato il metodo che più migliora le performance del nostro modello.
4. Creare nuove feature categoriali da variabili categoriali
Guardiamo la colonna ord_1 insieme a ord_2
È possibile creare nuove variabili categoriali unendo le variabili esistenti. Ad esempio, possiamo unire ord_1 con ord_2 per creare una nuova feature
df_train[""new_1""] = df_train[""ord_1""].astype(str) + ""_"" + df_train[""ord_2""].astype(str)
df_train[""new_1""]
>>
0 Contributor_Hot
1 Grandmaster_Warm
2 nan_Freezing
3 Novice_Lava Hot
4 Grandmaster_Cold
...
599995 Novice_Freezing
599996 Novice_Boiling Hot
599997 Contributor_Freezing
599998 Master_Warm
599999 Contributor_Boiling Hot
Name: new_1, Length: 600000, dtype: object
Questa tecnica può essere applicata in praticamente ogni casistica. L’idea che deve guidare l’analista è quella di migliorare le performance del modello andando ad aggiungere informazioni originariamente difficilmente comprensibili al modello di apprendimento.
5. Usare i NaN come variabile categoriale
Molto spesso i valori nulli vengono rimossi. Questa non è tipicamente una mossa che io consiglio, in quanto nei NaN sono contenute informazioni potenzialmente utili al nostro modello.
Una soluzione è quella di trattare i NaN come una categoria a se stante.
Guardiamo nuovamente la colonna ord_2
df_train[""ord_2""].value_counts()
>>
Freezing 142726
Warm 124239
Cold 97822
Boiling Hot 84790
Hot 67508
Lava Hot 64840
Name: ord_2, dtype: int64
Ora proviamo ad applicare il
.fillna(""NONE"") per vedere quante celle vuote esistono
df_train[""ord_2""].fillna(""NONE"").value_counts()
>>
Freezing 142726
Warm 124239
Cold 97822
Boiling Hot 84790
Hot 67508
Lava Hot 64840
NONE 18075
In percentuale, NONE rappresenta circa il 3% dell’intera colonna. Non è poco. Sfruttare il NaN ha ancora più senso e può essere fatto con il One Hot Encoder menzionato poco fa.
Tenere traccia di categorie rare
Ricordiamoci cosa fa il
OneHotEncoder: crea una matrice sparsa il cui numero di colonne e righe è uguale al numero delle categorie uniche nella colonna di riferimento. Questo significa che dobbiamo tener conto anche delle categorie che potrebbero essere presenti nel test set e che potrebbero essere assenti nel train set.
Il discorso è analogo per il
LabelEncoder - potrebbero esistere delle categorie nel test set ma che non sono presenti nel training set e questo potrebbe crearci problemi in fase di trasformazione.
Risolviamo questo problema andando a concatenare i dataset. Questo ci permetterà di applicare gli encoder su tutti i dati e non solo su quelli di addestramento.
test[""target""] = -1
data = pd.concat([train, test]).reset_index(drop=True)
features = [f for f in train.columns if f not in [""id"", ""target""]]
for feature in features:
le = preprocessing.LabelEncoder()
temp_col = data[feature].fillna(""NONE"").astype(str).values
data.loc[:, feature] = le.fit_transform(temp_col)
train = data[data[""target""] != -1].reset_index(drop=True)
test = data[data[""target""] == -1].reset_index(drop=True)
Questa metodologia ci aiuta se abbiamo il test set. Qualora non avessimo il test set, terremo conto di un valore come NONE quando una categoria nuova entrerà a far parte del nostro training set.
Modellare i dati categoriali
Passiamo ora alla parte di addestramento di un semplice modello. Seguiremo i passi dall’articolo su come progettare e implementare una cross-validazione al seguente link 👇
Iniziamo da zero, importando i nostri dati e creando i nostri fold con
StratifiedKFold di Sklearn.
train = pd.read_csv(""/kaggle/input/cat-in-the-dat-ii/train.csv"")
test = pd.read_csv(""/kaggle/input/cat-in-the-dat-ii/test.csv"")
df = train.copy()
df[""kfold""] = -1
df = df.sample(frac=1).reset_index(drop=True)
y = df.target.values
kf = model_selection.StratifiedKFold(n_splits=5)
for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):
df.loc[v_, 'kfold'] = f
Questo piccolo snippet di code creerà un dataframe Pandas con 5 gruppi su cui testare il nostro modello.
Ora andiamo a definire una funzione che andrà a testare un modello regressione logistica su ogni gruppo.
def run(fold: int) -> None:
features = [
f for f in df.columns if f not in (""id"", ""target"", ""kfold"")
]
for feature in features:
df.loc[:, feature] = df[feature].astype(str).fillna(""NONE"")
df_train = df[df[""kfold""] != fold].reset_index(drop=True)
df_valid = df[df[""kfold""] == fold].reset_index(drop=True)
ohe = preprocessing.OneHotEncoder()
full_data = pd.concat([df_train[features], df_valid[features]], axis=0)
print(""Fitting OHE on full data..."")
ohe.fit(full_data[features])
x_train = ohe.transform(df_train[features])
x_valid = ohe.transform(df_valid[features])
print(""Training the classifier..."")
model = linear_model.LogisticRegression()
model.fit(x_train, df_train.target.values)
valid_preds = model.predict_proba(x_valid)[:, 1]
auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)
print(f""FOLD: {fold} | AUC = {auc:.3f}"")
run(0)
>>
Fitting OHE on full data...
Training the classifier...
FOLD: 0 | AUC = 0.785
Invito il lettore interessato a leggere l’articolo sulla cross-validazione comprendere più in dettaglio il funzionamento del codice mostrato.
Vediamo ora come invece applicare un modello ad albero come XGBoost, che funziona bene anche con un LabelEncoder.
def run(fold: int) -> None:
features = [
f for f in df.columns if f not in (""id"", ""target"", ""kfold"")
]
for feature in features:
df.loc[:, feature] = df[feature].astype(str).fillna(""NONE"")
print(""Fitting the LabelEncoder on the features..."")
for feature in features:
le = preprocessing.LabelEncoder()
le.fit(df[feature])
df.loc[:, feature] = le.transform(df[feature])
df_train = df[df[""kfold""] != fold].reset_index(drop=True)
df_valid = df[df[""kfold""] == fold].reset_index(drop=True)
x_train = df_train[features].values
x_valid = df_valid[features].values
print(""Training the classifier..."")
model = xgboost.XGBClassifier(n_jobs=-1, n_estimators=300)
model.fit(x_train, df_train.target.values)
valid_preds = model.predict_proba(x_valid)[:, 1]
auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)
print(f""FOLD: {fold} | AUC = {auc:.3f}"")
# eseguiamo su 2 fold
for fold in range(2):
run(fold)
>>
Fitting the LabelEncoder on the features...
Training the classifier...
FOLD: 0 | AUC = 0.768
Fitting the LabelEncoder on the features...
Training the classifier...
FOLD: 1 | AUC = 0.765
Conclusioni
In conclusione, esistono anche altre tecniche che vale la pena menzionare per la gestione delle variabili categoriali:
- L’encoding basato sul target, dove si converte la categoria nel valore medio che assume la variabile target in corrispondenza della stessa
- gli embedding di una rete neurale, che possono essere utilizzati per rappresentare l’entità testuale
- trattare sempre i valori vuoti
- applicare
LabelEncoder o
OneHotEncoder in base al tipo di variabile e modello che vogliamo usare
- ragionare in termini di arricchimento delle variabili, andando a considerare i NaN o NONE come variabili categoriali che possono informare il modello
- Modeling!
Commenti dalla community"
https://www.diariodiunanalista.it/posts/guida-prompt-engineering/,"L'avvento degli LLM (Large Language Models) ha permesso al termine ""AI"" di farsi spazio nel vocabolario ormai giornaliero della maggior parte degli internauti.
Sistemi come ChatGPT vengono interrogati milioni di volte al giorno per rispondere ai quesiti degli utenti, che stanno si stanno gradualmente chiedendo se il paradigma dei motori di ricerca non sia effettivamente una cosa del passato.
Il prompt engineering è un termine che è associato agli LLM ed è anch'esso molto utilizzato oggi dai fruitori di tali sistemi. Consiste nello sviluppare e ottimizzare i prompt (cioè le domande che vengono poste al modello da parte dell'umano) al fine di ottenere risposte più precise da parte dei modelli linguistici.
Mentre i ricercatori utilizzano il prompt engineering per migliorare la capacità dei LLM su un'ampia gamma di compiti comuni e complessi, gli sviluppatori lo utilizzano per progettare tecniche di prompt robuste ed efficaci che si interfacciano con LLM e altri strumenti.
Questo articolo ha l'obiettivo di introdurti a delle tecniche emergenti di prompt engineering che puoi implementare già da subito nel tuo progetto. Infatti, il prompt engineering non è solamente una banale ottimizzazione testuale delle domande da porre al modello, ma una serie di logiche che in alcuni casi si interfacciano proprio al funzionamento interno degli LLM.
- Come progettare un prompt, andando a comprendere requisiti e anatomia degli elementi che lo compongono
- Alcune delle tecniche più efficaci di prompt engineering
- Casi d'uso ed esempi
Iniziamo subito!
Basi del prompting
Iniziamo dai concetti semplici. È possibile ottenere molto da un modello come ChatGPT chiedendo domande semplici, come le si porrebbero ad un essere umano...ma l'esempio che qui in basso mostra come il modello non si comporti come ci si aspetta al prompt Sopra la panca
Basta aggiungere un incipit e il modello comprende e risponde correttamente
Questo è un esempio molto basilare di prompt engineering, poiché abbiamo ottimizzato la richiesta testuale per ottenere una risposta in linea con le nostre aspettative.
Un prompt non è sempre una domanda - infatti nel caso appena visto si tratta di una istruzione. In una tecnica che vedremo dopo chiamata zero-shot prompting, l'utente può differenziare tra i due tipi di formati e fornire al modello una scaletta da seguire (o meglio, riempire).
R:
Qui formattiamo la domanda con il token di apertura D: e forniamo un template per la risposta che il modello andrà semplicemente a continuare. Questa tecnica funziona anche con altri tipi di richieste, non solo coppie domande-risposta.
Fornendo più esempi stiamo applicando una tecnica chiamata few-shot prompting
Sorriso | Salute
Tosse | Malattia
Sport | Salute
Divertirsi | ?
Vediamo come ChatGPT stesso menzioni la parola correlazione. È di fatto proprio questo il tipo di ragionamento che cerchiamo di stimolare andando a usare il few-shot prompting.
Questi esempi mostrano come delle semplici domande o istruzioni possano essere difficili da interpretare da parte del LLM e che esistono delle tecniche, anche semplici, che permettono al modello di rimuovere ambiguità e fornire delle risposte più adeguate.
Progettazione del prompt
Come praticamente tutto nella data science e nel machine learning, anche il prompt engineering è di fatto una attività iterativa.
Significa che bisogna sperimentare per ottenere il risultato atteso.
Scomporremo la fase di progettazione del prompt in vari requisiti che ti aiuteranno sia a comprendere come ragiona un LLM che a controllare il comportamento del modello.
Ti piace quello che stai leggendo?
Iscriviti al blog per rimanere sempre aggiornato sui contenuti di settore. No pubblicità. No spam.
Solo contenuto utile per la tua carriera nel ML e DS.
Puoi rimuovere l'iscrizione quando vuoi seguendo il link nella email.
Requisito # 1: Comprendere l'anatomia del prompt
Uno dei primi passi per ottenere risultati più velocemente è comprendere gli elementi che compongono un buon prompt e che linguaggio usare per massimizzare i risultati.
Analizzando la struttura di un prompt, è possibile estrarre diversi elementi concettuali che si ripetono: entry point, istruzione, contesto, input dell'utente, indicatore di output.
Contesto
Porzione iniziale del prompt. Indica al modello uno scenario rappresentativo che deve assumere per meglio interpretare l'istruzione.
Sei un esperto di negoziazione e sei disponibile a dare consigli a chi te lo chiede. Il tuo obiettivo è quello di aiutarmi a convincere il mio capo a implementare lo smart working nel luogo di lavoro. # <- CONTESTO
...
Istruzione
Il secondo step per scrivere un buon prompt è comunicare al LLM quello che deve fare. Usare parole come scrivi, classifica, rimuovi, modifica, riassumi permetterà al modello di performare meglio sul compito.
Sei un esperto di negoziazione e sei disponibile a dare consigli a chi te lo chiede. Il tuo obiettivo è quello di aiutarmi a convincere il mio capo a implementare lo smart working nel luogo di lavoro. # <- CONTESTO
Data questa conversazione, dimmi cosa diresti all'interlocutore da negoziatore esperto. # <- ISTRUZIONE
...
Input umano
La componente che si presenta il 100% delle volte, ma che da sola ottiene risultati di qualità variabile. Seguita da un contesto e da istruzioni chiare, l'input umano risulta molto più interpretabile da parte del modello.
Sei un esperto di negoziazione e sei disponibile a dare consigli a chi te lo chiede. Il tuo obiettivo è quello di aiutarmi a convincere il mio capo a implementare lo smart working nel luogo di lavoro. # <- CONTESTO
Data questa conversazione, dimmi cosa diresti all'interlocutore da negoziatore esperto. # <- ISTRUZIONE
Conversazione: # <- INPUT UMANO
...
Indicatore di output
Indicare quando il modello deve restituire un output faciliterà il suo compito durante l'inferenza. È possibile specificare il tipo o il formato della risposta per migliorare la qualità della risposta.
Sei un esperto di negoziazione e sei disponibile a dare consigli a chi te lo chiede. Il tuo obiettivo è quello di aiutarmi a convincere il mio capo a implementare lo smart working nel luogo di lavoro. # <- CONTESTO
Data questa conversazione, dimmi cosa diresti all'interlocutore da negoziatore esperto. # <- ISTRUZIONE
Conversazione: # <- INPUT UMANO
...
---
Nuova conversazione: # <- INDICATORE DI OUTPUT
Mettendo insieme tutti i pezzi, avremmo un prompt così composto:
Non tutti questi elementi sono necessari per avere una risposta da parte del modello, ma averli aiuta.
Requisito # 2: Massimizzare la chiarezza minimizzando la lunghezza del testo
Il nostro prompt potrà anche avere tutti gli elementi menzionati, ma se non è chiaro allora il modello interpellato avrà difficoltà a rispondere in maniera soddisfacente.
Occorre essere molto specifici riguardo alle istruzioni e al compito che desideri che il modello esegua. Più il prompt è descrittivo e dettagliato, migliori saranno i risultati.
Aggiungere dettagli ai fini della chiarezza porta con sé un costo: un testo più lungo.
Testi più lunghi sono vincolati dalla context window - il modello troncherà la risposta e non potrà più processare la richiesta.
I dettagli quindi devono essere pertinenti e contribuire al compito da svolgere. Fornire esempi nel prompt è molto efficace per ottenere l'output desiderato in formati specifici.
Requisito # 3: Dire al modello cosa NON fare
È altrettanto utile dire al modello cosa non deve restituire all'utente. Molto spesso, gli LLM tendono ad aggiunere parole in più che rendono il testo più conversazionale. Se non vogliamo questo, dobbiamo espressamente dire al modello cosa non deve fare.
Sei un esperto di negoziazione e sei disponibile a dare consigli a chi te lo chiede. Il tuo obiettivo è quello di aiutarmi a convincere il mio capo a implementare lo smart working nel luogo di lavoro.
Data questa conversazione, dimmi cosa diresti all'interlocutore da negoziatore esperto.
Quando rispondi, non fare mai riferimento agli altri dipendenti. <- VINCOLO ALLA RISPOSTA
Conversazione:
...
---
Nuova conversazione:
Tecniche di Prompt Engineering
Abbiamo già visto un esempio di zero-shot e few-shot prompting. Per riassumere, si fornisce al modello una istruzione diretta e si chiede ad esempio di classificare l'input.
Classifica il sentiment della seguente affermazione.
Affermazione: mi sento molto bene oggi!
Sentiment:
In questo caso stiamo sfruttando le capacità del modello per l'inferenza, poiché non spieghiamo cosa sia la parola ""sentiment"". Ha già questa nozione, e noi sfruttiamo questa situazione.
Nel caso in cui lo zero-shot prompting non da buoni risultati, provare con il few-shot tipicamente porta buoni risultati.
Ora passiamo ad una rassegna delle tecniche più efficaci attualmente usate dai prompt engineer per sfruttare al massimo le potenzialità degli LLM.
Tecnica # 1: Chain-of-Thought
Tradotto significa catena di pensieri ed è una tecnica di prompting che aiutare il modello nel ragionamento fornendo degli step intermedi che lo aiutano a comprendere meglio il task.
Gli autori di questa tecnica (Wei et al. 2022), mettono a disposizione questa immagine che aiuta a capire meglio come funziona il CoT.
Dall'immagine è possibile vedere come il prompt ""standard"" di sinistra sia solo una lista di domande e risposte. Il CoT si distingue perché permette al modello di estrarre un pattern di ragionamento dalla risposta alla domanda, che fornisce l'utente. Comprendendo la sequenza di step, il modello è in grado di replicarlo e applicarlo anche ad un problema nuovo.
Il CoT è spesso accompagnato dal few-shot prompting, perché espone al LLM più pattern di ragionamento.
Automatic Chain-of-Thought (AutoCoT)
Questa è una variante del CoT, che semplifica la creazione di prompt basati su questa tecnica.
Aggiungere una frase come ""Ragioniamo passo dopo passo"" permette al modello di generare step intermedi di ragionamento che portano ad un risultato più accurato.
Tecnica # 2: ReAct
ReAct sta per Reason + Act ed è una tecnica pubblicata la prima volta da Yao et al. nel 2022 e si basa sul Chain of Thought.
La tecnica consiste nel fornire al modello tracce di ragionamento (reasoning traces) e azioni da compiere specifiche per il task (task-specific actions).
Le reasoning traces aiutano il modello a indurre, tracciare e aggiornare le strategie di esecuzione mentre le task-specifc action consentono al modello di interfacciarsi con l'utente o fonti esterne per raccogliere informazioni aggiuntive.
Esatto. ReAct presuppone che il nostro modello possa interagire con il mondo esterno.
Ecco un esempio di come un LLM reagisce alla ricerca nel framework ReAct.
Una libreria come LangChain permette all'utente di usare tale framework in maniera semplice.
Prendendo l'esempio dalla pagina linkata, vediamo come sia possibile specificare dei tool per permettere al modello di fare azioni su internet (come navigare una pagina web).
from langchain.llms import OpenAI
from langchain.docstore import Wikipedia
from langchain.agents import initialize_agent, Tool
from langchain.agents import AgentType
from langchain.agents.react.base import DocstoreExplorer
docstore = DocstoreExplorer(Wikipedia()) # sistema per salvare la ricerca
# definizione degli strumenti
tools = [
Tool(
name=""Search"",
func=docstore.search,
description=""useful for when you need to ask with search"",
),
Tool(
name=""Lookup"",
func=docstore.lookup,
description=""useful for when you need to ask with lookup"",
),
]
# creazione del modello
llm = OpenAI(temperature=0, model_name=""text-davinci-002"")
react = initialize_agent(tools, llm, agent=AgentType.REACT_DOCSTORE, verbose=True)
question = ""Author David Chanoff has collaborated with a U.S. Navy admiral who served as the ambassador to the United Kingdom under which President?""
react.run(question)
La ""difficoltà"" nell'usare ReAct è che bisogna saper programmare in Python e conoscere le basi della libreria LangChain.
Tecnica # 3: Tree of Thoughts (ToT)
Si basa anch'essa sul Chain of Thoughts, solo che è una tecnica incoraggia l'esplorazione dei pensieri nella CoT che fungono da passaggi intermedi per la risoluzione generale dei problemi da parte degli LLM.
ToT mantiene un albero di ragionamenti fatti dal LLM, dove i pensieri rappresentano sequenze linguistiche coerenti che servono come passaggi intermedi verso la risoluzione di un problema. Questo approccio consente al LLM non solo di dichiarare, ma di valutare come questi step intermedi aiutano a risolvere il problema.
La capacità del LM di generare e valutare gli step intermedi viene combinata con algoritmi di ricerca (ad esempio, ricerca in ampiezza o in profondità) per consentire l'esplorazione sistematica dei pensieri in tutte le direzioni dell'albero.
Per costruire un prompt per un LLM usando il ToT faremo riferimento al repository git di Dave Hulbert:
Ipotizziamo un prompt del genere, che chiede una ricostruzione precisa degli eventi per capire dove si trova la palla,
Vediamo come ChatGPT abbia sbagliato valutazione. Infatti, la palla si trova in camera da letto.
Applichiamo ora il ToT al prompt precedente aggiungendo questo blocco:
Immagina che tre diversi esperti stiano rispondendo a questa domanda.
Tutti gli esperti scrivono 1 passaggio del loro pensiero, e poi lo condividono con il gruppo. In seguito, tutti gli esperti passeranno al passaggio successivo, ecc.
Se qualche esperto si rende conto di aver sbagliato in qualsiasi momento, se ne va.
La domanda è:
ChatGPT, che è basato in questo caso sulla versione free di GPT-3.5, risponde correttamente. Notevole!
Il ToT performa, ovviamente, meglio con GPT-4 che con GPT-3.5 ma usando un po' di astuzia e seguendo il framework, allora anche quest'ultimo è in grado di dare informazioni corretta a domande complesse.
Tecnica # 4: Optimization by PROmpting (OPRO)
OPRO è una tecnica introdotta da Google DeepMind che mira ad ottimizzare la risposta degli LLM. Fondamentalmente, OPRO sfrutta gli LLM per valutare iterativamente gli output e ottimizzare i prompt.
OPRO ottimizza in modo iterativo i prompt per generare continuamente nuove soluzioni, perfezionando i propri output in base alla descrizione del problema e alle soluzioni scoperte in precedenza.
""OPRO consente al LLM di generare gradualmente nuovi prompt che migliorano la precisione dell'attività durante tutto il processo di ottimizzazione, in cui i prompt iniziali sono associati ad una bassa precisione dell'attività"", scrivono gli autori del paper.
Il framework è progettato per integrare 2 LLM (un ottimizzatore e un valutatore) per migliorare in modo iterativo l'output del meta-prompt.
Supponiamo di avere questo prompt per estrarre delle keyword rilevanti da un testo:
Dato del contenuto testuale, estrai le keyword più rilevanti che informano dell'argomento trattato.
Esempio 1:
Nell'ambito della data science e del machine learning un dataset multidimensionale è una raccolta di dati organizzata in modo da includere molteplici colonne o attributi, ciascuna delle quali rappresenta una caratteristica del fenomeno che si sta studiando o cercando di predire.
Keyword: data science, machine learning, dataset, caratteristica, fenomeno
Esempio 2:
In base al numero di componenti che scegliamo, l'algoritmo PCA permette la riduzione del numero di variabili nel dataset originale andando a preservare quelle che spiegano meglio la varianza totale del dataset stesso, andando a combattere la maledizione della dimensionalità (curse of dimensionality).
Keyword: PCA, variabili, dataset, maledizione della dimensionalità
TESTO:
{{ testo in input }}
Ora questo prompt viene in realtà definito come meta-prompt e viene fornito ad un LLM valutatore per analizzarlo e migliorarlo. Questo modello interpreta la richiesta di comprendere il compito di classificazione, rivedendo gli esempi e la tipologia di dati con cui ha a che fare. L'LLM genera una serie di nuovi suggerimenti che ritiene potrebbero migliorare i risultati della classificazione o estrazione di informazioni.
Dato questo meta-prompt, i suoi obiettivi e esempi, genera una serie di nuovi prompt che potrebbero potenzialmente portare a risultati migliori nell'estrazione delle keyword in base al contenuto presente nei testi. Puntare a suggerimenti chiari, concisi e in grado di guidare il modello in modo efficace.
{{ meta-prompt }}
Il modello quindi genererà diversi prompt che verranno poi inseriti nel meta-prompt stesso.
Dopo aver testato questi prompt, i relativi risultati verranno aggiunti al meta-prompt:
Dato del contenuto testuale, estrai le keyword più rilevanti che informano dell'argomento trattato.
Esempio 1:
Nell'ambito della data science e del machine learning un dataset multidimensionale è una raccolta di dati organizzata in modo da includere molteplici colonne o attributi, ciascuna delle quali rappresenta una caratteristica del fenomeno che si sta studiando o cercando di predire.
Keyword: data science, machine learning, dataset, caratteristica, fenomeno
Esempio 2:
In base al numero di componenti che scegliamo, l'algoritmo PCA permette la riduzione del numero di variabili nel dataset originale andando a preservare quelle che spiegano meglio la varianza totale del dataset stesso, andando a combattere la maledizione della dimensionalità (curse of dimensionality).
Keyword: PCA, variabili, dataset, maledizione della dimensionalità
PROMPT GENERATI PRECEDENTEMENTE E RELATIVE PERFORMANCE:
Estrai le parole chiave principali dal seguente testo: 88%
Identifica i termini chiave nel testo seguente: 78%
Quali parole chiave emergono dal testo sottostante? 40%
Dall'estratto seguente, individua le parole chiave più rilevanti: 78%
Riporta le parole chiave cruciali contenute nel passaggio successivo: 80%
Da qui, il processo iterativo continua. Dopo aver valutato i nuovi prompt, viene utilizzato il feedback per perfezionare il meta-prompt e il processo viene ripetuto finché non si è soddisfatti delle prestazioni del prompt.
Di recente questa tecnica è stata parecchio discussa online, sottolineando che gli autori sono arrivati ad inserire una frase del tipo ""Take a deep breath and work on this problem step-by-step."" (Fai un respiro profondo e lavora su questo problema passo dopo passo). Questa semplice inclusione nel prompt ha migliorato drasticamente la performance degli LLM coinvolti.
La sfida di usare questa tecnica è quella di registrare le varie performance delle risposte ai prompt creati dagli LLM per il modello valutatore. Può valerne sicuramente la pena se pensiamo alle performance che possiamo raggiungere con il prompt finale.
Conclusioni
Hai imparato le basi del prompt engineering e alcune tecniche importanti per aumentare le performance delle risposte degli LLM.
In particolare hai letto di
- basi del prompting, e di come il prompting semplice non porta a risultati eccellenti
- come si progetta un prompt e la sua anatomia
- i requisiti per creare un buon prompt
- alcune delle tecniche più efficaci per massimizzare le performance degli LLM
Questo articolo sarà periodicamente aggiornato per includere le tecniche più valide nello spazio del prompt engineering.
A presto,
Andrea
Commenti dalla community"
https://www.diariodiunanalista.it/posts/il-compromesso-bias-varianza-nel-machine-learning/,"In questo articolo esplorerò un altro concetto fondamentale nella nostra battaglia contro l'over/underfitting: il compromesso bias-varianza.
Uno dei principali obiettivi del machine learning è quello di costruire modelli che possano generalizzare bene su dati che non sono stati ancora visti.
Quando si costruisce un modello, è importante assicurarsi che non sia troppo semplice da non poter catturare le complessità del problema o troppo complesso da sovrapporsi ai dati di addestramento.
In questo articolo, discuteremo del compromesso tra bias e varianza nel machine learning e di come trovare il giusto equilibrio tra i due può aiutare a migliorare le prestazioni del modello.
Iniziamo definendo cosa sia il bias e la varianza:
Bias: rappresenta l'errore di previsione del modello dovuto alle sue semplificazioni. Ad esempio, un modello lineare potrebbe avere un bias elevato se il problema è intrinsecamente non lineare.
Bias vuol dire pregiudizio. Rappresenta quanto il modello sia incline ad interpretare il dato in una certa maniera prima di vedere il dato stesso.
Varianza: rappresenta l'errore di previsione del modello dovuto alla sua sensibilità ai dati di addestramento. Ad esempio, un modello che si adatta molto bene ai dati di addestramento potrebbe avere una varianza elevata se non generalizza bene su dati non visti.
La varianza indica quanto il modello è adattato ai dati di addestramento. Un modello con una varianza elevata si adatta molto bene ai dati di addestramento ma generalizza male su dati non visti.
Ad esempio, immaginiamo di avere un modello di regressione che prevede il prezzo delle case in base a diversi fattori come la dimensione, la posizione, il numero di camere da letto, ecc.
Se il modello ha una varianza elevata, significa che si adatta troppo ai dati di addestramento e potrebbe prevedere prezzi sbagliati per le case che non sono presenti nel set di addestramento.
D'altro canto, se il modello ha una varianza bassa, potrebbe non essere in grado di catturare le complessità del problema e di conseguenza generalizzare male su nuovi dati.
Come trovare il giusto equilibrio
L'obiettivo principale del machine learning è di ridurre l'errore di generalizzazione del modello, ovvero l'errore che si verifica quando il modello viene applicato a dati non visti. Per farlo, è necessario trovare il giusto equilibrio tra bias e varianza.
La scelta del giusto modello dipende dalle esigenze del problema. Ecco una lista di spunti utili su cui ragionare per comprendere come bilanciare il nostro modello.
- Valutare la complessità del modello: modelli semplici come le regressioni lineari hanno un alto bias e bassa varianza, mentre i modelli complessi come le reti neurali hanno un basso bias e alta varianza.
- Dimensione del dataset: aumentando la dimensione del dataset, è possibile ridurre la varianza del modello. Infatti, con più dati di addestramento, il modello avrà maggiori informazioni per generalizzare e ridurre l'adattamento ai dati di addestramento.
- Regolarizzazione: la regolarizzazione è una tecnica utilizzata per controllare la complessità del modello. Ad esempio, la regolarizzazione L1 e L2 possono aiutare a ridurre la varianza del modello.
- Cross-validazione: la validazione incrociata è una tecnica utilizzata per valutare le prestazioni del modello su dati non visti. Ciò aiuta a evitare l'overfitting e a trovare il giusto equilibrio tra bias e varianza.
- Selezione delle feature: la selezione delle feature è un'altra tecnica utilizzata per controllare la complessità del modello. Rimuovere le feature non rilevanti o ridondanti può aiutare a ridurre la varianza del modello.
È importante ricordare che non esiste un unico modello perfetto per tutti i problemi di machine learning. È necessario valutare attentamente le esigenze specifiche del problema e scegliere il modello più adatto per quel contesto.
Per questo motivo l'analista dovrebbe iterare tra diversi modelli, attraverso una fase chiamata selezione del modello, e selezionare quello più performante per il dato problema.
Inoltre, è importante tenere sempre presente che il bilanciamento tra bias e varianza è un processo continuo e dinamico. Le prestazioni del modello possono variare nel tempo e a seconda delle nuove informazioni a disposizione. Pertanto, è necessario monitorare costantemente le prestazioni del modello e apportare eventuali modifiche quando necessario.
Esempi di vari livelli di bilanciamento di bias e varianza nel machine learning
Vediamo come si comportano modelli tra i vari livelli di bias e varianza. Tipicamente i modelli underfittano, overfittano o sono bilanciati.
Dato un dataset fittizio fisso, i modelli nei grafici mostrano rispettivamente come appaiono underfitting (bias alto, varianza bassa), overfitting (bias basso, varianza alta) e come questi due siano bilanciati.
Il nostro obiettivo da analisti è quello di trovare il giusto bilanciamento tale che dati non visti siano modellati con un errore alquanto basso rispetto alla ground truth (verità del mondo osservabile).
Questo breve articolo si unisce alla collezione seguente di articoli che hanno come argomento centrale la interpretabilità e la generalizzazione:
- Il più grande ostacolo nel machine learning: l’overfitting
- Regolarizzazione L1 vs L2 nel Machine Learning: differenze, vantaggi e come applicarle in Python
- Perché avere un grosso numero di feature può peggiorare il tuo modello
- Cosa è la cross-validazione nel machine learning
A presto!
Andrea
Commenti dalla community"
https://www.diariodiunanalista.it/posts/il-piu-grande-ostacolo-nel-machine-learning-overfitting/,"Il modo migliore per spiegare cosa sia l'overfitting è attraverso un esempio.
Poniamo questo scenario: siamo appena stati assunti come data scientist in una azienda che sviluppa software di elaborazione fotografica.
L'azienda nell'ultimo periodo ha deciso di provare a implementare delle funzionalità di machine learning e l'intenzione è quella di creare un software in grado di distinguere tra foto originali e foto ritoccate.
Il nostro compito è quello di creare un modello che sia specializzato nel rilevare fotoritocchi che hanno come soggetti gli esseri umani.
Siamo entusiasti dell'opportunità, ed essendo alla prima esperienza lavorativa, lavoriamo con molta energia per fare bella figura.
Addestriamo correttamente un modello, che sembra performare molto bene sui dati di training. Siamo molto contenti al riguardo, e comunichiamo i nostri risultati preliminari agli stakeholder.
Il prossimo step è servire il modello in produzione con un gruppo ristretto di utenti. Configuriamo tutto con il team tecnico e poco dopo il modello è online e mostra i suoi risultati con gli utenti.
La mattina seguente apriamo la casella email e leggiamo una serie di messaggi sconcertanti. Gli utenti hanno riportato feedback molto negativo! Il nostro modello non sembra essere in grado di classificare correttamente le immagini.
Com'è possibile che in fase di addestramento il nostro modello performava bene mentre ora in produzione osserviamo risultati così scadenti?
Semplice. Siamo stati vittima di overfitting.
Abbiamo perso il posto. Che botta.
L'intuizione dietro l'overfitting
L'esempio sopra rappresenta una situazione un po' esagerata. Un analista alle prime armi ha almeno una volta sentito parlare del termine overfitting.
È probabilmente una delle prime parole che si imparano quando si lavora nel settore, seguendo e ascoltando tutorial online.
Ciononostante, l'overfitting è un fenomeno che si osserva molto spesso quando si addestra un modello predittivo. Questo porta l'analista a fronteggiare continuamente lo stesso problema che può essere causato da una moltitudine di ragioni.
- A definire cosa è l'overfitting (contrapposto all'underfitting)
- Perché rappresenta l'ostacolo più grande che un analista deve affrontare quando fa machine learning
- Come evitare che questo si presenti attraverso alcuni tecniche
Spiegare in maniera chiara cosa sia voglia dire overfitting non è cosa facile.
Questo perché bisogna partire proprio da cosa significa addestrare un modello e valutare le sue performance (nel link qui in basso una introduzione al machine learning, che include risposte ad alcune domande rilevanti per questo articolo).
Prendendo spunto proprio dall'articolo menzionato,
L'atto di mostrare […] dati al modello e di permettere a quest'ultimo di apprendere si chiama training (addestramento). […] Durante il training, l'algoritmo di apprendimento cerca di imparare i pattern che legano i dati insieme partendo da certe ipotesi.
Ad esempio, gli algoritmi probabilistici fondano il loro funzionamento proprio nel dedurre le probabilità che un evento accada in presenza di certi dati.
Quando il modello viene addestrato, usiamo una metrica di valutazione per stabilire quanto le predizioni di quest'ultimo siano lontane dal valore reale osservato.
Ad esempio per un problema di classificazione (come quello del nostro esempio) potremmo usare lo score F1 per capire come stia performando il modello sui dati di addestramento.
L'errore commesso dall'analista junior nell'esempio introduttivo ha a che vedere con una cattiva interpretazione proprio della metrica di valutazione durante la fase di training e dell'assenza di un framework di validazione dei risultati.
Di fatto, l'analista ha posto attenzione sulle performance del modello durante l'addestramento, dimenticando di guardare e analizzare la performance sui dati di test.
Cosa è l'overfitting?
L'overfitting si manifesta quando il nostro modello impara bene a generalizzare i dati di training ma non quelli di test.
Quando questo capita il nostro algoritmo non riesce a performare bene con dati che non ha mai visto prima. Questo distrugge completamente il suo scopo, e diventa quindi un modello inutile.
Questo è il motivo per cui l'overfitting è il peggior nemico di un analista: rende il nostro modello, e quindi il nostro lavoro, inutile.
Quando un modello viene addestrato, questo sfrutta un training set per apprendere i pattern e mappare il feature set alla variabile target.
Può succedere però, come abbiamo già visto, che un modello possa iniziare a imparare informazioni rumorose o addirittura inutili - ancora peggio, queste informazioni sono presenti solamente nel training set.
Il termine ""overfitting"" si traduce in italiano con ""overadattamento"" al dataset di addestramento - il nostro modello si sofferma e impara informazioni irrilevanti e queste nozioni inficiano negativamente durante l'inferenza.
Esempio in Python
Usiamo il famoso Red Wine Dataset da Kaggle per visualizzare un caso di overfitting. Questo dataset ha 11 dimensioni che definiscono la qualità di un vino rosso. In base a queste dobbiamo costruire un modello in grado di predire la qualità di un vino rosso, che è un valore tra 1 e 10.
Useremo un classificatore basato su alberi decisionali (
Sklearn.tree.DecisionTreeClassifier) per mostrare come un modello possa essere portato a overfittare.
Ecco come appare il dataset se stampiamo le prime 5 righe
Usiamo questo codice per addestrare un albero decisionale.
Train accuracy: 0.623
Test accuracy: 0.591
Abbiamo inizializzato il nostro albero decisionale con l’iperparametro max_depth=3. Proviamo a usare ora un diverso valore - ad esempio 7.
clf = tree.DecisionTreeClassifier(max_depth=7) # il resto del codice rimane uguale
Guardiamo i nuovi valori dell’accuracy
train accuracy: 0.754
Test accuracy: 0.591
La accuracy si sta alzando per il set di addestramento, ma non per quello di test. Inseriamo tutto in un loop dove andremo a modificare
max_depth in maniera dinamica e addestrando un modello ad ogni iterazione.
train_accs = []
test_accs = []
cols = [
'fixed.acidity', 'volatile.acidity', 'citric.acid','residual.sugar', 'chlorides', 'free.sulfur.dioxide',
'total.sulfur.dioxide', 'density', 'pH', 'sulphates', 'alcohol',
]
# inizializziamo un loop dove cambieremo il valore di max depth, partendo da 1 a 25
for depth in range(1, 25):
clf = tree.DecisionTreeClassifier(max_depth=depth)
clf.fit(df_train[cols], df_train.quality)
train_predictions = clf.predict(df_train[cols])
test_predictions = clf.predict(df_test[cols])
train_acc = metrics.accuracy_score(df_train.quality, train_predictions)
test_acc = metrics.accuracy_score(df_test.quality, test_predictions)
# inseriamo in liste vuote le nostre accuracies
train_accs.append(train_acc)
test_accs.append(test_acc)
# visualizziamo i dati
plt.figure(figsize=(10, 5))
sns.set_style('whitegrid')
plt.plot(train_accs, label='train accuracy')
plt.plot(test_accs, label='test accuracy')
plt.legend(loc='upper left', prop={'size': 15})
plt.xticks(range(0, 26, 5))
plt.xlabel('max_depth', size=20)
plt.ylabel('accuracy', size=20)
plt.show()
Guardate come ad una
max_depth alta corrisponda una accuracy molto alta in training (addirittura 100%) ma come questa sia intorno al 55-60% nel test set.
Quello che stiamo osservando è proprio l’overfitting!
Infatti, il valore più alto della accuratezza nel test set lo si nota a
max_depth = 9. Al di sopra di questo valore l’accuratezza non migliora. Non ha senso dunque aumentare il valore del parametro al di sopra del 9.
Questo valore di
max_depth = 9 rappresenta lo “sweet spot” - vale a dire il valore ideale per non avere un modello che overfitta, ma che sia comunque in grado di generalizzare bene i dati. Infatti, un modello potrebbe essere anche molto “superficiale” e manifestare underfitting, l’opposto dell’overfitting.
Ti piace quello che stai leggendo?
Iscriviti al blog per rimanere sempre aggiornato sui contenuti di settore. No pubblicità. No spam.
Solo contenuto utile per la tua carriera nel ML e DS.
Puoi rimuovere l'iscrizione quando vuoi seguendo il link nella email.
Le cause più frequenti di overfitting
Le cause più frequenti che portano un modello a overfittare sono le seguenti:
- I nostri dati contengono rumore e altre informazioni non rilevanti
- I set di training e test sono troppo piccoli
- Il modello è troppo complesso
I dati contengono rumore
Quando i nostri dati di addestramento contengono rumore, il nostro modello impara quei pattern e cerca poi di applicare tale conoscenza con il test set, senza ovviamente successo.
I dati sono pochi e non rappresentativi
Se abbiamo pochi dati, quei potrebbero non essere sufficienti ad essere rappresentativi della realtà che sarà poi fornita dagli utenti che useranno il modello.
Il modello è troppo complesso
Un modello troppo complesso si focalizzerà su informazioni che fondamentalmente sono irrilevanti a mappare la variabile target. Nell’esempio precedente, l’albero decisionale con
max_depth=9 non era ne troppo semplice ne troppo complesso. Aumentare questo valore ha portato ad aumentare la metrica di performance in training, ma non in test.
Come evitare l'overfitting
Ci sono diversi modi per evitare l’overfitting. Vediamo qui i più comuni ed efficaci da usare praticamente sempre
- Cross-validazione
- Aggiungere più dati al nostro dataset
- Rimuovere feature
- Utilizzare un meccanismo di early stopping
- Regolarizzare il modello
Ognuna di queste tecniche permette all’analista di comprendere bene le performance del modello e di raggiungere più velocemente lo “sweet spot” menzionato precedentemente.
Cross-validazione
La cross-validazione è una tecnica molto comune ed estremamente potente che permette di testare le performance del modello su diversi “mini-set” di validazione, invece di usare un singolo set come abbiamo fatto noi in precedenza.
Questo permette di capire come il modello generalizzi su diverse porzioni dell’intero dataset, dando quindi una idea più chiara del comportamento del modello.
Aggiungere più dati al nostro dataset
Il nostro modello può avvicinarsi di più allo sweet spot semplicemente andando ad integrare più informazioni.
Aumentiamo i dati ogni volta che possiamo in modo da offrire al nostro modello delle porzioni di “realtà” sempre più rappresentative. Consiglio al lettore di leggere questo articolo dove spiego come costruire un dataset da zero.
Rimuovere feature
Tecniche di selezione delle feature (come Boruta) possono aiutarci a comprendere quali feature sono inutili alla predizione della variabile target. Rimuovere queste variabili può aiutare a ridurre il rumore di fondo che osserva il modello.
Utilizzare un meccanismo di early stopping
L’early stopping è una tecnica principalmente utilizzata nel deep learning e consiste nel fermare il modello quando non vi è un aumento della performance per una serie di epoche di training. Questo permette di salvare lo stato del modello al suo momento migliore e utilizzare solo questa versione più performante.
Regolarizzare il modello
Attraverso il tuning degli iperparametri possiamo spesso e volentieri controllare il comportamento del modello per ridurre o aumentare la sua complessità. Possiamo modificare questi iperparametri direttamente durante la cross-validazione per comprendere come il modello performi sui diversi split di dati.
Alcuni dei parametri tunabili sono quelli relativi alla regolarizzazione: è possibile rendere il modello più o meno sensibile ai dati rumorosi andando a cambiare il valore di questi ultimi. Nella regressione logistica ad esempio, il parametro
C fa esattamente questo.
Conclusione
Ora il lettore ha una idea più precisa di cosa sia l'overfitting (e la sua controparte, l'underfitting) e che ripercussioni può avere sulle performance del proprio modello.
Abbiamo anche visto quali sono delle tecniche frequentemente usate nella data science e machine learning per evitare l'overfitting.
Infine abbiamo toccato il tema del tuning degli iperparametri, molto importante in una pipeline di machine learning.
Grazie per la vostra attenzione, a presto! 👋
Commenti dalla community"
https://www.diariodiunanalista.it/posts/intro-ai-generativa/,"Il 2023 segna un'era definitiva per l'intelligenza artificiale: l'intero mondo sta toccando con mano quello che anni di data science e machine learning applicato sono riuscite a produrre grazie ai dati.
Le funzionalità che oggi noi internauti siamo in grado di esperire grazie ad aziende come OpenAI, Google, X e Meta (e anche ai contributori dell'open source, beninteso) definiscono quello che noi oggi chiamiamo IA generativa.
Questo termine identifica le tecnologie che permettono alle macchine non solo di apprendere ma anche di creare.
L'IA generativa sta ridefinendo i confini del possibile: basta immaginare il programmatore che risparmia ore e ore di tempo grazie ad assistenti alla scrittura di codice come Copilot o il marketer che crea automazioni senza scrivere una riga di codice grazie agli assistenti GPT.
Questa tecnologia si basa su algoritmi sofisticati che, attraverso tecniche di apprendimento di deep learning, sono in grado di analizzare vasti dataset per poi generarne di nuovi, imitando stili, pattern e strutture.
Come già accennato, il concetto di Generative AI non è nuovo, ma è solo recentemente che abbiamo assistito a un salto qualitativo impressionante nelle sue capacità, grazie a progressi significativi nei modelli di rete neurale e nella disponibilità di enormi quantità di dati.
I primi passi del Generative AI sono stati mossi con progetti come DeepDream di Google e GANs (Generative Adversarial Networks) che hanno mostrato la capacità delle macchine di generare immagini sorprendenti. Tuttavia, è con l'arrivo di modelli come GPT-3 di OpenAI che il Generative AI ha veramente preso il volo. Questi modelli non solo generano testi coerenti e fluidi ma sono anche capaci di creare codice di programmazione, poesie, e persino dialoghi per videogiochi, superando le aspettative più ottimistiche.
Il 2023 si sta rivelando un anno cruciale per il Generative AI, con applicazioni che spaziano in settori diversi e con un impatto sempre più tangibile sul quotidiano. L'arte, la letteratura, la programmazione, il marketing e persino la medicina stanno sperimentando un cambiamento radicale grazie a questa tecnologia, aprendo scenari prima inimmaginabili.
L'entusiasmo attorno al Generative AI è palpabile, ma non mancano le sfide e le domande etiche. Questa introduzione ampliata getta le basi per esplorare non solo le capacità del Generative AI ma anche le sue implicazioni più ampie, un argomento che affronteremo nei dettagli nelle sezioni successive dell'articolo.
- Le capacità della IA generativa
- Come strumenti come GPT hanno rivoluzionato l'automazione digitale
- Come puoi muovere i primi passi nel campo della IA
- Le sfide e i limiti delle tecnologie odierne nello spazio IA
Se sei quindi un nuovo arrivato nel campo della IA, ti do il benvenuto! Questa introduzione ti servirà per capire cosa sta succedendo nello spazio dell'intelligenza artificiale e ti permetterà di muovere i primi passi per l'utilizzo consapevole di queste tecnologie.
Ti piace quello che stai leggendo?
Iscriviti al blog per rimanere sempre aggiornato sui contenuti di settore. No pubblicità. No spam.
Solo contenuto utile per la tua carriera nel ML e DS.
Puoi rimuovere l'iscrizione quando vuoi seguendo il link nella email.
Esempi di Applicazioni della IA Generativa
Partire con degli esempi pratici è sempre utile. In questa sezione dell'articolo ti parlerò di cosa è in grado di fare la IA generativa - capirai velocemente perché sta esplodendo così tanto.
Creazione di Contenuti Digitali
L'AI generativa sta rivoluzionando la creazione di contenuti digitali. Ad esempio, nel campo dell'arte digitale, sono nati nuovi stili e tecniche, con artisti che utilizzano algoritmi generativi per creare opere d'arte che sfidano le convenzioni tradizionali.
DALL-E di OpenAI è un esempio della IA / machine learning applicato proprio al campo artistico. Infatti, questa è una tecnologia in grado di creare immagini da un prompt testuale (un prompt è una richiesta dell'utente, come ad esempio ""crea una immagine di una zebra che va in bici"")
In letteratura, gli scrittori stanno sperimentando IA generativa per elaborare trame e dialoghi, dando vita a storie uniche e coinvolgenti. Nel campo musicale, compositori e produttori stanno utilizzando l'IA per creare nuove melodie e armonie, spingendo i confini della creatività musicale.
Sviluppo di Software e Automazione del Codice
Nello sviluppo software, l'IA generativa sta aprendo nuove frontiere. Piattaforme basate sull'intelligenza artificiale sono ora in grado di generare codice di programmazione, facilitando il lavoro degli sviluppatori e riducendo i tempi di sviluppo.
Questo si estende anche alla manutenzione del codice, dove l'IA può identificare e correggere errori, ottimizzando l'efficienza complessiva del processo di sviluppo.
Applicazioni nel Settore Sanitario
Nel settore sanitario, il Generative AI sta avendo un impatto significativo. Ad esempio, sta aiutando i ricercatori a generare dati sintetici per studi clinici oppure creando protocolli di accettazione e trattamento per i pazienti partendo dalla loro cartella clinica, mantenendo la privacy dei pazienti
Inoltre, è utilizzato per sviluppare modelli predittivi per la diagnosi e il trattamento delle malattie, migliorando l'accuratezza e l'efficacia delle cure mediche.
Utilizzo nel Marketing e nella Pubblicità
Per alcuni dei motivi anche citati precedentemente, queste tecnologie generative stanno trasformando il marketing e la pubblicità.
Aziende di tutti i settori stanno sfruttando questa tecnologia per creare annunci pubblicitari personalizzati, utilizzando testi e immagini generati attraverso prompt linguistici che sono a loro volta generati da un modello linguistico che è a conoscenza dei dati del consumatore finale.
Questo si traduce in campagne pubblicitarie più efficaci, capaci di coinvolgere il pubblico in modo più diretto e significativo.
Progettazione e Architettura
Nel campo della progettazione e dell'architettura, il Generative AI offre nuove possibilità per la creazione di design innovativi.
Gli architetti stanno utilizzando questa tecnologia per generare automaticamente progetti di edifici, combinando estetica e funzionalità in modi prima inimmaginabili, aprendo la strada a nuove forme di espressione architettonica.
Intrattenimento e Media
Infine, nel settore dell'intrattenimento e dei media, il Generative AI sta trasformando il modo in cui vengono prodotti film, serie TV e videogiochi.
Dalla generazione di scenari virtuali alla creazione di dialoghi e trame, l'IA sta aprendo nuove strade nella produzione di contenuti, offrendo esperienze immersive e personalizzate ai consumatori.
Ora vedremo quali sono stati gli impatti dei sistemi generativi come GPT nelle aziende del mondo.
Impatto del'IA Generativa sulle Aziende e il Mercato del Lavoro
L'IA generativa sta portando con sé un'ondata di trasformazioni che impatta profondamente su come le aziende operano e le persone lavorano.
Questa tecnologia offre a imprese di tutti i settori la possibilità di innovare, migliorare l'efficienza operativa e personalizzare l'esperienza del cliente in modi prima inimmaginabili.
Oltre alle applicazioni discusse precedentemente (soprattutto quella legata al settore pubblicitario e media) l'IA generativa impatta parecchio l'efficienza operativa dell'azienda.
L'automatizzazione dei processi ripetitivi e l'analisi di grandi quantità di dati libera risorse umane che possono concentrarsi su compiti più strategici e creativi.
Le implicazioni di questa transizione si estendono ben oltre la semplice riduzione dei costi, introducendo nuove modalità di lavoro che enfatizzano l'innovazione e il valore aggiunto.
Parallelamente, il mercato del lavoro sta subendo mutamenti radicali anche loro parzialmente riconducibili alle innovazioni nel campo IA. Nuove competenze e ruoli professionali emergono, richiedendo una riqualificazione e una formazione continua dei lavoratori. Ad esempio LinkedIn ha di recente pubblicato il suo piano di innovazione, includendo una serie di strumenti basati su IA per aiutare il professionista ad orientarsi tra le migliaia di competenze a disposizione.
Questo scenario richiede un impegno concertato tra aziende, istituzioni educative e professionisti per garantire che le competenze necessarie siano sviluppate e che i lavoratori siano preparati a prosperare in questo nuovo ambiente lavorativo. La gestione di modelli di IA, l'analisi di dati e la supervisione etica dell'IA sono solo alcuni dei campi che stanno guadagnando rilevanza.
Aziende famose che usano l'IA Generativa
Questa lista è sicuramente breve, perché nascono ogni giorno centinaia di aziende che implementano tecnologia IA per vendere i loro servizi.
Se non conosci questi strumenti, ti consiglio altamente di dar loro una controllata!
Ecco alcuni player che dall'origine del movimento si sono fatte un nome usando l'IA generativa:
OpenAI
Impossibile non menzionare il gigante americano. OpenAI ha letteralmente cambiato il mondo da quando ha introdotto per la prima volta GPT-3, sebbene versioni precedenti fossero comunque usabili dal pubblico.
L'avvento della rivoluzione è arrivato con ChatGPT, un chatbot conversazionale che può rispondere a domande di follow-up, ammettere i propri errori, contestare premesse errate e rifiutare richieste inappropriate.
Oggi ChatGPT viene usato da milioni di utenti al giorno ed è basato su GPT-4, un modello migliore di GPT-3. Inoltre, è integrato con DALL-E 3 e altri plugin per fornire funzionalità come la creazione di immagini e creazione di assistenti personali.
NVIDIA
Conosciuta per le sue GPU (schede video, graphic processing unit), NVIDIA è all'avanguardia nel tema IA generativa applicata alla computer vision, in particolare per creare ambienti virtuali realistici e migliorare le grafiche dei videogiochi.
Famoso è il DLSS (deep learning super sampling) (deep learning super sampling) che è una tecnica di deep learning applicata ai videogiochi in grado di ridurre il consumo di risorse computazionali per renderizzare una scena mantenendo praticamente inalterata la qualità visiva.
Canva
Canva è una piattaforma di design grafico che sta utilizzando l'IA generativa per aiutare gli utenti a creare design personalizzati in modo efficiente. Da sempre punto di riferimento per designer e non, Canva afferma che ora i suoi utenti risparmiano quasi il 60% del loro tempo utilizzando gli strumenti di automazione integrati nella piattaforma.
Notion
Notion è uno strumento per l'organizzazione di pensieri, appunti e risorse. È un singolo applicativo in grado di aiutare l'utente ad organizzare il suo lavoro e il suo tempo.
Notion ha introdotto il suo Notion AI, un sistema di IA generativa che permette di compilare appunti, note e database in maniera automatica partendo da prompt umano in linguaggio naturale.
Adobe, Figma e FigJam
Adobe, la celebre azienda dietro i prodotti come Photoshop e Illustrator, ha acquisito Figma, lo strumento per eccellenza per la creazione di interfacce e mock up in fase di sviluppo prodotto.
Adobe utilizza Firefly, la sua IA generativa proprietaria per creare immagini attraverso linguaggio naturale.
Attraverso Adobe, Figma sta ora introducendo l'IA per creare wireframe e widget di interazione, sempre attraverso prompt testuale.
Leggi qui l'articolo 5 strumenti per avanzare nella carriera di data scientist
Muovere i Primi Passi nel Campo della IA
Facciamo subito una dovuta precisazione: usare l'IA è diverso dal comprendere l'IA.
Usarla è semplice: le interfacce di questi strumenti si stanno evolvendo velocemente proprio per abattere le barriere tecniche.
Ma se invece desideri imparare cosa c'è alle spalle dell'IA, e andare magari nel machine learning vero e proprio, allora devi andare oltre e addentrarti nei tecnicismi. Questo blog rappresenta per te solo uno degli step che possono esserti utili per addentrarti e orientarti in questo spazio.
Ti parlo come se fossi un mio studente: l'IA generativa sicuramente suscita interesse, ma non è questo il primo punto dove iniziare.
La formazione di base in data science e machine learning è il punto di partenza essenziale.
Questa conoscenza fondamentale può essere acquisita attraverso corsi online offerti da piattaforme educative riconosciute, che coprono i principi fondamentali dell'IA e del machine learning. Questa fase iniziale è cruciale per comprendere i concetti che stanno alla base delle tecnologie generative più avanzate.
Temi come l'algebra lineare, l'analisi matematica ed il calcolo di probabilità sono fondamentali per lavorare o contribuire nel settore.
Ho una risorse per te che puoi raggiungere ora per seguire un percorso formativo: una rassegna di tutti gli articoli di questo blog, divisi per livello di expertise. Copre le basi di Python e della data science e ti introduce ai concetti di base del machine learning fino a toccare alcuni più avanzati
Poiché non tocco (per ora) tutti i temi rilevanti per la IA generativa, continuerò a darti il mio consiglio su come raggiungere il tuo obiettivo.
Una volta imparata le basi, il passo successivo è approfondire la conoscenza specifica. Risorse come articoli accademici, white paper, e tutorial su argomenti specifici come le reti generative avversarie (GANs) e i modelli di linguaggio avanzati offrono una visione più approfondita di questa specializzazione. È anche utile esplorare le ultime ricerche e sviluppi nel campo per rimanere aggiornati sulle tecnologie emergenti.
Ottenere esperienza pratica è fondamentale. Lavorare su progetti personali o contribuire a progetti open source può essere un ottimo modo per applicare le conoscenze acquisite. Questi progetti non solo rafforzano la comprensione teorica, ma sviluppano anche competenze pratiche cruciali (che sono quelle che cercano i datori di lavoro).
Partecipare attivamente a community e forum online è un altro passo importante. Queste piattaforme sono luoghi ideali per apprendere dalle esperienze di altri professionisti, scambiare idee, e trovare ispirazione. Le community online possono anche essere fonti preziose per rimanere aggiornati sulle ultime tendenze e per scoprire opportunità di networking e collaborazione. Alcuni luoghi dove puoi riunirti online con colleghi sono
- Kaggle
- Hugging Face
- Discord
- Gruppi Facebook / Telegram
Seguire i leader di pensiero nel settore e tenersi informati sulle attività delle aziende che guidano l'innovazione in IA generativa offre ulteriori spunti di apprendimento e ispirazione. Questi esperti e organizzazioni spesso condividono insights preziosi sulle loro ricerche, progetti, e visioni future del campo.
Alcuni esperti che ti consiglio sono
Sfide da Superare nella IA Generativa
Sono tutte rose e fiori? Ovviamente no.
Le sfide attualmente fronteggiate dai ricercatori nel campo sono affascinanti, così come le loro graduali soluzioni.
Una delle principali preoccupazioni è legata ai problemi etici e di bias. I modelli di IA generativa apprendono dai dati disponibili, che possono includere pregiudizi impliciti.
Questo può portare a risultati distorti o ingiusti, specialmente in contesti sensibili come la selezione del personale, il credito bancario o i sistemi giudiziari. L'industria e la comunità accademica stanno lavorando per sviluppare strategie per mitigare questi bias, ma la soluzione a questa sfida è complessa e richiede un approccio multidisciplinare.
Un'altra importante limitazione tecnica riguarda la qualità e la quantità dei dati necessari per l'addestramento dei modelli. I modelli di IA generativa richiedono enormi set di dati per apprendere in modo efficace. Questo non solo presenta sfide logistiche, ma solleva anche preoccupazioni sulla privacy e sulla sicurezza dei dati, specialmente quando si tratta di informazioni sensibili come quelle mediche o personali.
Inoltre, c'è la questione dell'affidabilità e della verificabilità dei contenuti generati dall'IA. Nel contesto di notizie false e disinformazione, i contenuti generati dall'IA possono essere utilizzati in modo improprio per creare materiale ingannevole o manipolativo. Caldo è il tema dei deepfake, video o immagini sintetici che che ritraggono personaggi famosi e non.
Questo solleva la necessità di sviluppare meccanismi di verifica e autenticazione dei contenuti, un'area che è attualmente oggetto di intensa ricerca e sviluppo.
Un'ulteriore sfida è rappresentata dall'accessibilità e dall'equità nell'uso dell'IA in generale. Grandi aziende e istituzioni come OpenAI al momento possiedono la maggior parte dei modelli più performanti, creando quindi un monopolio closed-source.
Questa disparità nell'accesso potrebbe portare a una concentrazione di potere e controllo nelle mani di pochi attori dominanti.
Conclusione
Questo articolo ti ha dato una infarinatura di cosa sia l'IA generativa e del perché sta rivoluzionando il mondo.
Se ti ha intrigato e vuoi saperne di più, ti consiglio di seguire i link inseriti nell'articolo e di seguire sui social i mentori che ho linkato. La loro contribuzione non solo all'IA ma proprio al machine learning è stata enorme e alcuni di loro sono proprio degli educatori. Non c'è motivo per non seguirli se si è interessati.
Se ci sono domande o dubbi, contattami via il form di contatto presente nel blog oppure sui social.
A presto,
Andrea
Commenti dalla community"
https://www.diariodiunanalista.it/posts/introduzione-a-pytorch-dal-training-loop-alla-predizione/,"In questo post vedremo come implementare un modello di regressione logistica usando PyTorch in Python.
PyTorch è uno dei framework di deep learning più famosi e usati dalla community di data scientist e ingegneri del machine learning al mondo, e imparare questo strumento diventa fondamentale se si vuole costruire una carriera nel campo della IA applicata.
Esso si affianca a TensorFlow, un altro framework di deep learning molto famoso sviluppato da Google.
Non ci sono differenze fondamentali notevoli, tranne per la struttura e organizzazione delle loro API, che possono risultare molto diverse.
Mentre entrambi i framework permettono di creare reti neurali molto complesse, PyTorch è generalmente preferito grazie al suo stile più Pythonesco e alla libertà che lascia allo sviluppatore di integrare logiche custom nel software.
Useremo il breast cancer dataset di sklearn, dataset open source e già usato precedentemente in alcuni degli articoli presenti in questo blog per addestrare un modello di classificazione binaria.
L'obiettivo è quello di spiegare come:
- passare da Pandas per l'organizzazione del dataset ai
Datasete
DataLoaderdi PyTorch
- creare una rete neurale per la classificazione binaria in PyTorch
- creare predizioni
- valutare le performance del nostro modello con delle funzioni di utilità e matplotlib
- usare tale rete per fare delle predizioni
Per la fine di questo articolo avremo una idea chiara di come creare una rete neurale in PyTorch e di come il loop di addestramento funzioni.
Installare PyTorch e le altre dipendenze
Iniziamo il nostro progetto andando a creare un ambiente virtuale in una cartella dedicata.
Visita questo link per imparare come creare un ambiente virtuale con Conda.
Una volta creato il nostro ambiente virtuale, scriviamo nel terminale
pip install torch -U. Questo comando installerà l'ultima versione di PyTorch, che alla data di scrittura di questo articolo è la versione 2.0.
Avviando un notebook, possiamo verificare la versione della libreria usando
torch.__version__ dopo aver fatto
import torch.
Possiamo verificare che PyTorch sia installato correttamente nell'ambiente andando a importare e lanciare un piccolo script di test, come mostrato nella guida ufficiale.
import torch
x = torch.rand(5, 3)
print(x)
>>> tensor([[0.3890, 0.6087, 0.2300],
[0.1866, 0.4871, 0.9468],
[0.2254, 0.7217, 0.4173],
[0.1243, 0.1482, 0.6797],
[0.2430, 0.4608, 0.8886]])
Se lo script viene eseguito correttamente allora siamo pronti a procedere con il progetto. In caso contrario suggerisco al lettore di riferirsi alla guida ufficiale che si trova qui https://pytorch.org/get-started/locally/.
Continuiamo con l'installazione delle dipendenze aggiuntive:
- Sklearn;
pip install scikit-learn
- Pandas;
pip install pandas
- Matplotlib;
pip install matplotlib
Librerie come Numpy sono automaticamente installare quando si installa PyTorch.
Importare ed esplorare il dataset
Iniziamo importando le librerie installate e il breast cancer dataset da Sklearn con il seguente snippet di codice
import torch
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
import matplotlib.pyplot as plt
breast_cancer_dataset = load_breast_cancer(as_frame=True, return_X_y=True)
Creiamo un dataframe dedicato a contenere le nostre X e y in questo modo
df = breast_cancer_dataset[0]
df['target'] = breast_cancer_dataset[1]
df
Il nostro obiettivo è quello di creare un modello in grado di prevedere la colonna target in base alle caratteristiche presenti nelle nelle altre colonne.
Andiamo a fare un minimo di analisi esplorativa per avere po' di consapevolezza del dataset. Useremo la libreria sweetviz per creare automaticamente un report di analisi.
Installiamo sweetviz con il comando
pip install sweetviz e creiamo un report di EDA (exploratory data analysis) con questo snippet
import sweetviz as sv
eda_report = sv.analyze(df)
eda_report.show_notebook()
Sweetviz creerà un report direttamente nel nostro notebook da poter esplorare.
Vediamo come diverse colonne siano altamente associate con un valore di 0 o 1 della nostra colonna target.
Essendo un dataset multidimensionale avendo variabili con distribuzioni diverse, una rete neurale è una valida opzione per creare un modello predittivo. Detto ciò, questo dataset può essere modellato anche da modelli più semplici, come gli alberi decisionali.
Importiamo altre due librerie per poter mettere su grafico il dataset. Useremo la PCA (Principal Component Analysis) da Sklearn e Seaborn per visualizzare il dataset multidimensionale.
La PCA ci aiuterà a comprimere il grosso numero di variabili in solamente due, che useremo come asse X e Y in uno scatterplot di Seaborn. Seaborn accetta un ulteriore parametro chiamato hue per colorare i punti in base ad una variabile aggiuntiva. Useremo il nostro target.
import seaborn as sns
from sklearn import decomposition
pca = decomposition.PCA(n_components=2)
X = df.drop(""target"", axis=1).values
y = df['target'].values
vecs = pca.fit_transform(X)
x0 = vecs[:, 0]
x1 = vecs[:, 1]
sns.set_style(""whitegrid"")
sns.scatterplot(x=x0, y=x1, hue=y)
plt.title(""Proiezione PCA"")
plt.xlabel(""PCA 1"")
plt.ylabel(""PCA 2"")
plt.xticks([])
plt.yticks([])
plt.show()
Vediamo come i record che hanno come target 1 si raggruppino in base a delle caratteristiche comuni. Sarà obiettivo della nostra rete neurale differenziare in maniera matematica le righe con target 0 o 1.
Creare dataset e dataloader
PyTorch mette a disposizione gli oggetti
Dataset e
DataLoader per permetterci di organizzare e caricare efficientemente i nostri dati nella rete neurale.
Sarebbe possibile usare direttamente Pandas, ma questo avrebbe degli svantaggi perché rendere il nostro codice meno efficiente.
La classe
Dataset permette di specificare il formato giusto per i nostri dati e applicare le logiche di reperimento e trasformazione che sono spesso fondamentali (si pensi alla data augmentation applicata alle immagini).
Vediamo come creare un oggetto
Dataset di PyTorch.
from torch.utils.data import Dataset
class BreastCancerDataset(Dataset):
def __init__(self, X, y):
# creiamo tensori per le feature
self.features = torch.tensor(X, dtype=torch.float32)
# creiamo tensori per le etichette
self.labels = torch.tensor(y, dtype=torch.long)
def __len__(self):
# definiamo un metodo per recuperare la lunghezza del dataset
return self.features.shape[0]
def __getitem__(self, idx):
# override necessario del metodo __getitem__ che aiuta a indicizzare i nostri dati
x = self.features[idx]
y = self.labels[idx]
return x, y
Questo è una classe che eredita da
Dataset e che permette al Dataloader, che creeremo a breve, di recuperare efficacemente batch di dati.
La classe accetta X e y come input.
Dataset di addestramento, validazione e test
Prima di procedere negli step seguenti, è importante creare dei set di training, validation e test.
Questi ci aiuteranno a valutare le performance del nostro modello e a capire la qualità delle predizioni.
Per il lettore interessato suggerisco di leggere l'articolo 6 linee guida per addestrare correttamente il tuo modello e cosa è la cross-validazione nel machine learning per comprendere meglio perché dividere i nostri dati in tre partizioni sia una metodica efficace per la valutazione delle prestazioni.
Con Sklearn questo diventa semplice con il metodo
train_test_split
from sklearn import model_selection
train_ratio = 0.50
validation_ratio = 0.20
test_ratio = 0.20
x_train, x_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=1 - train_ratio)
x_val, x_test, y_val, y_test = model_selection.train_test_split(x_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio))
print(x_train.shape, x_val.shape, x_test.shape)
>>> (284, 30) (142, 30) (143, 30)
Con questo piccolo snippet di codice abbiamo creato i nostri set di training, validazione e test secondo degli split controllabili.
Normalizzazione dei dati
Quando si fa deep learning, anche per un task semplice come la classificazione binaria, è sempre necessario normalizzare i nostri dati.
Normalizzare significa portare tutti i valori delle varie colonne presenti nel dataset sulla stessa scala numerica. Questo aiuta la rete neurale a convergere più efficacemente e quindi fare predizioni accurate più velocemente.
Useremo lo
StandardScaler di Sklearn.
from sklearn import preprocessing
scaler = preprocessing.StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_val_scaled = scaler.transform(x_val)
x_test_scaled = scaler.transform(x_test)
Notiamo come
fit_transform sia applicato solo al training set, mentre
transform sia applicato agli altri due dataset. Questo serve ad evitare il fenomeno del data leakage - quando informazioni del nostro di validazione o di test sono trasferiti involontariamente nel nostro set di training. Vogliamo che il nostro set di addestramento sia l'unica fonte di apprendimento, senza influenze da dati di test.
Questi dati ora sono pronti per essere forniti in input alla classe
BreastCancerDataset.
train_dataset = BreastCancerDataset(x_train_scaled, y_train)
val_dataset = BreastCancerDataset(x_val_scaled, y_val)
test_dataset = BreastCancerDataset(x_test_scaled, y_test)
Importiamo il dataloader e inizializziamo gli oggetti.
from torch.utils.data import DataLoader
train_loader = DataLoader(
dataset=train_dataset,
batch_size=16,
shuffle=True,
drop_last=True
)
val_loader = DataLoader(
dataset=val_dataset,
batch_size=16,
shuffle=False,
drop_last=True
)
test_loader = DataLoader(
dataset=test_dataset,
batch_size=16,
shuffle=False,
drop_last=True
)
La potenza del
DataLoader è che permette di specificare se fare lo shuffling dei nostri dati e in che numero di batch i dati debbano essere forniti al modello. La batch size è da considerarsi un iperparametro del modello e quindi può impattare i risultati delle nostre inferenze.
Implementazione della rete neurale in PyTorch
Creare un modello in PyTorch potrebbe sembrare complesso, ma in realtà richiede la comprensione solo di alcuni concetti di base.
- Quando scriviamo un modello in PyTorch, useremo un approccio basato su oggetti, come per i dataset. Significa che creeremo una classe come
class MyModelche eredita dalla classe
nn.Moduledi PyTorch
- PyTorch è un software di differenziazione automatica. Significa che quando andiamo a scrivere una rete neurale basata sull'algoritmo di backpropagation, il calcolo delle derivate per calcolare la loss è fatto dietro alle quinte. Questo richiede la scrittura di un po' di codice dedicato che potrebbe trarre in confusione la prima volta.
Consiglio al lettore vuole conoscere le basi di come funzionano le reti neurali di consultare l'articolo Introduzione alle reti neurali - pesi, bias e attivazione
Detto ciò, vediamo come appare il codice per scrivere un modello di regressione logistica.
class LogisticRegression(nn.Module):
""""""
La nostra rete neurale accetta num_features e num_classes.
num_features: numero di caratteristiche da cui apprendere
num_classes: numero di classi in output da prevedere (in questo caso, 1 o 2, poiché l'output è binario (0 o 1))
""""""
def __init__(self, num_features, num_classes):
super().__init__() # inizializziamo il metodo init di nn.Module
self.num_features = num_features
self.num_classes = num_classes
# creiamo un singolo strato di neuroni su cui applicare la regr. logistica
self.linear1 = nn.Linear(in_features=num_features, out_features=num_classes)
def forward(self, x):
logits = self.linear1(x) # facciamo passare in nostri dati nello strato
probs = torch.sigmoid(logits) # applichiamo una funzione sigmoide per ottenere le probabilità di appartenenza ad una classe (0 o 1)
return probs # restituiamo le probabilità
La nostra classe eredita da
nn.Module. Questa classe fornisce i metodi che dietro alle quinte permettono al modello di funzionare.
Metodo __init__
Il metodo
__init__ di una classe contiene la logica che viene eseguita all'instanziamento di una classe in Python. Qui passiamo due argomenti: il numero di feature e il numero di classi da predire.
num_features corrisponde al numero di colonne che compongono il nostro dataset meno la nostra variabile target, mentre
num_classes corrisponde al numero di risultati che la rete neurale deve restituire.
Oltre ai due argomenti e alle loro variabili di classe, vediamo la riga
super().__init__(). La funzione super inizializza il metodo init della classe madre. Questo permette di avere le funzionalità di
nn.Module all'interno del nostro modello.
Sempre nel blocco init, vediamo l'implementazione di uno strato lineare
self.linear1, che accetta come argomenti il numero di feature e il numero di risultati da restituire.
Metodo forward()
Facendo
def forward(self, x) diciamo a Python di fare un override del metodo forward all'interno della classe madre
nn.Moduledi PyTorch. Infatti, questo metodo viene chiamato all'esecuzione di un forward pass - cioé quando i nostri dati passano da uno strato all'altro.
forward accetta l'input \( x \), che contiene le feature su cui il modello tarerà la sua performance.
L'input passa attraverso il primo strato, creando la variabile
logits. I logit sono i calcoli della rete neurale che non sono ancora convertiti in probabilità dalla funzione di attivazione finale, che in questo caso è una sigmoide. Di fatto, sono la rappresentazione interna della rete neurale prima di essere mappata ad una funzione che ne permette l'interpretazione.
In questo caso la funzione sigmoide andrà a mappare i logit a delle probabilità comprese tra 0 e 1. Se l'output è minore di 0, allora la classe sarà 0 altrimenti sarà 1. Questo avviene nella riga
self.probs = torch.sigmoid(x).
Funzioni di utility per plotting e calcolo accuracy
Creiamo delle funzioni di utilità da utilizzare nel training loop che vedremo a breve. Queste due servono per calcolare la accuratezza alla fine di ogni epoca e a visualizzare le curve di performance alla fine del training.
def compute_accuracy(model, dataloader):
""""""
Questa funzione mette il modello in modalità di valutazione (model.eval()) e calcola la accuratezza rispetto al dataloader in input
""""""
model = model.eval()
correct = 0
total_examples = 0
for idx, (features, labels) in enumerate(dataloader):
with torch.no_grad():
logits = model(features)
predictions = torch.where(logits > 0.5, 1, 0)
lab = labels.view(predictions.shape)
comparison = lab == predictions
correct += torch.sum(comparison)
total_examples += len(comparison)
return correct / total_examples
def plot_results(train_loss, val_loss, train_acc, val_acc):
""""""
Questa funzione riceve delle liste di valori e crea dei grafici fianco a fianco per mostrare le performance di training e validation
""""""
fig, ax = plt.subplots(1, 2, figsize=(15, 5))
ax[0].plot(
train_loss, label=""train"", color=""red"", linestyle=""--"", linewidth=2, alpha=0.5
)
ax[0].plot(
val_loss, label=""val"", color=""blue"", linestyle=""--"", linewidth=2, alpha=0.5
)
ax[0].set_xlabel(""Epoch"")
ax[0].set_ylabel(""Loss"")
ax[0].legend()
ax[1].plot(
train_acc, label=""train"", color=""red"", linestyle=""--"", linewidth=2, alpha=0.5
)
ax[1].plot(
val_acc, label=""val"", color=""blue"", linestyle=""--"", linewidth=2, alpha=0.5
)
ax[1].set_xlabel(""Epoch"")
ax[1].set_ylabel(""Accuracy"")
ax[1].legend()
plt.show()
Addestramento del modello
Ora veniamo alla parte dove la maggior parte dei neofiti del deep learning hanno difficoltà: il loop di training di PyTorch.
Guardiamo il codice e poi commentiamolo
import torch.nn.functional as F
model = LogisticRegression(num_features=x_train_scaled.shape[1], num_classes=1)
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
num_epochs = 10
train_losses, val_losses = [], []
train_accs, val_accs = [], []
for epoch in range(num_epochs):
model = model.train()
t_loss_list, v_loss_list = [], []
for batch_idx, (features, labels) in enumerate(train_loader):
train_probs = model(features)
train_loss = F.binary_cross_entropy(train_probs, labels.view(train_probs.shape))
optimizer.zero_grad()
train_loss.backward()
optimizer.step()
if batch_idx % 10 == 0:
print(
f""Epoch {epoch+1:02d}/{num_epochs:02d}""
f"" | Batch {batch_idx:02d}/{len(train_loader):02d}""
f"" | Train Loss {train_loss:.3f}""
)
t_loss_list.append(train_loss.item())
model = model.eval()
for batch_idx, (features, labels) in enumerate(val_loader):
with torch.no_grad():
val_probs = model(features)
val_loss = F.binary_cross_entropy(val_probs, labels.view(val_probs.shape))
v_loss_list.append(val_loss.item())
train_losses.append(np.mean(t_loss_list))
val_losses.append(np.mean(v_loss_list))
train_acc = compute_accuracy(model, train_loader)
val_acc = compute_accuracy(model, val_loader)
train_accs.append(train_acc)
val_accs.append(val_acc)
print(
f""Train accuracy: {train_acc:.2f}""
f"" | Val accuracy: {val_acc:.2f}""
)
A differenza di TensorFlow, PyTorch richiede la scrittura di un loop di training in puro Python.
Vediamo il procedimento per punti:
- Instanziamo il modello e l'ottimizzatore
- Decidiamo un numero di epoche
- Creiamo un for loop che itera tra le epoche
- Per ogni epoca, settiamo il modello in modalità training con
model.train()e cicliamo nel
train_loader
- Per ogni batch del
train_loader, calcolare la loss, portare a 0 il calcolo delle derivate con
optimizer.zero_grad()e aggiornare i pesi della rete con
optimizer.step()
A questo punto il training loop è completo, e se si vuole si può integrare la stessa logica sul dataloader di validazione come scritto nel codice.
Ecco il risultato del training dopo il lancio di questo codice
Valutazione delle performance della rete neurale
Usiamo la utility function creata precedentemente per mettere su grafico loss in training e validazione.
plot_results(train_losses, val_losses, train_accs, val_accs)
Il nostro modello di classificazione binaria riesce velocemente a convergere ad una accuracy alta, e vediamo come la loss si abbassi alla fine di ogni epoca.
Il dataset risulta essere semplice da modellare e il numero basso di esempi non aiuta a vedere una convergenza più graduale verso performance alte da parte della rete.
Sottolineo che è possibile integrare il software TensorBoard in PyTorch per poter loggare le metriche di performance automaticamente tra i vari esperimenti.
Creare le predizioni
Siamo arrivati alla parte finale di questa guida. Vediamo il codice per creare le predizioni per il nostro intero dataset.
# trasformiamo tutte le nostre feature con lo scaler
X_scaled_all = scaler.transform(X)
# trasformiamo in tensori
X_scaled_all_tensors = torch.tensor(X_scaled_all, dtype=torch.float32)
# impostiamo il modello in modalità inferenza e creiamo le predizioni
with torch.inference_mode():
logits = model(X_scaled_all_tensors)
predictions = torch.where(logits > 0.5, 1, 0)
df['predictions'] = predictions.numpy().flatten()
Ora importiamo il pacchetto
metrics da Sklearn che permette di calcolare velocemente la matrice di confusione e il report di classificazione direttamente sul nostro dataframe pandas.
from sklearn import metrics
from pprint import pprint
pprint(metrics.classification_report(y_pred=df.predictions, y_true=df.target))
E la matrice di confusione, che mostra sulla diagonale il numero di risposte corrette
metrics.confusion_matrix(y_pred=df.predictions, y_true=df.target)
>>> array([[197, 15],
[ 13, 344]])
Ecco una piccola funzione per creare un linea di demarcazione che separa le classi nel grafico PCA
def plot_boundary(model):
w1 = model.linear1.weight[0][0].detach()
w2 = model.linear1.weight[0][1].detach()
b = model.linear1.bias[0].detach()
x1_min = -1000
x2_min = (-(w1 * x1_min) - b) / w2
x1_max = 1000
x2_max = (-(w1 * x1_max) - b) / w2
return x1_min, x1_max, x2_min, x2_max
sns.scatterplot(x=x0, y=x1, hue=y)
plt.title(""Proiezione PCA"")
plt.xlabel(""PCA 1"")
plt.ylabel(""PCA 2"")
plt.xticks([])
plt.yticks([])
plt.plot([x1_min, x1_max], [x2_min, x2_max], color=""k"", label=""Classificazione"", linestyle=""--"")
plt.legend()
plt.show()
Ed ecco come il modello separa le cellule benigne da quelle maligne
Conclusioni
In questo articolo abbiamo visto come creare un modello di classificazione binaria con PyTorch, partendo da un dataframe Pandas.
Abbiamo visto come appare il loop di training, come valutare il modello e come creare delle predizioni e delle visualizzazioni per aiutare l'interpretazione.
Con PyTorch è possibile creare reti neurali molto complesse...basti pensare che Tesla, l'azienda costruttrice di auto elettriche basate su AI, usa proprio PyTorch per creare i suoi modelli.
Per chi vuole iniziare il suo percorso nel deep learning, imparare PyTorch quanto prima diventa un task a priorità alta poiché consente di creare tecnologie importanti in grado di risolvere problemi complessi basati sui dati.
Commenti dalla community"
https://www.diariodiunanalista.it/posts/introduzione-a-tensorflow-2-0-api-e-modello-sequenziale-di-deep-learning/,"Tensorflow è il framework creato da Google che permette ai praticanti di machine learning di creare modelli di deep learning ed è spesso la prima soluzione che viene proposta agli analisti che si approcciano per la prima volta al deep learning.
Il motivo è da ricercarsi nella semplicità e intuitività dell'API sequenziale di Tensorflow. Questa è molto semplice ma permette all'analista di creare reti neurali molto complesse e potenti.
API significa Application Programming Interface (interfaccia di programmazione dell'applicazione) e rappresenta proprio l'interfaccia tra l'utente e l'applicativo che vogliamo usare.
L'interfaccia di Tensorflow è semplice, diretta e facilmente comprensibile anche da chi non ha mai fatto deep learning pratico, ma che conosce solo la teoria.
L'API sequenziale di Tensorflow è molto beginner-friendly e quindi si presta molto bene ad essere insegnata come introduzione al deep learning.
In questo articolo propongo una introduzione al deep learning proprio che sfrutta l'API sequenziale, mostrando esempi e codice in modo da aiutare il lettore nella comprensione di questi concetti.
Intuizione alla base del deep learning e delle reti neurali
Prima di vedere come funziona Tensorflow e il suo modello sequenziale, è necessario avere un minimo di background su cosa sia il deep learning e come funzionano le reti neurali.
Le reti neurali sono lo strumento principale del deep learning. L'apprendimento di un fenomeno avviene attraverso l'attivazione e disattivazione dei neuroni presenti all'interno di una rete, che riescono a rappresentare il problema che vogliamo risolvere attraverso pesi e bias.
Tra ogni strato neuronale c'è una funzione di attivazione. Questa va a trasformare l'output di uno strato verso il prossimo, con diverse conseguenze sulla abilità della rete a generalizzare il problema. La più comune delle funzioni di attivazione è chiamata ReLU.
Ogni neurone è caratterizzato da pesi e bias. Attraverso gli algoritmi di gradient descent e backpropagation una rete neurale è in grado di modificare e aggiornare la combinazione di pesi e bias in maniera iterativa in modo da avvicinarsi sempre di più alla funzione reale che descrive il fenomeno nella realtà.
Una rete neurale è in grado di comprendere se sta migliorando o meno le sue performance andando a confrontare quanto le sue predizioni siano vicine ai valori reali. Questo comportamento è descritto da una loss function (funzione di perdita). Come praticanti di machine learning, vogliamo sempre andare a minimizzare la loss.
Il nostro obiettivo come analisti è quello di ridurre quanto più possibile la loss, evitando al contempo l'overfitting.
Non andrò nel dettaglio in questo pezzo anche perché ho parlato di questo argomento nell'articolo Introduzione alle reti neurali - pesi, bias e attivazione. Consiglio al lettore che vuole espandere la sua intuizione delle basi del deep learning di leggere questo articolo.
Questa piccola intuizione però sarà sufficiente per aiutare nella comprensione della sezione che segue.
Cosa è e come funziona l'API sequenziale di Tensorflow?
Tensorflow mette a disposizione una miriade di funzionalità grazie a Keras. Le API di TensorFlow si basano su quelle di Keras per la definizione e l'addestramento delle reti neurali.
Il modello sequenziale ci permette di specificare una rete neurale, per l'appunto, sequenziale: da input ad output, passando per una serie di strati neurali, uno dopo l'altro.
Tensorflow permette di usare anche l'API funzionale per la creazione di modelli di deep learning. È un approccio usato da utenti esperti e non è user-friendly quanto l'API sequenziale. La differenza fondamentale sta nel fatto che l'API funzionale permette di creare reti neurali non sequenziali (quindi multi-output e con integrazioni ad altri componenti).
In Python, è implementabile così
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
model = keras.Sequential(
[...]
)
keras.Sequential accetta una lista che contiene l'architettura della rete neurale. Ogni strato viene inserito nella lista, e il dato fluisce in maniera sequenziale attraverso ognuno di essi finché non diventa output.
Possiamo specificare degli strati neurali attraverso
keras.layers.
Lo strato di input
Il primo strato da inserire in una rete neurale è quello di input. Attraverso
keras è molto semplice.
Creiamo uno strato di 256 neuroni, con attivazione relu e una dimensione di 4.
# strato input
layers.Dense(256, input_dim=4, activation=""relu"", name=""input"")
Questo strato sarà diverso dagli altri, poiché gli altri non necessitano di specificare l'argomento
input_dim.
Uno dei problemi più comuni nei principianti del deep learning è quello di comprendere quale sia la shape dell'input.
Trovare il valore di
input_dim non è banale, e dipende molto dai dati che abbiamo. Poiché nel deep learning ragioniamo in tensori (strutture che contengono dati multidimensionali), a volte diventa difficile intuire quale sia la forma dei nostri dati da fornire alla rete neurale.
In un dataset tabellare, la nostra input shape sarà uguale al numero di colonne del dataset. Con Pandas e Numpy basta usare
.shape[-1] sull'oggetto per ottenere questa informazione.
Nel caso di immagini invece, saremmo costretti a passare il numero di pixel totale dell'immagine. Nel caso di una immagine 28 * 28, l'input dimension sarà 784.
Discorso ancora diverso per le serie temporali, dove bisogna passare la dimensione del dataset, la lunghezza della finestra temporale e la dimensione del dataset contenuta in essa.
Ponendo che il nostro dataset sia tabellare e che abbia 4 colonne, ci basterà specificare 4 come input dimension.
Per quanto riguarda il numero di neuroni, il valore di 256 è arbitrario. Bisogna sperimentare con questi parametri e valutare qual è l'architettura più performante.
Gli strati successivi all'input
Andiamo ad aggiungere degli strati al nostro modello sequenziale.
model = keras.Sequential(
[
layers.Dense(256, input_dim=4, activation=""relu"", name=""input"")
layers.Dense(128, activation=""relu"", name=""strato1""),
layers.Dense(64, activation=""relu"", name=""strato2""),
# ...
]
)
Gli strati dopo il primo non richiedono di specificare la input dimension, poiché dopo lo strato di input sarà la rappresentazione dell'input in termini di pesi e bias a passare da uno strato all'altro.
Lo strato di output
Lo strato di output si differenzia dagli altri strati in quanto deve rispecchiare il numero di valori che vogliamo ricevere in output dalla rete neurale.
Se per esempio vogliamo fare una regressione, e quindi predire un singolo numero, il numero di unità nello strato finali dovrà essere uno.
# strato output
layers.Dense(1, name=""output"")
In questo caso, non va nemmeno specificata la funzione di attivazione, poiché senza di essa si ha una rappresentazione lineare (non influenzata dalla ReLU ad esempio) della rete neurale.
Se invece volessimo predire delle categorie, ad esempio differenziare tra cani e gatti in una immagine, avremmo bisogno di una funzione di attivazione nell'ultimo strato che si chiama softmax. Softmax mappa la rappresentazione della rete neurale alle classi presenti nel nostro dataset, assegnando una probabilità alla predizione di ciascuna classe.
Per l'esempio menzionato, il nostro strato di output sarebbe
# strato output
layers.Dense(2, activation=""softmax"", name=""output"")
Il risultato sarebbe simile a [[ 0.98, 0.02]] , dove il primo numero indica il livello di confidenza della rete neurale a predire la classe 1, che potrebbe essere cane o gatto.
Stampare un sommario del modello
Mettiamo insieme i pezzi di codice visti finora., aggiungiamo un nome al modello e stampiamo un riassunto della nostra architettura con
.summary().
model = keras.Sequential(
layers=[
layers.Dense(256, input_dim=4, activation=""relu"", name=""input""),
layers.Dense(128, activation=""relu"", name=""strato1""),
layers.Dense(64, activation=""relu"", name=""strato2""),
layers.Dense(2, activation=""softmax"", name=""output"")
],
name=""modello_sequenziale1""
)
model.summary()
Osserviamo il risultato
Questo sommario mostra informazioni molto importanti per comprendere l'architettura della nostra rete neurale e come il dato si muove tra gli strati.
La colonna più importante è Output Shape. Nel caso di un esempio così semplice può non sembrare rilevante, ma questa colonna mostra come il nostro dato cambia forma nei vari strati della rete neurale.
Il sommario diventa specialmente utile quando usiamo le reti neurali convoluzionali oppure LSTM. Questo perché la forma del dato cambia in modi non facilmente intuibili. In caso di errori, queste informazioni ci possono aiutare a debuggare il codice.
La colonna Param # indica il numero di parametri che possono essere modificati dalla rete neurale. In termini matematici, è il numero di dimensioni del nostro problema di ottimizzazione. Ricordiamo che ogni neurone ha un parametro di peso e uno di bias e quindi
n_parametri = n_neuroni * ( n_input + 1).
Nel primo strato output e input sono uguali, quindi sarebbe 256 x 5.
Aggiungere strati in maniera incrementale
C'è anche un metodo alternativo, puramente basato sullo stile e quindi arbitrario, di aggiungere strati ad un modello sequenziale.
In maniera incrementale, è possibile usare
model.add() per aggiungere un oggetto.
model = keras.Sequential()
model.add(layers.Dense(256, input_dim=4, activation=""relu"", name=""input""))
model.add(layers.Dense(128, activation=""relu"", name=""strato1""))
model.add(layers.Dense(64, activation=""relu"", name=""strato2""))
model.add(layers.Dense(2, activation=""softmax"", name=""output""))
Il risultato finale è uguale a quello visto precedentemente attraverso la lista
layers, quindi si può usare l'approccio che si preferisce.
Compilare il modello sequenziale completo
Andiamo ora a compilare il modello - un processo necessario per l'addestramento della rete neurale.
Compilare significa andare a settare un una funzione di perdita, un ottimizzatore e le metriche di valutazione delle performance.
Una volta stabilita la architettura della rete, compilare richiede solo un piccolo pezzo di codice. Portando avanti l'esempio della classificazione tra cani e gatti, useremo come loss function la cross-entropia categoriale, Adam come ottimizzatore e la accuratezza come metrica di valutazione.
Per leggere di più su questi parametri, vi invito a leggere l'articolo sulla classificazione binaria di immagini fatta in Tensorflow.
model.compile(
loss=""categorical_crossentropy"",
optimizer=""adam"",
metrics=[""accuracy""]
)
Addestrare il modello sequenziale
Per addestrare il modello sequenziale basta usare
model.fit() dopo averlo compilato. Basta passare X e y, dove X è il nostro feature set e y è la nostra variabile target.
Ci sono anche altri parametri passabili in
.fit(). Ecco alcuni dei più importanti:
batch_size: permette di stabilire il numero di esempi da valutare ad ogni iterazione di addestramento prima di aggiornare i pesi e i bias del modello
epochs: stabilisce il numero di volte che il modello processa tutto il dataset. Una epoca è superata quando tutti gli esempi nel dataset sono stati sfruttati per aggiornare i pesi del modello
validation_data: qui passiamo il set di dati di test su cui fare la valutazione dell'addestramento.
model = keras.Sequential(
layers=[
layers.Dense(256, input_dim=4, activation=""relu"", name=""input""),
layers.Dense(128, activation=""relu"", name=""strato1""),
layers.Dense(64, activation=""relu"", name=""strato2""),
layers.Dense(2, activation=""softmax"", name=""output"")
],
name=""modello_sequenziale1""
)
model.compile(
loss=""categorical_crossentropy"",
optimizer=""adam"",
metrics=[""accuracy""])
history = model.fit(X_train, y_train, batch_size=32, epochs=200, validation_data=(X_test, y_test),)
da qui parte il processo di addestramento che mostrerà in terminale il progresso con loss e metrica di performance.
Ho scritto un articolo sull'Early Stopping con Tensorflow, una callback in grado di aiutare una rete neurale a migliorare le sue performance in training.
Valutazione del modello sequenziale
Il lettore attento avrà notato una particolarità nello snippet di codice appena visto. Mi riferisco a
history = model.fit(...). Come mai bisogna assegnare una operazione di addestramento ad una variabile? Perché
model.fit(...) restituisce un oggetto che contiene le performance di training.
In Tensorflow, usare
.fit() restituisce un oggetto con le performance di training del modello. Questo oggetto è usabile per visualizzare queste performance e per analizzarle in dettaglio.
Possiamo accedere ai valori nel dizionario esplorando l'attributo history all'interno della variabile.
Usando questi dati possiamo visualizzare le performance di addestramento sul set di training e validazione.
def plot_model(metric):
plt.plot(history.history[metric])
plt.plot(history.history[f""val_{metric}""])
plt.title(f""model {metric}"")
plt.ylabel(f""{metric}"")
plt.xlabel(""epoch"")
plt.legend([""train"", ""validation""], loc=""upper left"")
plt.show()
plot_model(""loss"");
plot_model(""accuracy"");
Guardiamo la loss
E stesso discorso per la metrica di valutazione scelta, in questo caso la accuratezza
Nel caso in cui avessimo un set di validazione e test, è possibile valutare il modello usando
model.evaluate(X_test, y_test).
Fare predizioni con il modello sequenziale
Una volta addestrato, è tempo di usare il modello per fare predizioni. Questa operazione si fa con
train_predictions = model.predict(X_train)
In questo caso, l'API è simile a quella di Sklearn e le predizioni della rete neurali sono assegnate a
train_predictions.
Salvare e caricare un modello Tensorflow
L'ultimo step è tipicamente quello di salvare il modello che abbiamo addestrato. L'API di Tensorflow permette di farlo semplicemente con
model.save(""posizione\su\disco"")
Verrà creata una cartella alla posizione su disco specificata che conterrà la nostra rete neurale.
Per caricare il modello in un momento successivo, basta fare
model = keras.models.load_model(""posizione\su\disco"")
Da qui è possibile usare il modello per fare predizioni come abbiamo visto poco fa.
Quando NON usare un modello sequenziale?
Come già menzionato, Tensorflow permette di creare reti neurali non sequenziali attraverso l'utilizzo dell'API funzionale.
In particolare, vale la pena considerare l'approccio funzionale quando:
- abbiamo bisogno di più output, quindi una rete neurale multi-output
- uno strato richiede più input dagli strati precedenti
- bisogna far comunicare due reti neurali tra di loro
- abbiamo bisogno di una rete neurale custom, con una architettura non usuale
In tutti questi, e forse altri, richiediamo una rete non sequenziale. L'approccio sequenziale è generalmente molto flessibile in quanto permette di risolvere parecchi problemi, come la classificazione binaria di immagini, ma per problemi più complessi una rete del genere potrebbe non fare a caso nostro.
Conclusioni
L'API e modello sequenziale di Tensorflow sono degli strumenti potenti e semplici da usare per il praticante di deep learning.
Questa guida vuole mettere il principiante di deep learning in condizione di poter sperimentare con tali strumenti nei suoi progetti personali, evitando di sentirsi perso e ricorrere alla documentazione ufficiale.
Spero di aver contribuito alla vostra formazione. Alla prossima!
Commenti dalla community"
https://www.diariodiunanalista.it/posts/introduzione-al-campionamento-statistico-e-al-ricampionamento/,"Il desiderio di qualsiasi ricercatore è quello di poter avere dati sull’intera popolazione oggetto di studio.
Studiare la popolazione intera permette al ricercatore di ottenere una comprensione completa del fenomeno in esame, poiché ciò consente di raccogliere informazioni su tutti gli individui che compongono la popolazione.
Tuttavia, nella maggior parte dei casi, questo obiettivo è irrealizzabile, sia per motivi pratici che teorici.
Ad esempio, in molti casi è impossibile individuare e contattare tutti gli individui appartenenti a una popolazione, o la raccolta di dati sull'intera popolazione sarebbe troppo costosa o richiederebbe troppo tempo.
Per questo motivo, è spesso necessario utilizzare un campione come approssimazione della popolazione.
In questo articolo parleremo di cosa sia il campionamento statistico, perché è importante per qualsiasi analista (che faccia analisi dati o machine learning) e alcune tecniche più utilizzate proprio per stimare la popolazione date delle informazioni di partenza.
Cosa è un campione
Un campione è un sottoinsieme della popolazione che si intende studiare. A differenza della popolazione, che rappresenta l'intero gruppo di individui o oggetti che si desidera analizzare, il campione rappresenta solo una porzione di esso.
L'utilizzo di un campione è importante perché può essere più pratico e conveniente raccogliere dati su un gruppo più piccolo e rappresentativo di individui, piuttosto che sulla popolazione intera.
Perché è difficile studiare una popolazione?
Le motivazioni possono essere innumerevoli, ma alcune delle più comuni sono
- la popolazione è troppo grande per poter essere presa tutta in esame
- mancanza di risorse, come tempo e denaro, per raccogliere dati su tutta la popolazione
- difficoltà a individuare tutti gli individui appartenenti alla popolazione
- impossibilità di raccogliere dati su alcuni individui, ad esempio a causa di malattie o inaccessibilità di altro tipo
E molto altri aspetti che dipendono dal progetto del ricercatore.
In questi casi, il campionamento statistico diventa una soluzione pratica ed efficiente per stimare le caratteristiche della popolazione. Una volta raccolti i dati dal campione, è possibile utilizzarli per fare inferenze sulla popolazione più ampia, utilizzando tecniche di stima e di ricampionamento.
Cosa significa campione “rappresentativo”?
Un campione rappresentativo è un sottoinsieme della popolazione che è stato scelto in modo tale da avere le stesse caratteristiche della popolazione nella sua interezza.
Un campione rappresentativo è quello che riflette accuratamente la distribuzione delle caratteristiche della popolazione di riferimento.
Durante la fase di raccolta dati, che sia attraverso il web scraping oppure la somministrazione di un questionario, è importante che il ricercatore sia sensibile e attento agli individui che include all’interno dell’esperimento.
La maggior parte delle volte il ricercatore non avrà alcuna idea di come sia distribuita la popolazione, quindi dovrà utilizzare delle tecniche di campionamento, come ad esempio quello casuale, che cerca di garantire che ogni individuo della popolazione abbia la stessa probabilità di essere incluso nel campione. In questo modo, il campione sarà rappresentativo della popolazione e le inferenze fatte sui dati del campione saranno valide per la popolazione più ampia.
Ma non è così semplice, poiché l'unico modo empirico che il ricercatore ha per validare le sue tecniche di campionamento è attraverso l'osservazione e la sperimentazione.
Fattori che impattano il campionamento
Ecco una lista di fattori che impattano la qualità del campionamento e la sua proprietà di approssimare correttamente la popolazione di riferimento:
- La popolazione di riferimento: la scelta del campione dipende dalla conoscenza della popolazione di riferimento, cioè il gruppo di persone, oggetti, eventi o dati da cui si intende estrarre il campione.
- Il metodo di campionamento: esistono diversi metodi di campionamento, tra cui il campionamento casuale semplice, il campionamento stratificato, il campionamento a grappoli, il campionamento sistematico e il campionamento basato su quote. La scelta del metodo dipende dalle caratteristiche della popolazione e dagli obiettivi dello studio.
- La dimensione del campione: la dimensione del campione dipende dal livello di precisione e dall'affidabilità richiesti per le stime. In generale, maggiore è la dimensione del campione, maggiore è la precisione delle stime. Questo perché si approssima sempre di più la popolazione di riferimento a mano a mano che il numero di individui del campione aumenta.
- Criteri di inclusione: i criteri di inclusione utilizzati possono influenzare la rappresentatività del campione e l'accuratezza delle stime. È importante utilizzare tecniche di selezione appropriate e di evitare bias di selezione.
- La modalità di raccolta dei dati: la modalità di raccolta dei dati (ad esempio, interviste telefoniche, questionari online, osservazioni sul campo) può influire sulla qualità dei dati e sulla rappresentatività del campione.
- La durata del periodo di raccolta dei dati: la durata del periodo di raccolta dei dati può influenzare la rappresentatività del campione, in quanto le caratteristiche della popolazione possono variare nel tempo.
Tecniche di campionamento
Vedremo ora una lista di tecniche di campionamento a disposizione del ricercatore per approssimare la popolazione.
- casuale
- stratificato
- sistematico
- per quote
- bootstrapping
Campionamento casuale
Il campionamento casuale semplice è una delle tecniche di campionamento più utilizzate e prevede la selezione casuale degli individui della popolazione, in modo tale che ogni individuo abbia la stessa probabilità di essere incluso nel campione.
Questa tecnica di campionamento è utile quando la popolazione è omogenea e non ci sono ragioni per suddividerla in gruppi. Inoltre, il campionamento casuale semplice è relativamente facile da implementare e non richiede conoscenze specialistiche.
Il campionamento casuale semplice ha tuttavia alcuni limiti, come la difficoltà di garantire la rappresentatività del campione quando la popolazione è altamente eterogenea.
In questo caso è importante avere conoscenza specifica del dominio (domain knowledge) per comprendere e affrontare correttamente questi fenomeni.
Campionamento stratificato
Il campionamento stratificato è una tecnica di campionamento che prevede la suddivisione della popolazione in gruppi omogenei, chiamati strati, in base a una o più caratteristiche.
Una volta definiti gli strati, si procede a selezionare un campione casuale semplice all'interno di ciascuno strato.
Questa tecnica è utile quando la popolazione presenta eterogeneità nelle caratteristiche di interesse e si vuole garantire che ogni strato venga rappresentato adeguatamente nel campione.
Ad esempio, se si vuole studiare la soddisfazione dei clienti di un'azienda in diverse regioni, si potrebbe suddividere la popolazione in base alla regione di provenienza e selezionare un campione casuale semplice di clienti da ciascuna regione.
Campionamento sistematico
l campionamento sistematico è un metodo di campionamento probabilistico in cui gli elementi della popolazione sono disposti in un elenco ordinato e viene selezionato un elemento ogni k-esimo elemento (ad esempio, ogni decimo elemento) a partire da un punto di partenza casuale.
Questo metodo di campionamento viene utilizzato quando l'elenco degli elementi della popolazione è già disponibile e viene richiesta una selezione casuale di un campione.
Il campionamento sistematico è utile quando la popolazione è grande e l'individuazione di ogni singolo individuo richiederebbe troppo tempo o risorse. Tuttavia, il campionamento sistematico può essere soggetto a bias se l'intervallo tra gli individui scelti coincide con un particolare pattern nella popolazione.
Campionamento per quote
Il campionamento per quote è un metodo di campionamento non probabilistico in cui vengono selezionati i soggetti per il campione in modo da ottenere una rappresentazione proporzionale delle caratteristiche della popolazione di riferimento.
In questo metodo, la popolazione viene suddivisa in categorie o ""quote"" sulla base di alcune caratteristiche di interesse (ad esempio, genere, età, istruzione, regione geografica) e viene determinato il numero di soggetti da selezionare per ogni quota sulla base delle proporzioni della popolazione.
La selezione dei soggetti all'interno di ogni quota può essere effettuata utilizzando un metodo di campionamento casuale o non casuale, a seconda delle esigenze dello studio.
Il vantaggio del campionamento per quote è che permette di ottenere un campione che rappresenta proporzionalmente le caratteristiche della popolazione di riferimento, anche se la scelta dei soggetti all'interno di ogni quota non è effettuata in modo casuale.
Questo metodo di campionamento è spesso utilizzato in sondaggi di opinione, poiché consente di raggiungere un campione rappresentativo in modo relativamente rapido ed economico. Tuttavia, il campionamento per quote può essere influenzato dalla conoscenza e dalle opinioni del selezionatore che sceglie i soggetti per il campione, e può quindi essere soggetto a bias di selezione.
Bootstrapping
Il bootstrapping è una tecnica di ricampionamento che permette di approssimare la popolazione andando a selezionare casualmente con sostituzione un certo numero di elementi dalla popolazione per generare un campione.
Questo processo viene ripetuto molte volte, in modo tale da creare una grossa quantità di campioni. Da questi campioni viene estratta una qualsivoglia statistica (come la media o la mediana) e questa diventa parte del campione finale che andrà ad approssimare la popolazione.
Il bootstrapping è utile quando si desidera stimare la precisione di una statistica campionaria o di un modello di machine learning. Invece di fare supposizioni sulla distribuzione della popolazione, il bootstrapping utilizza la distribuzione dei campioni sintetici per stimare l'errore standard e gli intervalli di confidenza.
Il bootstrapping è particolarmente utile quando la distribuzione della popolazione è sconosciuta o quando non è possibile ottenere campioni ripetuti dalla popolazione originale.
Conclusione
In questo articolo abbiamo visto come il campionamento statistico sia un concetto fondamentale durante il processo di ricerca.
Abbiamo visto come il campionamento può aiutare a ottenere informazioni sulla popolazione di interesse con un'efficienza maggiore rispetto alla raccolta di dati sull'intera popolazione, in base alle conoscenze del ricercatore nel dominio di riferimento e ai vari bias alla quale è esposto.
Abbiamo anche discusso delle tecniche di campionamento più utilizzate, tra cui il campionamento casuale, quello stratificato, quello sistematico e per quote, nonché la tecnica del bootstrapping.
Infine, abbiamo sottolineato l'importanza di una corretta definizione della popolazione di riferimento e della scelta del metodo di campionamento più appropriato per gli obiettivi dello studio.
Commenti dalla community"
https://www.diariodiunanalista.it/posts/introduzione-al-clustering-non-supervisionato-con-k-means/,"L'apprendimento non supervisionato è una branca fondamentale dell'analisi dei dati che si concentra sulla scoperta di strutture nascoste nei dati senza la presenza di etichette di output.
Tra le diverse tecniche di apprendimento non supervisionato, l'algoritmo K-Means è uno dei più utilizzati e apprezzati per il clustering dei dati.
Nel presente articolo, esploreremo in dettaglio il funzionamento e l'applicazione di K-means, un algoritmo di clustering partizionale che ha dimostrato di essere efficace nella suddivisione dei dati in gruppi omogenei.
Partizionale significa che, a ogni passo, lavora su una porzione diversa di dati. Questo lo differenzia dai metodi gerarchici che invece trovano nuovi cluster basandosi su cluster precedentemente stabiliti.
L'obiettivo di K-means è assegnare ogni punto dati al cluster più vicino rispetto al centroide di quel cluster, al fine di minimizzare l'errore quadratico totale.
- come funziona l'algoritmo K-Means, comprendendo l'intuizione alla sua base
- come applicarlo in Python con Sklearn
- come valutare i risultati della sua applicazione
Come funziona l'algoritmo K-Means
Il processo di raggruppamento non supervisionato può essere descritto in cinque step:
- Gli elementi del campione vengono assegnati casualmente ai K cluster definiti inizialmente. Ricordiamoci che il K-Means necessita che l'utente definisca K come iperparametro.
- Per ogni elemento del campione, viene calcolata la distanza tra esso e tutti i centroidi di classe, che rappresentano i punti medi dei cluster.
- Successivamente, ogni elemento del campione viene assegnato al cluster il cui centroide è più vicino.
- I centroidi vengono ricalcolati in base ai punti assegnati a ciascun cluster, rappresentando nuovi punti medi.
- Il processo di assegnazione e ricalcolo viene ripetuto fino a quando non si verifica la convergenza, ovvero fino a quando non ci sono ulteriori spostamenti di elementi dai cluster.
La domanda di come trovare i valore di K è legittima.
Come menzionato, K è un parametro in input al K-Means e il suo valore può cambiare completamente il comportamento di clustering.
Un metodo comune per determinare il valore di K è utilizzare il grafico a gomito (elbow plot), che mostra l'andamento dell'errore quadratico totale al variare di K.
Nel grafico a gomito, il punto in cui la curva assume un angolo a gomito rappresenta il numero ottimale di cluster desiderati.
Nei prossimi paragrafi vedremo insieme un esempio pratico segmentazione dei clienti, un caso di utilizzo classico del clustering.
Segmentazione dei clienti
Prima di tutto vediamo il dataset che ci terrà compagnia durante la lettura.
Il dataset Mall Customer, noto anche come dataset di segmentazione dei clienti, fornisce informazioni preziose sui clienti presenti in un centro commerciale.
Il dataset contiene le seguenti caratteristiche:
CustomerID: Un identificatore unico per ciascun cliente.
Gender: Il genere del cliente ( Maschio o Femmina).
Age: L'età del cliente.
Annual Income (k$): Il reddito annuale del cliente in migliaia di dollari.
Spending Score (1-100): Un punteggio assegnato a ciascun cliente in base al loro comportamento di spesa e altre caratteristiche legate agli acquisti.
L'obiettivo dell'analisi del dataset in questione è quello di eseguire la segmentazione dei clienti, ovvero suddividere i clienti in gruppi distinti in base alle loro caratteristiche simili.
La segmentazione dei clienti aiuta le aziende a identificare segmenti di clientela preziosi, comprendere le loro esigenze e sviluppare strategie di marketing personalizzate per raggiungere ciascun segmento in modo efficace.
Utilizzando vari algoritmi di clustering come K-Means, le aziende possono identificare modelli e preferenze tra i diversi gruppi di clienti.
Questa segmentazione consente alle aziende di offrire promozioni mirate, creare raccomandazioni di prodotti personalizzati e ottimizzare la disposizione dei negozi per migliorare l'esperienza del cliente.
Iniziamo a vedere gli import
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly as ply
import plotly.express as px
import plotly.graph_objs as go
from plotly.subplots import make_subplots
import plotly.subplots as sp
from sklearn.cluster import KMeans
df = pd.read_csv(""/content/drive/MyDrive/Colab Notebooks/Mall_Customers.csv"")
df.head()
È composto da 200 osservazioni di cui distinguiamo 112 Donne e 88 Uomini.
sns.countplot(y = 'Gender', data = df)
plt.show()
Grazie a Seaborn possiamo creare efficacemente grafici che vengono divisi per una variabile specifica. In questo caso,
genere.
df.drop(['CustomerID'], axis = 1, inplace = True)
sns.pairplot(df, hue = 'Gender')
Viene creato un grafico con una griglia di grafici di dispersione (scatter plot). Sulla diagonale principale della griglia, troviamo le densità delle distribuzioni delle variabili condizionatamente al genere.
Negli altri pannelli della griglia, abbiamo gli scatter plot per le coppie di variabili, consentendo di esplorare le relazioni tra di esse in modo rapido ed efficace.
from sklearn.preprocessing import StandardScaler
X1 = df.loc[:,['Age', 'Spending Score (1-100)']].values
X2 = df.loc[:,['Annual Income (k$)', 'Spending Score (1-100)']].values
X3 = df.loc[:,['Age', 'Annual Income (k$)']].values
# Standardizziamo i dati
scaler = StandardScaler()
X1 = scaler.fit_transform(X1)
X2 = scaler.fit_transform(X2)
X3 = scaler.fit_transform(X3)
Applichiamo la standardizzazione ai dati poi X1, X2, X3 sono tre diverse combinazioni di feature prese dal DataFrame. Ogni variabile X rappresenta una diversa combinazione di due colonne del DataFrame.
Ora creiamo un grafico a gomito.
sse = []
k_rng = range(1,10) # numero di cluster testati
for k in k_rng:
km1 = KMeans(n_clusters=k)
km1.fit(X1)
sse.append(km1.inertia_)
# SSE per tutti i punti dati nel dataset rispetto ai centroidi dei cluster a cui sono stati assegnati.
Calcoliamo la somma degli errori quadratici (SSE) per diverse configurazioni del numero di cluster utilizzando l'algoritmo K-means su dati standardizzati. Alla fine del loop, avremo una lista contenente i valori SSE per ciascun numero di cluster testato su X1.
Questi valori possono essere utilizzati per tracciare il grafico a gomito (elbow plot) e identificare il numero ottimale di cluster per i dati.
Di fatto, il grafico a gomito mostra come la somma degli SSE cambia all'aumentare di K. Il valore più basso dove la diminuzione del SSE non è repentina viene considerato come gomito, cioè numero di K.
plt.xlabel('K')
plt.ylabel('Sum of squared error')
plt.plot(k_rng,sse, linewidth=2, marker='8')
Vediamo come il valore ottimale è K = 4 poiché dopo il 4 il valore del SSE diminuisce più lentamente.
A questo punto implementiamo l'algoritmo K-means.
km1 = KMeans(n_clusters=4)
y_predicted1 = km1.fit_predict(X1)
y_predicted1
L'array
y_predicted1 contiene le etichette dei cluster per i dati X1 in base all'algoritmo K-means con 4 cluster. Ogni elemento dell'array rappresenta il cluster assegnato per il corrispondente punto dati in X1.
plt.scatter(X1[:,0],X1[:,1],c=km1.labels_, cmap='rainbow')
plt.scatter(km1.cluster_centers_[:,0],km1.cluster_centers_[:,1], color='black', marker='*', label='centroid')
plt.xlabel('Age (scaled)')
plt.ylabel('Spending Score (scaled)')
plt.legend()
plt.show()
plt.show()
Visualizziamo i risultati del clustering ottenuti dall'algoritmo K-Means sui dati standardizzati X1.
Il grafico mostra i punti dati nel piano bidimensionale delle feature
Age e
Spending Score (1-100) colorati in base all'etichetta del cluster assegnata loro dall'algoritmo K-Means, e i centroidi dei cluster saranno rappresentati da stelle nere.
In questi casi, i gruppi possono essere creati sul risultato della PCA, avendo quindi una rappresentazione di tutte le feature (X1, X2, ..., etc).
Rappresentazione dei gruppi avanzata
Adesso non resta che ripetere la procedura per le altre due configurazioni di dati X2 e X3.
Useremo però una modalità di visualizzazione più chiara usando una funzione helper con Sklearn.
kmeans_kwargs = {
'init': 'k-means++', #inizializzazione uniforme dei centroidi
'n_init': 20, # numero di inizializzazioni diverse
'max_iter': 300,
'random_state': 42
}
Definiamo un dizionario
kmeans_kwargs contenente diversi parametri da utilizzare nell'algoritmo K-Means. Questi parametri possono essere passati come argomenti per personalizzare il comportamento dell'algoritmo K-means.
Questi parametri sono opzionali e possono essere utilizzati per ottimizzare le prestazioni dell'algoritmo in base alle caratteristiche specifiche del dataset.
def kmeans_model(k, x): #k = cluster x = dati
model = KMeans(k, **kmeans_kwargs)
model.fit(x)
labels = model.labels_
centroids = model.cluster_centers_
return model, labels, centroids
def plot_clusters(x, h, model, labels, centroids):
# meshgrid
x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1
y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
z = model.predict(np.c_[xx.ravel(), yy.ravel()])
z = z.reshape(xx.shape) #stessa dimensione meshgrid
# rappresentazione cluster con meshgrid
sns.set_style('ticks')
plt.clf()
plt.figure(figsize=(12, 6))
plt.imshow(z, interpolation = 'nearest',
extent = (xx.min(), xx.max(), yy.min(), yy.max()),
cmap = 'Pastel1', aspect = 'auto',
origin = 'lower')
sns.scatterplot(x = x[:, 0], y = x[:, 1],
hue = labels, palette = 'tab10', s = 100)
plt.scatter(x = centroids[:, 0] , y = centroids[:, 1],
s = 100, c = 'black', alpha = 0.8, marker = 'X')
plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)
plt.show()
Creiamo una funzione per un grafico utile a visualizzare la suddivisione del piano bidimensionale in cluster e i centroidi dei cluster.
Viene utilizzato meshgrid per creare una griglia di punti in tutto lo spazio dei dati, e per ciascun punto del meshgrid, viene previsto il cluster a cui appartiene utilizzando il modello K-Means addestrato.
In seguito, i punti dati sono colorati in base alle etichette dei cluster, i centroidi dei cluster sono rappresentati come simboli ""X"" neri.
X2_model, X2_labels, X2_centroids = kmeans_model(5, X2)
plot_clusters(X2, 0.02, X2_model, X2_labels, X2_centroids)
Visualizziamo i risultati del clustering ottenuti dall'algoritmo K-Means sui dati standardizzati X2.
Il grafico mostra i punti dati nel piano bidimensionale delle feature
Annual Income (k$) e
Spending Score (1-100) colorati in base all'etichetta del cluster assegnata loro dall'algoritmo K-Means, e i centroidi dei cluster saranno rappresentati da stelle nere.
Per questa configurazioni di dati il valore di K è 5.
X3_model, X3_labels, X3_centroids = kmeans_model(3, X3)
plot_clusters(X3, 0.02, X3_model, X3_labels, X3_centroids)
Visualizziamo i risultati del clustering ottenuti dall'algoritmo K-Means sui dati standardizzati X2.
Il grafico mostra i punti dati nel piano bidimensionale delle feature
Age e
Annual Income (k$) colorati in base all'etichetta del cluster assegnata loro dall'algoritmo K-Means, e i centroidi dei cluster saranno rappresentati da stelle nere.
Interpretazione del dato
La segmentazione vista sopra è molto rilevante per il business.
Questo perché gruppi diversi hanno comportamenti diversi.
Se implementata nel nostro business, questo tipo di logica permette di collocare facilmente un utente in uno specifico gruppo di comportamenti.
Questi comportamenti possono essere poi oggetto di campagne di marketing oppure di strategie dedicate.
Vuoi introdurre la segmentazione dei clienti nel tuo business?Contattaci
Conclusione
La tecnica di clustering K-Means è un algoritmo di apprendimento non supervisionato ampiamente utilizzato nella data science per identificare strutture nascoste nei dati e suddividerli in gruppi omogenei.
Ha diversi pregi e difetti che devono essere considerati quando si applica questa tecnica.
Pregi del K-Means
- Semplice da implementare e veloce: K-Means è un algoritmo semplice e intuitivo da implementare, rendendolo ideale per la rapida analisi esplorativa dei dati.
- Scalabilità: K-Means funziona bene su dataset con un gran numero di punti dati e feature. Può essere utilizzato anche con dati di alta dimensionalità senza compromettere significativamente le prestazioni.
- Interpretabilità dei risultati: I cluster creati da K-Means sono facilmente interpretabili, poiché ogni cluster è rappresentato da un centroide e le istanze più vicine a quel centroide appartengono a quel cluster. Ciò facilita la comprensione delle strutture nascoste nei dati.
- Adattabilità: K-Means è flessibile e può essere facilmente adattato per affrontare diverse sfide di clustering. È possibile scegliere il numero di cluster desiderato e personalizzare i parametri per ottenere i risultati desiderati.
Difetti del K-Means:
- Numero di cluster predefinito: L'utente deve specificare il numero di cluster desiderato (K) prima dell'esecuzione dell'algoritmo. Tuttavia, scegliere il numero ottimale di cluster può essere un'attività soggettiva e talvolta difficile.
- Sensibile all'inizializzazione dei centroidi: L'output di K-means può variare a seconda dell'inizializzazione casuale dei centroidi dei cluster. Ciò può portare a risultati diversi in esecuzioni multiple.
- Non adatto a forme di cluster complesse: K-Means fa l'assunzione che i cluster siano di forma sferica e con la stessa varianza. Pertanto, potrebbe non funzionare bene con cluster di forma irregolare o di dimensioni diverse.
- Sensibile alla normalizzazione delle feature: K-Means è influenzato dalla scala delle feature presenti nel dataset. Pertanto, è fondamentale standardizzare i dati prima di eseguire l'algoritmo per garantire che tutte le feature abbiano lo stesso impatto.
- Computazionalmente oneroso: se il dataset è molto grande (già 100k+ righe) oppure pieno di feature, il K-Means potrebbe non essere la soluzione ideale per il clustering, in quanto la convergenza sarebbe molto lenta.
K-Means rimane uno strumento prezioso. Nella pratica, è sempre consigliabile eseguire diverse prove con valori diversi di K e considerare l'uso di altre tecniche di clustering per affrontare le limitazioni di K-Means in caso di strutture di cluster più complesse.
Commenti dalla community"
https://www.diariodiunanalista.it/posts/introduzione-alla-pca-in-python-con-sklearn-pandas-e-matplotlib/,"Nel contesto dell'analisi dei dati, ci troviamo spesso ad affrontare sfide complesse dovute alla crescente quantità di informazioni disponibili.
È innegabile che l'accumulo di dati da svariate fonti sia diventato una costante nella nostra vita. Data scientist o meno, tutti praticamente descrivono un fenomeno come una collezione di attributi che lo caratterizzano.
È molto raro lavorare alla risoluzione di una sfida analitica senza dover gestire un insieme di dati multidimensionali - questo è particolarmente evidente oggi, in cui la raccolta di dati è sempre più automatizzata e la tecnologia ci permette di acquisire informazioni da una vasta gamma di fonti, compresi sensori, dispositivi IoT, social media, transazioni online e molto altro.
Per un data scientist, l'obiettivo è comprendere, estrarre significato e ottenere previsioni o insight dai dati stessi: ma solitamente maggiore è la complessità del fenomeno, maggiore sarà la difficoltà di raggiungere questi obiettivi.
Con l'aumentare della complessità dei dati, sorgono nuove sfide. Ecco alcune di queste sfide:
- Dimensionalità elevata: Avere molte colonne può portare a problemi di dimensionalità elevata, che possono rendere i modelli più complessi e difficili da interpretare. La riduzione della dimensionalità o l'uso di tecniche di selezione delle caratteristiche possono aiutare a mitigare questo problema.
- Dati rumorosi o sporchi: La raccolta automatica di dati può comportare la presenza di errori, dati mancanti o dati inaffidabili. La pulizia dei dati è una parte critica del processo di analisi.
- Interpretazione: Con molti attributi, può essere difficile interpretare le relazioni tra di essi e comprendere quali siano le caratteristiche più influenti per un determinato problema.
- Overfitting: Modelli troppo complessi possono soffrire di overfitting, ossia l'adattamento eccessivo ai dati di addestramento, con conseguente scarsa capacità di generalizzazione ai nuovi dati.
- Risorse computazionali: L'analisi di dataset grandi e complessi richiede spesso risorse computazionali significative. La scalabilità è una considerazione importante.
- Privacy e sicurezza: La gestione di dati sensibili o personali richiede attenzione alla privacy e alla sicurezza dei dati.
- Comunicazione dei risultati: Spiegare le scoperte ottenute da un dataset multidimensionale in modo comprensibile è una sfida importante, soprattutto quando si comunica con stakeholder non tecnici.
Ho scritto un articolo che si collega a questo argomento, che puoi leggere qui 👇
Nell'ambito della data science e del machine learning un dataset multidimensionale è una raccolta di dati organizzata in modo da includere molteplici colonne o attributi, ciascuna delle quali rappresenta una caratteristica del fenomeno che si sta studiando o cercando di predire.
Un dataset che contiene informazioni sugli immobili, come case o appartamenti, rappresenta un esempio concreto di dataset multidimensionale. Infatti ogni casa può essere descritta con i suoi metri quadri, il numero di stanze, se esiste un garage o meno e così via.
In questo articolo, esploreremo come utilizzare la PCA per semplificare e visualizzare dati multidimensionali in modo efficace, rendendo accessibili informazioni complesse multidimensionali.
- L'intuizione dietro l'algoritmo della PCA
- Applicare la PCA con Sklearn su un dataset giocattolo
- Utilizzare Matplotlib per visualizzare i dati ridotti
- I principali casi d'uso della PCA
Iniziamo subito!
Intuizione alla base dell'algoritmo PCA
L'analisi dei componenti principali (in inglese Principal Component Analysis, PCA) è una tecnica statistica non supervisionata di decomposizione di dati multidimensionali.
Il suo scopo principale è quello di ridurre in un numero di variabili arbitrarie dette componenti un nostro dataset multidimensionale al fine di
- selezionare feature importanti nel dataset originale
- aumentare il rapporto segnale / rumore
- creare nuove feature da fornire ad un modello di machine learning
- visualizzare dati multidimensionali
In base al numero di componenti che scegliamo, l'algoritmo PCA permette la riduzione del numero di variabili nel dataset originale andando a preservare quelle che spiegano meglio la varianza totale del dataset stesso, andando a combattere la maledizione della dimensionalità (curse of dimensionality).
La maledizione della dimensionalità è un concetto nel machine learning che si riferisce alla difficoltà che si incontra quando si lavora con dati a elevata dimensionalità.
Con l'aumentare del numero di dimensioni, il numero di dati necessari per rappresentare un insieme di dati in modo affidabile aumenta esponenzialmente. Ciò può rendere difficile trovare pattern interessanti nei dati e può causare problemi di overfitting nei modelli di apprendimento automatico.
La trasformazione applicata dalla PCA riduce le dimensioni del dataset creando delle componenti che raccolgono al meglio la varianza dei dati originali. Questo permette di isolare le variabili più rilevanti e di ridurre la complessità del dataset.
La PCA è tipicamente una tecnica difficile da assimilare, soprattutto per studenti e praticanti alle prime armi nel campo della data science e analytics.
La motivazione di questa difficoltà va ricercata nelle basi strettamente matematiche dell'algoritmo.
Ma cosa fa dal punto di vista matematico la PCA?
La PCA permette di proiettare un dataset n-dimensionale in un piano dimensionale più basso.
Sembra complesso, ma in realtà non lo è. Proviamo a intuire il procedimento generale:
quando disegniamo qualcosa su un foglio di carta, stiamo di fatto prendendo una rappresentazione mentale (che possiamo rappresentare in 3 dimensioni) e proiettandola sul foglio, andando a ridurre la qualità e la precisione della rappresentazione.
La rappresentazione su foglio rimane comunque comprensibile e apprezzabile dal prossimo!
Infatti durante l'atto di disegnare andiamo a rappresentare forme, linee e ombreggiature al fine di permettere al prossimo di comprendere quello che noi stiamo pensando nel nostro cervello.
Prendiamo come esempio questa immagine di uno squalo
se volessimo disegnarlo su un foglio di carta, in base al nostro livello di bravura (il mio è molto basso come si può notare), potremmo rappresentarlo così
Il punto che voglio passare è che nonostante la rappresentazione non sia perfettamente 1:1, un osservatore può comprendere facilmente che il disegno rappresenta uno squalo.
Di fatto, l'algoritmo ""mentale"" che abbiamo usato è simile alla PCA - abbiamo ridotto la dimensionalità, quindi le caratteristiche dello squalo in fotografia, e usato solamente le dimensioni più rilevanti per comunicare il concetto di ""squalo"" sul foglio di carta.
Matematicamente parlando quindi, non vogliamo solo proiettare il nostro oggetto in un piano dimensionale inferiore, ma vogliamo anche preservare il maggior numero di informazioni rilevanti possibili.
Ti piace quello che stai leggendo?
Iscriviti al blog per rimanere sempre aggiornato sui contenuti di settore. No pubblicità. No spam.
Solo contenuto utile per la tua carriera nel ML e DS.
Puoi rimuovere l'iscrizione quando vuoi seguendo il link nella email.
La compressione dei dati
Utilizziamo un semplice dataset per procedere con un esempio. Questo dataset contiene informazioni strutturali su immobili, come grandezza in metri quadri, numero di stanze e così via.
Qui l'obiettivo è mostrare quanto facile è toccare i limiti della visualizzazione dei dati quando si ha un dataset multidimensionale e come la PCA può aiutarci a superare queste limitazioni.
La dimensionalità di un dataset può essere intesa semplicemente come il numero di colonne all'interno dello stesso. Una colonna rappresenta un attributo, una caratteristica, del fenomeno che stiamo studiando. Più dimensioni ci sono, più complesso è il fenomeno.
In questo caso abbiamo un dataset con 5 dimensioni.
Ma che si intende limitazioni della visualizzazione dei dati? Iniziamo a visualizzare i dati partendo da
metri_quadri.
Vediamo come gli immobili 1 e 2 abbiano un basso valore di
metri_quadri mentre tutti gli altri sono intorno o superiori al valore di 100. Questo è un grafico uni-dimensionale proprio perché prendiamo in considerazione una sola variabile.
Ora aggiungiamo una dimensione al grafico.
Questo tipo di grafico, chiamato a dispersione (scatterplot in inglese), mostra la relazione tra due variabili. È molto utile per visualizzare correlazioni e interazioni tra variabili.
Questa visualizzazione inizia già ad inserire un buon livello di complessità interpretativa, in quanto richiede una attenta ispezione per comprendere la relazione tra le variabili anche da analisti esperti.
Ora andiamo a inserire ancora un'altra variabile.
Questa è sicuramente una immagine complessa da elaborare. Dal punto di vista matematico però, questa è una visualizzazione che ha perfettamente senso. Dal punto di vista percettivo e interpretativo, siamo al limite della comprensione umana.
Sappiamo tutti come la nostra interpretazione del mondo si fermi al tridimensionale. Sappiamo anche però che questo dataset è caratterizzato da 5 dimensioni.
Ma quindi come facciamo a visualizzarle tutte?
Non possiamo, a meno che non visualizziamo relazioni bidimensionali tra tutte le variabili, una di fianco all'altra.
Nell'esempio in basso, vediamo come
metri_quadri sia messa in relazione bidimensionale con
numero_stanze e
numero_vicini.
Immaginiamo ora di mettere fianco a fianco tutte le combinazioni possibili...saremmo presto sopraffatti dal grande numero di informazioni da tenere a mente.
Ecco che entra in gioco la PCA. Usando Python (vedremo in seguito), possiamo applicare su questo dataset la classe PCA di Sklearn e ottenere un grafico del genere.
Quello che vediamo qui è un grafico che mostra le componenti principali restituite dalla PCA.
In pratica, l'algoritmo PCA effettua una trasformazione lineare sui dati in modo da trovare la combinazione lineare di feature che meglio spiega la varianza totale del dataset.
Questa combinazione di feature viene chiamata componente principale. Il processo viene ripetuto per ogni componente principale finché non si raggiunge il numero desiderato di componenti.
Il vantaggio di utilizzare la PCA è che permette di ridurre la dimensionalità dei dati mantenendo le informazioni più importanti, eliminando quelle meno rilevanti e rendendo i dati più facili da visualizzare e da utilizzare per costruire modelli di machine learning.
Se ti interessa andare più a fondo sulla matematica dietro la PCA, ti suggerisco le seguenti risorse in inglese:
- PCA Explained Step-By-Step da StatQuest
- Principal Component Analysis (PCA) Explained Visually with Zero Math
Se ti piace apprendere leggendo, ti consiglio questo manuale che copre in dettaglio l'argomento PCA con il focus sul machine learning.
Implementazione in Python
Per applicare la PCA in Python, possiamo utilizzare la libreria scikit-learn, che offre un'implementazione semplice ed efficace.
A questo collegamento è possibile leggere la documentazione della PCA.
Il wine dataset sarà il dataset giocattolo che useremo per l'esempio.
Iniziamo importanto le librerie essenziali
# Importiamo le librerie necessarie
from sklearn.datasets import load_wine
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
# carichiamo il dataset
wine = load_wine()
# convertiamo il dataset in un dataframe Pandas
df = pd.DataFrame(data=wine.data, columns=wine.feature_names)
# creiamo la colonna per il target
df[""target""] = wine.target
df
>>>
La dimensionalità dei dati è
(178, 14) - significa che ci sono 178 righe (esempi da cui un modello di machine learning può apprendere) ognuna di queste descritta da 14 dimensioni.
Applichiamo la normalizzazione dei dati prima di applicare la PCA. È possibile farlo con Sklearn.
# Standardizzare i dati
from sklearn.preprocessing import StandardScaler
X_std = StandardScaler().fit_transform(df.drop(columns=[""target""]))
Ora siamo pronti a ridurre le dimensioni. Applichiamo la PCA semplicemente così
# Oggetto PCA specificando il numero di componenti principali desiderate
pca = PCA(n_components=2) # vogliamo proiettare due dimensioni in modo da poterle visualizzare!
# Addestriamo il modello PCA sui dati standardizzati
vecs = pca.fit_transform(X_std)
È possibile specificare qualsiasi numero di dimensioni di output della PCA a patto che queste siano inferiori a 14, cioè il numero totale di dimensioni nel dataset originale.
Ora organizziamo la versione ridotta del dataframe in un nuovo oggetto Pandas Dataframe:
reduced_df = pd.DataFrame(data=vecs, columns=['Principal Component 1', 'Principal Component 2'])
final_df = pd.concat([reduced_df, df[['target']]], axis=1)
final_df
>>>
Principal Component 1 e 2 sono le dimensioni di output della PCA, che ora saranno possibili visualizzare con uno scatterplot.
plt.figure(figsize=(8, 6)) # settiamo la grandezza del canvas
targets = list(set(final_df['target'])) # creiamo una lista di possibili target (ne sono 3)
colors = ['r', 'g', 'b'] # definiamo una semplice lista di colori per differenziare i target
# cicliamo per assegnare ogni punto ad un target e colore
for target, color in zip(targets, colors):
idx = final_df['target'] == target
plt.scatter(final_df.loc[idx, 'Principal Component 1'], final_df.loc[idx, 'Principal Component 2'], c=color, s=50)
# infine, mostriamo il grafico
plt.legend(targets, title=""Target"", loc='upper right')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA su Wine Dataset')
plt.show()
Ed ecco fatto. Questo grafico mostra la differenza tra i vini descritta dalle 14 variabili iniziali, ma ridotte a 2 dalla PCA. La PCA ha ritenuto le informazioni più rilevanti e nel mentre ridotto il rumore presente nel dataset.
Ecco l'intero codice per applicare la PCA con Sklearn, Pandas e Matplotlib in Python.
import pandas as pd
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
wine = load_wine()
df = pd.DataFrame(data=wine.data, columns=wine.feature_names)
df[""target""] = wine.target
from sklearn.preprocessing import StandardScaler
X_std = StandardScaler().fit_transform(df.drop(columns=[""target""]))
pca = PCA(n_components=2)
vecs = pca.fit_transform(X_std)
reduced_df = pd.DataFrame(data=vecs, columns=['Principal Component 1', 'Principal Component 2'])
final_df = pd.concat([reduced_df, df[['target']]], axis=1)
plt.figure(figsize=(8, 6))
targets = list(set(final_df['target']))
colors = ['r', 'g', 'b']
for target, color in zip(targets, colors):
idx = final_df['target'] == target
plt.scatter(final_df.loc[idx, 'Principal Component 1'], final_df.loc[idx, 'Principal Component 2'], c=color, s=50)
plt.legend(targets, title=""Target"", loc='upper right')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA su Wine Dataset')
plt.show()
Casi d'uso della PCA
Di seguito una lista dei casi d'uso più comuni della PCA in ambito data science.
Migliorare le velocità di addestramento di modelli di machine learning
I dati compressi dalla PCA forniscono le informazioni importanti e sono molto più digeribili da un modello di machine learning, che ora basa il suo apprendimento su un numero di feature ridotto invece che sulla totalità delle feature presenti nel dataset originale.
Selezione delle feature
La PCA è essenzialmente uno strumento di selezione delle feature. Quando andiamo ad applicarla, cerchiamo le feature che spiegano la varianza del dataset al meglio.
È possibile creare una classifica delle componenti principali e ordinarle per importanza, con il primo componente che spiega la maggior varianza e l'ultimo componente che spiega il minimo.
Analizzando le componenti principali è possibile risalire alle feature originali e escludere quelle che non contribuiscono a preservare l'informazione nel piano dimensionale ridotto creato dalla PCA.
Identificazione delle anomalie
La PCA viene spesso utilizzato nell'identificazione delle anomalie perché può aiutare a identificare pattern nei dati che non sono facilmente distinguibili ad occhio nudo.
Le anomalie spesso appaiono come punti dati lontani dal gruppo principale nello spazio dimensionale inferiore, rendendoli più facili da rilevare.
Identificazione del segnale
In contrapposizione all'identificazione delle anomalie, la PCA è molto utile anche per la signal detection (identificazione del segnale).
Infatti, come la PCA può mettere in risalto le anomalie, può anche rimuovere il ""rumore di fondo"" che non contribuisce alla variabilità totale dei dati. Nel contesto del riconoscimento vocale, questo permette all'utilizzatore di isolare meglio le tracce vocali e di migliorare i sistemi di identificazione della persona attraverso voce.
Compressione delle immagini
Lavorare con le immagini può essere oneroso se abbiamo vincoli particolari, come quello di salvare l'immagine in un certo formato. Senza andare nel dettaglio, la PCA può essere utile per comprimere le immagini mantenendo comunque le informazioni presenti in esse.
Questo permette a algoritmi di machine learning di addestrarsi più velocemente a discapito di informazioni compresse ma di una certa qualità.
Conclusioni
Grazie per la tua attenzione 🙏Spero che la lettura sia stata piacevole e che tu abbia imparato qualcosa di nuovo.
Ricapitolando,
- hai appreso cosa significa dimensionalità di un dataset e le complicazioni che ne derivano
- hai imparato come funziona intuitivamente l'algoritmo PCA passo dopo passo
- hai imparato come implementarla in Python con Sklearn
- e infine hai appreso quali siano i casi d'uso più comuni della PCA in ambito data science
Se hai trovato questo articolo utile condividilo con i tuoi amici o colleghi appassionati e condividi il tuo feedback!
Alla prossima,
Andrea
Commenti dalla community"
https://www.diariodiunanalista.it/posts/introduzione-alle-reti-neurali-pesi-bias-e-attivazione/,"Questo articolo ha l'obiettivo di comunicarti come funzionano le reti neurali nell'ambito del machine e deep learning.
Le reti neurali sono un particolare tipo di tecnologia che ha completamente rivoluzionato gli ultimi tempi.
Infatti, modelli come GPT e Falcon sono proprio basati su diverse reti neurali che comunicano tra di loro in una architettura particolare chiamata transformer.
Spesso sentiamo dire che le reti neurali artificiali sono delle rappresentazioni dei neuroni cerebrali umani all'interno di un computer.
Questi insiemi di neuroni formano reti interconnesse, ma i loro processi che scatenano eventi e attivazioni sono alquanto diversi da quello di un cervello vero.
Un neurone, preso singolarmente, è relativamente inutile, ma se unito a centinaia o migliaia di altri neuroni formano un rete interconnessa che spesso supera le performance di qualsiasi altro algoritmo di machine learning.
Breve background storico
Il concetto di rete neurale è alquanto antico - i primi pensieri di modellare un software prendendo ispirazione dal cervello umano risalgono agli inizi del 1940, da parte di Donald Hebb, McCulloch e Pitts.
Per oltre 20 anni il concetto è rimasto sul piano della teoria, poiché l'addestramento delle reti neurali è stato possibile solo attraverso una maggiore potenza computazione e alla creazione dell'algoritmo di backpropagation da parte di Paul Werbos, un efficiente meccanismo che permette alla rete di imparare propagando il feedback di un neurone a quello che lo precede.
Oggi le reti neurali vengono utilizzate in una miriade di compiti grazie alla loro abilità di risolvere problemi prima considerati impossibili da risolvere come la traduzione simultanea tra lingue, sintesi di video e audio e guida autonoma.
Neurone naturale e neurone artificiale - quali sono le differenze?
Anche se è vero che le reti neurali si ispirano ai neuroni naturali, questo paragone è quasi fuorviante poiché le loro anatomie e comportamenti sono diversi.
Non andrò molto nell'aspetto neuroscientifico, ma i neuroni naturali sembrano preferire una attivazione basata su ""switch"", on oppure off.
Attività oppure nessuna attività. In seguito al periodo di attività, tra l'altro, i neuroni naturali mostrano un periodo refrattario, cioè dove la loro abilità di attivarsi nuovamente è soppressa.
Questo comportamento viene descritto nel concetto di potenziale d'azione.
Reti neurali come ""black box""
Le reti neurali sono considerate delle black box (scatola chiusa)- non sappiamo perché raggiungano queste performance, ma sappiamo come lo fanno.
I cosiddetti dense layers (strati densi), che sono gli strati più comuni in una rete neurale, creano interconnessioni tra i vari strati della rete.
Ogni neurone è connesso ad ogni altro neurone dello strato successivo, il che significa che il suo valore di output diventa l'input per i prossimi neuroni.
Ogni connessione tra neuroni possiede un peso (weight) che è uno dei fattori che viene modificato durante l'addestramento. Il peso della connessione influenza quanto input viene passato tra un neurone all'altro. Questo comportamento segue la formula \( inputs \times weights \).
Una volta che un neurone riceve gli input da tutti gli altri neuroni connessi ad esso, viene aggiunto un bias, un valore costante che va sommato al calcolo che coinvolge il peso menzionato. Anche il bias è un fattore che viene modificato durante l'addestramento.
L'output di un neurone è espresso dalla formula \( output = inputs \times weights + bias \).
L'aggiustamento di pesi e bias viene fatto nelle hidden layers (strati nascosti), che sono gli strati presenti tra lo strato di input e quello di output. Sono detti ""nascosti"" proprio perché non vediamo il comportamento di aggiustamento di pesi e bias.
Ecco perché le reti neurali sono delle black box.
Come apprende una rete neurale
La caratteristica che rende complesse le reti neurali è proprio la enorme mole di calcoli che avviene a livello sia di rete che di singolo neurone.
Insieme ai pesi e bias ci sono le funzioni di attivazione che aggiungono una ulteriore complessità matematica ma influenzano enormemente la performance di una rete neurale.
Si tratta di Deep Learning with Python di François Chollet. È un libro completo, dettagliato, matematicamente complesso ma ricco di esempi sia teorici che pratici.
Il deep learning non è argomento semplice, ma se sei interessato a questo percorso, questo manuale (e molti altri) non può mancare nella tua libreria.
Deep Learning with Python (seconda edizione) da F. Chollet
Una bibbia del deep learning in Python da uno degli esponenti del settore
Pesi e bias
Pesi e bias possono essere interpretati come un sistema di manopole che possiamo ruotare per ottimizzare il nostro modello - come quando cerchiamo di sintonizzare la nostra radio ruotando le manopole per cercare la frequenza gradita.
La differenza sostanziale è che in una rete neurale, abbiamo centinaia se non migliaia di manopole da girare per raggiungere il risultato finale.
Poiché pesi e bias sono dei parametri della rete, questi saranno oggetto del cambiamento generato dalla rotazione della manopola immaginaria.
Visto che i pesi sono moltiplicati all'input, questi influenzano la magnitudine dell'input. Il bias, invece, poiché è sommato all'espressione \( inputs \times weights \), sposterà la funzione nel piano dimensionale. Vediamo degli esempi.
Ricordiamo che la formula è \( output = inputs \times weights + bias \)
Com'è possibile notare, pesi e bias impattano il comportamento di ogni neurone artificiale, ma lo fanno in maniera rispettivamente diversa. I pesi sono solitamente inizializzati randomicamente mentre il bias a 0.
Il comportamento di un neurone è anche influenzato dalla sua funzione di attivazione che, parallela al potenziale d'azione per un neurone naturale, definisce le condizioni di attivazione e relativi valori dell'output finale.
Funzioni di attivazione
Il tema delle funzioni di attivazione merita un articolo a sé, ma qui presenterò una overview generale.
Se ricordate, ho menzionato come un neurone naturale abbia una attivazione a switch. In gergo informatico/matematico, chiamiamo questa funzione una step function (funzione gradino).
Seguendo la logica
\( 1 \ x > 0; 0 \ x \leq 0 \)
la funzione gradino permette al neurone di restituire 1 se l'input è maggiore di 0 oppure 0 se l'input è minore o uguale a 0. Questo comportamento simula il comportamento di un neurone naturale e segue la formula
\( output = sum(inputs \times weights) + bias \)
La step function è però molto semplice, e nel settore si tende ad usare delle funzioni di attivazione più complesse, come l'unità lineare rettificata (ReLU) e SoftMax.
Come scrivere una piccola rete in Python
Creemo una piccola rete neurale con 4 input e 3 neuroni per comprendere come funziona il calcolo di pesi e bias.
Iniziamo dal definire questi parametri manualmente a scopo d'esempio
inputs = [1, 2, 3, 4] # quattro input
# weights è un array 3x4 -> 3 neuroni, 4 pesi associati ad ogni connessione
weights = [[ 0.74864643, -1.00722027, 1.45983017, 1.34236011],
[-1.20116017, -0.08884298, -0.46555646, 0.02341039],
[-0.30973958, 0.89235565, -0.92841053, 0.12266543]]
biases = [0, 0.3, -0.5] # ogni neurone ha un bias
Ora creiamo il loop che andrà a creare la nostra piccola rete neurale
layer_outputs = [] # creiamo la lista che conterrà i risultati dell'elaborazione dei neuroni dello strato
# per ogni neurone
for neuron_weights, neuron_bias in zip(weights, biases):
# inizializziamo l'output a 0
neuron_output = 0
# per ogni input e peso
for n_input, weight in zip(inputs, neuron_weights):
# moltiplicare input e peso e aggiungerlo all'output
neuron_output += n_input * weight
# aggiungere il bias all'output
neuron_output += neuron_bias
# aggiungere il risultato del neurone allo strato
layer_outputs.append(neuron_output)
print(layer_outputs) # stampiamo il risultato
L'output finale è questo
Codice completo
inputs = [1, 2, 3, 4] # quattro input
# weights è un array 3x4 -> 3 neuroni, 4 pesi associati ad ogni connessione
weights = [[ 0.74864643, -1.00722027, 1.45983017, 1.34236011],
[-1.20116017, -0.08884298, -0.46555646, 0.02341039],
[-0.30973958, 0.89235565, -0.92841053, 0.12266543]]
biases = [0, 0.3, -0.5] # ogni neurone ha un bias
layer_outputs = [] # creiamo la lista che conterrà i risultati dell'elaborazione dei neuroni dello strato
# per ogni neurone
for neuron_weights, neuron_bias in zip(weights, biases):
# inizializziamo l'output a 0
neuron_output = 0
# per ogni input e peso
for n_input, weight in zip(inputs, neuron_weights):
# moltiplicare input e peso e aggiungerlo all'output
neuron_output += n_input * weight
# aggiungere il bias all'output
neuron_output += neuron_bias
# aggiungere il risultato del neurone allo strato
layer_outputs.append(neuron_output)
print(layer_outputs) # stampiamo il risultato
Commenti dalla community"
https://www.diariodiunanalista.it/posts/le-migliori-risorse-per-imparare-python-online/,"Una delle preoccupazioni maggiori per uno studente è quella di trovare un buon insegnante per la materia che si vuole imparare.
Questo discorso è eccezionalmente valido per la programmazione e il data science.
Sono dell'idea che tutti noi, a prescindere dall'età, possediamo già gli strumenti e le risorse per poter imparare e diventare bravi e operativi in questo campo. Dico questo perché una delle argomentazioni più comuni che sento da aspiranti analisti che però hanno gettato la spugna ha a che vedere con l'età e come questa sia un ostacolo che ferma lo sviluppo personale e professionale ancora prima di iniziare.
Ma questa è solo una scusa per non affrontare quello che abbiamo davanti: matematica, statistica e programmazione.
In termini invece di potenziale intellettivo, fatevi dire che tutti siamo in grado di imparare qualsiasi cosa a patto che non abbiamo particolari patologie o disturbi dell'apprendimento. Non fatevi dire l'opposto da nessuno.
Gli unici limiti sono la nostra motivazione e obiettivi.
Detto questo, vorrei proporre quelle che secondo me sono le migliori risorse per imparare Python online. Queste sono state selezionate in quanto hanno avuto un grosso impatto sul mio sviluppo da programmatore e a distanza di anni si rivelano essere ancora delle pietre miliari per la mia formazione.
Queste risorse sono siti web, canali YouTube, blog e altro. Non c'è un particolare ordine di priorità, in quanto tutte sono eccellenti e facilmente accessibili. Sono anche tutte gratis, ma nel caso di Coursera potete iscrivervi al corso e pagare per ottenere una certificazione. La maggior parte dei contenuti sono in lingua inglese.
Iniziamo.
Il corso che ha introdotto a Python me e altri centinaia di migliaia di studenti intorno al mondo. Charles Severance è un docente eccellente, che spiega concetti come pochi sanno fare. Questo corso da il benvenuto al mondo della programmazione, e tocca argomenti basilari con un dettaglio ma senza mai annoiare. La community è anche molto attiva e Dr. Severance risponde spesso online alle domande degli studenti.
Il curriculum include:
- Python for everybody - una introduzione alla programmazione per iniziare a capire di cosa si tratta e cosa possiamo fare
- Strutture dati in Python - liste, dizionari, tuple e altro
- Interfacciarsi al web con Python attraverso delle librerie specifiche come Requests
- Una introduzione ai database relazionali e SQLite3
Consiglio questa specializzazione a chiunque voglia iniziare la sua avventura nella programmazione o chi vuole migrare da un altro linguaggio a Python.
Senza ombra di dubbio una delle risorse online più rilevanti per Python. Fondato e gestito da Harrison Kinsley, noto come Sentdex su YouTube e Twitter, offre contenuto per ogni livello di esperienza con Python e data science. Sentdex è una sicurezza in termini di qualità e profondità di contenuti su Python, data science e analytics. Fate un favore a voi stessi e seguite questa persona su Twitter.
I contenuti sono raggruppati in argomenti facilmente accessibili. Alcuni dei più rilevanti sono:
- Python Fundamentals
- Data analysis
- Machine learning
Consiglio a tutti di spulciare questi argomenti e di andare nel dettaglio in quelli che interessano. Quelli su Python sono sicuramente ben curati e accessibili sia al neofita che all'utente intermedio.
C'è anche un server di Discord dove è possibile connettersi ad altri analisti appartenenti alla community.
Segue il punto precedente proprio il canale YouTube di Sentdex. Questo canale rappresenta tutto quello che un appassionato di programmazione e data analytics possa aspettarsi di ottenere da un insegnante online.
Harrison trasmette la passione per l'argomento, toccando argomenti basilari in Python e data science fino a quelli più complessi, come il reinforcement learning applicato al gaming, all'addestramento di sistemi di computer vision per il riconoscimento di oggetti in tempo reale e alla computazione quantistica.
Davvero un bacino di conoscenza enorme, dal quale sarebbe un errore non attingere.
Anche se Kaggle è conosciuto come l'hub principale del data science nel web, i suoi corsi introduttivi su Python sono molto validi ed efficaci per prepararci al nostro percorso da analisti. Sono curati proprio per dare consapevolezza all'utente dei concetti più rilevanti per poter affrontare i discorsi più legati al machine learning. Il curriculum comprende:
- introduzione a variabili e tipi di dato
- funzioni e condizioni
- strutture dati
- uso di librerie
Il corso su Python si raggiunge da questo link.
Corey Schafer ha contribuito parecchio alla mia formazione, soprattutto con le sue spiegazioni di Pandas e Flask. Purtroppo l'autore è inattivo da più di un anno su YouTube, ma il suo contenuto è in gran parte ancora molto ben posizionato e facilmente accessibile.
Ha una cadenza vocale e un modo di strutturare il suo contenuto che rapisce e aiuta la comprensione anche degli argomenti più ostici. Spiccano i tutorial su Pandas, Matplotlib, Django e Flask. Suggerisco di completare la specializzazione Python For Everybody e poi spulciare il suo canale per solidificare ulteriormente le basi.
Real Python espande sul contenuto proposto da Sentdex andando anche ad esplorare la parte più orientata alla software engineering, con tutorial e guide per chi vuole programmare applicazioni di vario genere in Python.
Offrono percorsi formativi per vari livelli di expertise, tutti di alta qualità e con numerosi commenti per migliorare la comprensione e connettere con la community attraverso quiz, podcast e Q&A con degli esperti.
Conclusione
Concludo l'articolo dicendo che queste risorse rappresentano ancora oggi per me dei pit stop quasi giornalieri per i problemi che devo risolvere nei miei progetti. Mentre alcuni, come Python for Everybody, sono introduttivi e si focalizzano sulle basi, i blog e i canali YouTube indicati vanno molto più a fondo in varie tematiche che possono interessarci.
Devo molto a ogni creatore coinvolto nel materiale esposto qui, e anche molti altri non menzionati. Le comunità nell'ambito della programmazione e data analytics sono davvero molte e partecipare ad esse ci permette di esperire il contatto diretto con gente più brava di noi. Ed è in questo particolare momento che cresciamo - proprio quando ci mettiamo nei panni degli allievi.
Se volete far parte di una community di appassionati di programmazione, data analytics e machine learning, iscrivetevi alla newsletter di questo blog e unitevi al server Discord di questo blog!
A presto,
Commenti dalla community"
https://www.diariodiunanalista.it/posts/machine-learning-vs-deep-learning/,"I termini machine learning e deep learning sono sicuramente centrali nel contesto della data science.
Le manifestazioni della IA più impattanti del momento, come ChatGPT, si basano proprio su machine e deep learning.
Il modo in cui questi termini vengono usati però è spesso lo stesso, e a volte possono significare anche la stessa cosa.
Questo porta spesso i non addetti ai lavori a fare confusione rispetto alla vera interpretazione di questi due termini.
L'obiettivo di questo articolo è proprio rispondere alla seguente domanda: qual è la differenza tra machine learning e deep learning?
Facciamo chiarezza introducendo alcune definizioni.
Definizioni di machine e deep learning
Per comprendere al meglio le differenze tra machine e deep learning, bisogna prima aver chiaro cosa siano effettivamente queste due discipline e in che contesto risiedono.
Questo è fondamentale soprattutto per chi vuole orientarsi nel campo e sta appena iniziando il suo viaggio nel mondo dell'analisi dati.
Ogni disciplina menzionata in questo articolo fa parte del campo dell'intelligenza artificiale. Questa fa a sua volta parte, come tanti sapranno, dell'informatica.
Di conseguenza, sia machine learning che deep learning fanno parte prima dell'IA e poi dell'informatica.
L'informatica è una disciplina scientifica molto ampia, quindi ci focalizzeremo solo sull'intelligenza artificiale perché è in questa che noi operiamo come analisti.
L'intelligenza artificiale è un campo molto ampio. Machine learning e deep Learning sono solo due delle discipline più conosciute.
Altre sono
- la creazione di algoritmi di ricerca e ottimizzazione
- creazione di strutture logiche, percettive, di apprendimento e pianificazione
- robotica
Ce ne sono molte altre, e in base al livello di dettaglio che vogliamo considerare queste possono essere espande per integrare altri gruppi.
Di conseguenza, machine learning e deep learning, facendo parte dell'IA, toccano e influenzano le discipline menzionate sopra e altrettanto fanno quest'ultime.
Per visualizzare questa gerarchia, consideriamo questa immagine
Molto probabilmente, uno studioso di robotica userà machine learning o deep learning per insegnare alla macchina a raggiungere un obiettivo. In questo caso, probabilmente userà il reinforcement learning (apprendimento per rinforzo).
Andiamo a vedere ora cosa siano machine learning e deep learning per capire le differenze.
Definizione di Machine Learning
Il termine machine learning si traduce in italiano apprendimento automatico ed è, come abbiamo menzionato, un ramo dell'intelligenza artificiale che a sua volta è una branchia dell'informatica.
Il machine learning permette ad una macchina di svolgere compiti senza essere espressamente programmarle per farlo.
Il machine learning quindi diventa una delle soluzioni più interessanti e potenti per automatizzare un compito.
È infatti questo uno dei motivi che hanno portato i data scientist a essere delle figure di spicco in contesto lavorativo - sono di fatto abilitatori dell'automazione.
Grazie al machine learning, un analista può potenzialmente considerare qualsiasi compito e inserire un livello di automazione in esso che può
- contribuire a velocizzare i tempi di esecuzione
- ridurre l'errore umano
- permettere di scalare il business su più fronti
Ognuno di questi punti è fondamentale in un contesto di business, in quanto hanno diretto impatto sull'aspetto economico.
Definizione di Deep Learning
Più che una disciplina a se stante, il termine deep learning indica una serie di strumenti specifici per risolvere un particolare gruppo di problemi.
Mentre il termine machine learning si riferisce alla disciplina, deep learning si riferisce al modo in cui la macchina impara.
Il termine di traduce come apprendimento profondo perché ha a che vedere con le reti neurali. Infatti, una rete neurale apprende attraverso i suoi strati, che possono arrivare ad essere molto profondi.
Le reti neurali sono considerate l'essenza del deep learning, e ce ne sono di veramente parecchi tipi...ognuna strutturata per risolvere un particolare tipo di problema.
Ad esempio, esistono delle reti neurali che ""ricordano"" sequenze molto lunghe, cosi che l'output sia influenzato non solo dagli ultimi dati, ma anche quelli che li precedevano parecchio prima. Queste sono chiamate LSTM (long-short term memory) neural networks.
Altre invece, chiamate reti neurali convoluzionali, applicano filtri alle immagini cosi da apprendere solo le caratteristiche più rilevanti dei soggetti rappresentati.
Se vuoi una introduzione alle reti neurali, segui il link qui in basso 👇
Differenze tra Machine Learning e Deep Learning
La differenza tra machine learning e deep learning sta prevalentemente nelle applicazioni e nelle performance degli algoritmi che appartengono a queste famiglie.
Molti di questi sono concepiti per risolvere specifici problemi, come le serie temporali. In ogni caso, esistono algoritmi per ogni salsa in entrambe le famiglie.
Ecco una lista delle differenze principali tra machine learning e deep learning.
Specializzazione
Il deep learning è un insieme di tecniche e algoritmi sicuramente più adatto per problemi specifici, solitamente molto complessi.
Poiché le reti neurali possono modellare qualsiasi funzione se hanno risorse e tempi illimitati, queste sono usate per fronteggiare problemi che coinvolgono dati non strutturati, come testo, video e audio.
Per i dati tabellari, quindi strutturati, tecniche di machine learning meno complesse possono anche superare le performance di un algoritmo di deep learning.
Inoltre, le reti neurali sono più efficaci degli algoritmi tradizionali ad imparare da dataset molto più grandi (big data).
Complessità
Sebbene esistano algoritmi di machine learning tradizionale molto complessi, come XGBoost, gli algoritmi di deep learning sono per definizione più complessi.
Progettare modelli di deep learning è una delle sfide più importanti nella data science. Ogni giorno, i machine learning engineers si prodigano per innovare nel campo. Alcune delle realtà più rilevanti nel campo sono Hugging Face, Google, Meta, Baidu, OpenAI e molti altri.
Richiesta computazionale
Tipicamente, gli algoritmi di machine learning tradizionale richiedono meno potenza computazionale rispetto a quelli di deep learning.
Questo perché le reti neurali possono sfruttare le GPU (graphic processing units - in pratica le schede video) per aumentare la velocità di addestramento.
Oltre alle GPU, possono sfruttare anche le TPU (tensor processing unit), che sono dei chip ottimizzati proprio per il deep learning.
Anche alcuni algoritmi di machine learning tradizionale possono sfruttare le GPU - tra di questi c'è XGBoost, LightGBM e Catboost.
Interpretabilità
Un aspetto spesso non considerato quando si parla di differenze tra machine learning e deep learning è quanto i modelli e algoritmi appartenenti a questi siano interpretabili.
Le reti neurali sono tipicamente considerate delle black box - vale a dire che sappiamo come queste funzionino, ma non sappiamo come queste raggiungano il risultato atteso, né possiamo prevederlo.
Solo sperimentando con diverse architetture possiamo gradualmente avvicinarci alla configurazione migliore.
Algoritmi tradizionali invece, come gli alberi decisionali, sono facilmente interpretabili e comunicare come questi funzionino e come raggiungano i risultati è relativamente facile.
Conclusione
Machine learning e deep learning sono essenzialmente la stessa cosa - metodi e tecniche che permettono ad una macchina di fare inferenze abbastanza precise da poter essere utilizzate in un contesto lavorativo e non. Queste metodiche dipendono dal contesto che abbiamo di fronte.
È assolutamente superfluo usare tecniche di deep learning su dataset tabellari di dimensioni ridotte, perché un ""banale"" random forest potrebbe performare molto meglio e convergere più velocemente alle soluzioni.
Il mio consiglio è come sempre di valutare attentamente il contesto e porsi delle domande chiare che aiutano a comprendere bene il problema che abbiamo di fronte.
Se vuoi leggere di più su consigli e approcci in generale, ti suggerisco di sfogliare la categoria carriera di questo blog che racchiude proprio articoli che vanno a toccare modelli mentali e template per ottimizzare il lavoro.
Commenti dalla community"
https://www.diariodiunanalista.it/posts/migliorare-i-propri-modelli-di-dati-con-pydantic/,"La modellazione dei dati è un aspetto importante di qualsiasi progetto basato sui dati e la scelta degli strumenti e delle tecniche giuste può fare una grande differenza nella qualità e nell'affidabilità dei modelli.
Uno strumento da prendere in considerazione è Pydantic, una libreria Python che fornisce un modo semplice ed efficace per definire e convalidare i modelli di dati.
Che cosa è Pydantic?
Pydantic è una libreria creata per fornire un modo semplice ed efficiente per definire e convalidare i modelli di dati in Python.
Si basa sulla popolare libreria
dataclass e offre una serie di funzionalità e vantaggi aggiuntivi, come:
Annotazioni di tipo (type annotations)
Pydantic utilizza le annotazioni di tipo per definire i campi e i tipi del modello di dati.
Questo non solo ci aiuta a documentare e comprendere il nostro modello, ma consente anche a Pydantic di convalidare automaticamente i dati e generare errori utili in caso di problemi.
Vincoli e valori predefiniti
Pydantic ci consente di definire vincoli e valori predefiniti per ogni campo del modello. Questo può aiutarci a garantire che i dati siano sempre validi e completi e può farci risparmiare molto tempo e fatica nella convalida e nella gestione degli errori.
Serializzazione e deserializzazione
Pydantic fornisce supporto integrato per la serializzazione e la deserializzazione dei modelli di dati in e da diversi formati, come JSON, YAML e XML. Questo può rendere molto più facile lavorare con i tuoi dati in contesti e ambienti diversi.
Come utilizzare Pydantic
Prima di tutto, installiamo Pydantic nel nostro ambiente virtuale con
pip install pydantic
Da qui è tutto abbastanza semplice e diretto. Ecco un esempio di come è possibile definire un modello di dati con Pydantic:
from pydantic import BaseModel
class User(BaseModel):
id: int
name: str
age: int
email: str
password: str
Questo semplice modello definisce una classe
User con quattro campi: id, nome, età ed e-mail. I tipi di campo vengono specificati utilizzando le annotazioni di tipo e Pydantic convaliderà automaticamente i dati per garantire che siano sempre del tipo corretto.
È quindi possibile creare istanze di questo modello passando i dati come argomenti:
user = User(id=1, name='Alice', age=30, email='alice@example.com', password='secret')
Pydantic convaliderà i dati e genererà un errore in caso di problemi, come campi mancanti o non validi.
Possiamo anche definire vincoli e valori predefiniti per i tuoi campi utilizzando i vincoli e gli argomenti predefiniti:
In questo caso, i campi età e password sono facoltativi e non hanno un valore predefinito, mentre il campo created_at è facoltativo e ha un valore predefinito None.
Infine, possiamo serializzare e deserializzare i modelli di dati utilizzando il metodo .dict():
data = user.dict() # Serializza usando un dizionario
print(data)
# {'id': 1, 'name': 'Alice', 'age': 30, 'email': 'alice@example.com
Puoi anche serializzare e deserializzare da e verso altri formati, come JSON, YAML e XML, utilizzando rispettivamente i metodi .json(), .yaml() e .xml().
Pydantic nel machine learning e data science
Pydantic ha anche alcuni casi d'uso interessanti nell'apprendimento automatico e può aiutarci a costruire sistemi più robusti e affidabili.
Ecco alcuni casi d'uso.
Caso 1: convalida dei dati di input
Un caso d'uso comune di Pydantic nel machine learning è la convalida dei dati di input prima che vengano inseriti nel modello.
Ciò è particolarmente importante quando i dati provengono da una fonte esterna, come un utente o un'API di terze parti. Convalidando i dati, possiamo garantire che soddisfi le aspettative del nostro modello e che sia privo di errori o incoerenze.
Per illustrare ciò, consideriamo un semplice esempio di un modello di machine learning che prende come input un elenco di numeri interi e ne restituisce la somma. Possiamo usare Pydantic per definire uno schema di convalida (parlo di schemi in ML in questo articolo) per i dati di input come segue:
class InputData(BaseModel):
numbers: List[int]
Questo schema specifica che i dati di input devono essere un elenco di numeri interi.
Ora, ogni volta che riceviamo un nuovo set di dati di input, possiamo convalidarlo rispetto a questo schema utilizzando il modello
InputData:
Se i dati di input sono validi,
validated_data conterrà gli stessi dati di
input_data. Se non è valido, ad esempio se contiene una stringa anziché un numero intero, Pydantic solleverà un errore.
Questo ci consente di rilevare eventuali problemi con i dati di input prima che possano causare problemi nel nostro modello.
Caso 2: definizione delle configurazioni del modello
Un altro caso d'uso di Pydantic nell'apprendimento automatico è la definizione delle configurazioni del modello.
Spesso i modelli di machine learning hanno una serie di iperparametri che possono essere regolati per migliorarne le prestazioni. Questi iperparametri possono essere specificati come un oggetto di configurazione, che può essere convalidato utilizzando Pydantic.
Ad esempio, consideriamo un modello di machine learning come
RandomForestClassifier che ha un iperparametro chiamato
n_estimators. Possiamo definire un modello Pydantic per la configurazione in questo modo:
class RFConfig(BaseModel):
n_estimators: int = 10 # default 10 estimators
Questo schema specifica che l'iperparametro
n_estimators deve essere un numero intero con un valore predefinito di 10.
Ora, ogni volta che vogliamo creare una nuova istanza del modello con una configurazione diversa, possiamo farlo creando un oggetto RFConfig e passandogli i valori desiderati:
config = RFConfig(n_estimators=20)
model = ensemble.RandomForestClassifier(**config.dict())
model.get_params()
>> {'bootstrap': True,
'ccp_alpha': 0.0,
'class_weight': None,
'criterion': 'gini',
'max_depth': None,
'max_features': 'sqrt',
'max_leaf_nodes': None,
'max_samples': None,
'min_impurity_decrease': 0.0,
'min_samples_leaf': 1,
'min_samples_split': 2,
'min_weight_fraction_leaf': 0.0,
'n_estimators': 20, # il nostro iperparametro
'n_jobs': None,
'oob_score': False,
'random_state': None,
'verbose': 0,
'warm_start': False}
L'utilizzo di Pydantic per definire e convalidare la configurazione del modello presenta quindi diversi vantaggi.
Assicura che gli iperparametri siano sempre impostati su valori validi, il che può aiutare a prevenire problemi che potrebbero derivare da configurazioni non valide.
Inoltre, semplifica la specifica dei valori predefiniti per gli iperparametri, che possono essere utili se si desidera utilizzare la stessa configurazione per più modelli.
Nel complesso, l'utilizzo di Pydantic per gestire le configurazioni dei modelli è un modo conveniente e affidabile per garantire che i nostri modelli di machine learning siano configurati correttamente e possano funzionare al massimo delle loro potenzialità.
Conclusione
In conclusione, Pydantic è uno strumento potente e versatile per la modellazione dei dati in Python.
La sua semplicità, efficienza, flessibilità e compatibilità lo rendono una scelta eccellente per qualsiasi progetto basato sui dati, in particolare i progetti di machine learning, e la sua comunità attiva e il supporto rendono facile trovare aiuto e risorse ogni volta che ne hai bisogno.
Se stai cercando di migliorare la qualità e l'affidabilità dei tuoi modelli di dati, vale sicuramente la pena considerare Pydantic.
Commenti dalla community"
https://www.diariodiunanalista.it/posts/perche-avere-un-grosso-numero-di-feature-puo-peggiorare-il-tuo-modello/,"Un lavoro di analisi dati inizia sempre data un dataset. Questo può essere stato consegnato dal cliente, trovato pubblicamente su siti come Kaggle.com oppure creato da noi e il nostro team.
In qualsiasi di questi casi, il dataset mostrerà una anatomia che varierà in base al tipo di fenomeno che vuole descrivere, e avrà un certo numero di colonne che comporranno tale struttura.
Durante lo sviluppo del nostro progetto, il team sarà interessato a diversi aspetti di questo dataset:
- Quanto i dati contenuti in esso sono rappresentativi del fenomeno che deve descrivere?
- Quali sono i data type delle nostre colonne?
- Quante righe e quante colonne sono presenti?
E molte altre. Diventa importante rispondere a queste domande perché contribuiscono a definire l'ambito di indagine del nostro team. Ci aiutano a capire quanta EDA (exploratory data analysis, analisi esplorativa del dato) fare, cosa possiamo predire e in che modo farlo (con il machine learning oppure con altri metodi statistici) e a strutturare un piano di preprocessing del dato.
Ho toccato alcuni di questi temi in articoli dedicati. Puoi leggerli qui
- Costruire il proprio dataset: vantaggi, approccio e strumenti
- Analisi esplorativa dei dati con Python e Pandas
- Come strutturare un progetto di machine learning
Una pipeline di analisi passa solitamente attraverso diversi step, e uno di questi prende il nome di feature engineering (ingegnerizzazione delle feature).
Durante questa fase, l'analista modifica, trasforma e aggiunge le colonne (anche dette feature, dimensioni, variabili e così via) presenti nel dataset con lo scopo di arricchire le informazioni riguardo al fenomeno che viene descritto. Solitamente questo viene fatto in vista di una fase di modellazione del fenomeno stesso.
Ad esempio, se stiamo studiando una serie temporale, una attività di feature engineering può essere quella di creare delle colonne addizionali partendo dalla data presente all'interno della serie per estrarre anno, mese, giorno, giorni appartenenti al weekend e così via.
L'ipotesi è che la variabile target (se stiamo parlando di apprendimento supervisionato) può essere modellata meglio se andiamo ad aggiungere più informazioni - in questo caso stiamo andando ad arricchire la serie andando ad ingegnerizzare le feature temporali.
Ma a volte questo processo può creare problemi invece che risolverne, e può essere difficile riconoscere le cause di tali problematiche. Queste portano, spesso e volentieri, ad overfittare il training set.
Ho già scritto di overfitting e del perché sia uno dei maggiori ostacoli nel machine learning. Vi consiglio di dare una lettura a questo articolo se vi interessa scoprire di più al riguardo.
In questo articolo scriverò nello specifico delle problematiche che possono sorgere quando ci si spinge troppo in là con il feature engineering, quindi quando si aggiungono troppe informazioni.
Perché fare feature engineering?
Il motivo che ci spinge a fare feature engineering è quello di aumentare la disponibilità di informazioni di valore e addestrare il modello includendo questa ""prospettiva"" nuova sui dati.
L'interpretazione di questa attività è, giustamente, corretta. Voler aggiungere informazione è generalmente una attività da perseguire, ma solo se si ha certezza che il dato sia in realtà veramente utile al modello.
Come spesso scrivo e pubblico sul web, la nostra prima preoccupazione dovrebbe quella di raccogliere dati di alta qualità, possibilmente quanti più possibili, in modo da poter rappresentare al meglio il fenomeno che vogliamo studiare e modellare.
Per ""alta qualità"" intendo dati che contengono esempi quanto più vicini alla realtà osservabile di un evento del mondo.
Vogliamo essere sicuri che le risposte alle nostre domande siano contenute nel nostro dataset.
Fare feature engineering, quindi, può aiutarci nel rispondere a tali domande in una fase di analisi oppure aiutare il nostro modello a trovare una via più facile nel predire il target.
Perché è un problema avere un grosso numero di feature?
L'analista incauto però potrebbe incorrere in problemi se inserisce troppe variabili nuove nel suo dataset.
Alcune di queste potrebbero effettivamente essere utili, altre potrebbero addirittura ostacolare il nostro modello alla generalizzazione.
Un modello che riceve delle informazioni potenzialmente irrilevanti farà più difficoltà a generalizzare correttamente il fenomeno.
Formalizzando, l'analista potrebbe commettere degli errori se aggiunge troppe colonne al suo dataset, quali
- aggiungere informazioni non rilevanti, aumentando il rapporto segnale / rumore (signal-to-noise ratio)
- aumentare la complessità del fenomeno da mappare
- inserire dei confound nel dataset
- avere più colonne che righe
Ognuno di questi aspetti può influenzare negativamente l'abilità del nostro modello a dare risultati usabili. Vediamo come.
Aggiungere informazioni non rilevanti (aumentare il rapporto segnale-rumore)
Più gli esempi nel nostro dataset sono rappresentativi del fenomeno, più il rapporto segnale-rumore sarà alto. Vogliamo sempre massimizzare questo rapporto in modo tale da poter allineare quanto più possibile il campione con la popolazione di dati raccolti.
Ogni esempio nel nostro dataset dovrebbe essere descritto da certe feature che lo rendono rappresentativo del fenomeno che vogliamo studiare.
Se questo non avviene, rendiamo tale esempio essenzialmente inutile. Questa inutilità non viene però filtrata dal nostro modello, che cercherà di apprendere il modo migliore di mappare \( X \) a \( y \) usando queste informazioni rumorose. Questo deteriora le performance del modello.
Aumentare la complessità del fenomeno da mappare
Più complesso è il fenomeno che vogliamo modellare, più sarà difficile per il modello trovare una funzione che descriva il suo comportamento.
Cosa si intende per complessità? In questo caso si intende il numero di colonne. Ogni colonna descrive qualcosa del fenomeno, e una descrizione più ricca cresce naturalmente di complessità.
A volte un fenomeno è intrinsecamente complesso da modellare (come ad esempio fare previsioni meteorologiche) - in questo caso dobbiamo essere abili a comprendere quanto le colonne che aggiungiamo vadano a complicare ulteriormente il problema.
Inserire dei confound
Un confound è una variabile che ""confonde"" il modello. Variabili correlate tra di loro, ma che non sono realmente impattanti (correlazioni spurie e simili) possono confondere il modello in fase di training.
In questo caso, il modello impara erroneamente che la variabile \( K \) esercita un grosso impatto sul target - quando poi quest'ultimo viene deployato in produzione le performance risultano lontane da quelle viste in training.
Anche in questo caso, la soluzione è la stessa: studiare attentamente la natura della variabile aggiunta al dataset, ragionando su come questa potrebbe effettivamente aiutare il modello.
Avere più colonne che righe
Questo è uno scenario un po' limite, ma può accadere se l'analista utilizza librerie esterne per fare feature engineering. Ad esempio, se si vuole fare FE su una serie temporale formata da candele finanziarie, delle librerie di analisi tecnica andrebbero ad applicare centinaia di colonne in più al dataset.
Ragiono in questo modo quando si parla di valutare numero di colonne vs numero di righe
Colonne: cose da apprendere
Righe: esempi dai quali apprendere
Tendenzialmente, il mio consiglio è quello di evitare sempre lo scenario dove abbiamo più colonne che righe.
Come identificare le feature più importanti?
Abbiamo visto come avere un processo troppo liberale di feature engineering possa arrecare danno al nostro modello.
Il processo per identificare invece quali siano le feature da includere, poiché importanti, nel nostro dataset si chiama feature selection.
La selezione delle feature ci aiuta a isolare le variabili che contribuiscono di più alla performance del modello.
Ci sono vari approcci alla feature selection. Questo articolo non andrà in dettaglio in questo tema, ma posso linkarvi un pezzo su Boruta, una libreria per la feature selection in Python. Questa libreria è facile da usare e permette di calcolare l'importanza di ogni feature nel dataset. Una feature importante è sicuramente da includere nel training set, mentre alcune potrebbero essere addirittura rimosse o trasformate.
Conclusione
L'ingegnerizzazione delle feature è sicuramente una attività alla quale dobbiamo sempre pensare, ma con una attenzione a non strafare.
È facile pensare che una certa variabile possa avere un impatto positivo delle performance del modello - il modo migliore per testare una ipotesi è proprio quella di sperimentare. Boruta aiuta proprio a questo: all'inserimento di una nuova variabile possiamo usare questa libreria (insieme ad altri approcci) per stimare la rilevanza di tale feature sui risultati.
Qualora non volessimo studiare le feature con un processo di selezione, il consiglio è quello di andare ad evitare gli ostacoli menzionati e di valutare come le performance del modello cambino all'inserimento graduale di più variabili.
Commenti dalla community"
https://www.diariodiunanalista.it/posts/perche-il-tuo-progetto-di-machine-learning-potrebbe-fallire/,"Quando ho iniziato la mia carriera da analista, credevo che la parte più difficile di un progetto fosse fare il lavoro vero e proprio. Mi sbagliavo. La maggior parte dei progetti di machine learning e analisi avanzata fallisce o non va in produzione. Le cause di questi fallimenti spaziano da quelle organizzative a quelle operative. Tratteremo entrambi in questo articolo.
Seppur la capacità di eseguire il lavoro pratico è fondamentale per il successo del progetto, un aspetto spesso trascurato ha a che fare con la comunicazione con il cliente e la comprensione delle aspettative.
Come persone dal background prevalentemente tecnico, ci concentriamo spesso sul lavoro effettivo invece di curare la conversazione da uomo a uomo: deleghiamo (quando possibile) le e-mail, le riunioni e le conversazioni con i clienti a colleghi che ricoprono ruoli specifici come account e project manager.
Questa formula viene utilizzata perché funziona nella maggior parte dei campi, ma oserei dire che non funziona molto bene data science nel machine learning per diversi motivi.
Scriverò del perché la comunicazione e la definizione delle aspettative sono così importanti e come possono gettare le basi per il fallimento fin dall'inizio se mal considerate. Toccheremo argomenti di project management e approfondiremo alcuni punti specifici che sono cruciali per l'esito di un progetto di data science o ML.
Come menzionato, la maggior parte dei problemi si dividono in due categorie: problemi organizzativi e problemi operativi.
Gli aspetti organizzativi hanno a che fare con il modo in cui le persone gestiscono i propri progetti, il team, la comunicazione, il flusso di lavoro e altro ancora. Ha a che fare con le persone, non direttamente con l'expertise tecnica.
I problemi operativi hanno a che fare con il modo in cui le persone fanno le cose. È il codice scritto male, sono i commenti mancanti, l'incapacità di comprendere le metriche e le performance di un modello.
Alcuni problemi non rientrano in categorie distinte, ma possono anche rientrare in entrambe.
La gestione del progetto dev'essere lasciata ad un esperto di dati
Capire quale tipo di soluzione è la migliore per un problema specifico è la chiave per fornire un lavoro che soddisfi il cliente. Chi parla con il cliente deve essere in grado di fornire informazioni chiare e comprensibili sulla soluzione per risolvere le esigenze di quest'ultimo. Questa persona deve essere in grado di ricevere il brief del cliente, porre le domande corrette e parlare con il team di data science per comprendere e inquadrare il problema.
Il più delle volte, una persona che ha solo una visione di alto livello del campo non può porre le domande giuste o inquadrare correttamente il problema. Ciò potrebbe portare a problemi lungo la strada che possono avere un impatto sulla consegna finale o sulla fiducia del cliente, come la consegna di un modello inadeguato o il mancato rispetto delle scadenze. Per questo motivo, la comunicazione e gli aspetti gestionali di un progetto dovrebbero essere svolte da qualcuno che abbia conoscenze avanzate di data science.
Conosci il tuo cliente
Una persona esperta di dati che gestisce il progetto dovrebbe sempre investire tempo e conoscere il cliente a un livello più profondo. Poiché veniamo pagati, spesso ci comportiamo come se il cliente avesse sempre ragione e capisse il problema che sta cercando di risolvere. Questo è un errore grave.
Il cliente si rivolge a noi perché non ha idea di come risolvere il suo problema. Sta a noi capire cosa stanno cercando di dire, cosa stanno vivendo e come risolverlo, e spesso il cliente non lo comunicherà nel modo più chiaro perché non è un tecnico o oppure abituato a lavorare con i dati.
Chi si occupa della comunicazione è un traduttore di intenzioni e aspettative.
Mancanza di expertise
Data science e machine learning sono discipline relativamente nuove. Mentre molti stanno costruendo conoscenze e nuove tecnologie, lo stanno facendo testando e imparando usando i propri metodi. Non esiste ancora uno standard di settore e non lo sarà ancora per molto tempo.
La maggior parte delle persone oggi ha una conoscenza superficiale di argomenti fondamentali come l'algebra lineare, l'analisi matematica e gli algoritmi. Una percentuale molto ridotta di persone che studia questi concetti avanzati ha effettivamente esperienza pratica sul posto di lavoro. Questo costringe le aziende ad “arrangiarsi” su problemi che potrebbero essere risolti con soluzioni più efficienti.
Questo non si applica solo ai data scientist. L'intero ciclo di vita del progetto dipende da più team che interagiscono tra loro, ad esempio il team di data science con il team di software engineering. La maggior parte dei programmatori ha poca conoscenza di come è strutturato un progetto di machine learning e questo può avere un impatto negativo sul successo della tua campagna.
Cattiva comunicazione tra colleghi e mancanza di collaborazione
Spesso il problema risiede nell'incapacità degli analisti di comunicare i risultati al top management. Questo è usuale in aziende non specializzate nell'analisi dei dati che si limitano a coprire il servizio come un addendum ai loro altri servizi.
Idealmente, l'analista medio dovrebbe avere un certo grado di competenza nello storytelling e nella visualizzazione dei dati, mentre gli stakeholder dovrebbero avere un certo grado di conoscenza tecnica.
Un altro problema che potrebbe avere un impatto sulla consegna è la leadership del team. I progetti dovrebbero essere guidati da persone con esperienza in quella nicchia. Ad esempio, se un data scientist senior ha esperienza nell'industria alimentare, ha senso che guidi un progetto che ha come cliente un proprietario di una catena di ristoranti.
La mancanza di una buona leadership può essere devastante e ferisce il team e i suoi membri su più livelli:
- obiettivi poco chiari portano ad aspettative poco chiare
- spreco di risorse a causa di una sperimentazione inconcludente
- percezione negativa della squadra, sia dall'interno che dall'esterno
- il dubbio pervade le menti dei membri
Bisogna dare la priorità ad avere una persona valida alla guida del team squadra.
Infrastruttura dati mancante
Noi data scientist abbiamo un grosso problema: non possiamo lavorare senza dati. È ancora peggio quando ci vengono forniti dati poco usabili dai nostri clienti. Come si può immaginare, una cattiva infrastruttura dati è spesso il vero colpevole dell'incapacità del data scientist di fornire un modello utilizzabile.
Se possediamo una società di consulenza di data science, assicuriamoci di poter lavorare con i dati del cliente prima di accettare il lavoro.
Progetti tecnicamente irrealizzabili
Ci sono alcuni progetti che a volte non si possono completare. Se il tuo commerciale vende servizi che non possono essere coperti adeguatamente, allora c'è un grosso problema nella tua azienda. Ecco perché hai bisogno di persone esperte di dati nel tuo team, a partire proprio dal team sales.
Quando questo accade significa che c'è una mancanza di conoscenza e allineamento tra i team, nonché l'incapacità di coprire servizi specifici. ""Machine Learning"" e ""Data Science"" sono parole d'ordine in questo momento: non lasciare che sia il cliente a decidere quale sia la soluzione adatta al suo problema. Se lo sapessero, non avrebbero il problema in primo luogo.
KPI non realistici, diagrammi di Gantt fatti a caso e promesse non realistiche rompono le relazioni con i clienti e sprecano risorse, il tutto a spese della tua azienda.
Conclusione
Per ricapitolare, ecco un elenco (incompleto) di eventi che potrebbero interrompere il tuo progetto di machine learning e portare ad uno spreco di risorse
- chi gestisce il progetto non è esperto di data science e non può comunicare in modo efficiente con il cliente
- il cliente viene preso troppo sul serio (o il contrario): ricorda sempre che sei l'esperto e il cliente non ha la più pallida idea di come risolvere il suo problema. Se lo sapessero non ci avrebbero interpellato in primo luogo
- Mancanza di expertise nel team
- Cattiva comunicazione e incapacità di collaborare in modo efficiente tra e all'interno dei team
- Il cliente non ha dati o la sua infrastruttura è mal gestita
- accettare progetti tecnicamente irrealizzabili
Commenti dalla community"
https://www.diariodiunanalista.it/posts/personalizzare-le-pipeline-di-sklearn-transformermixin/,"Una delle funzionalità più usate e apprezzate di Scikit-Learn sono le pipeline. Sebbene il loro utilizzo sia optional, quest'ultime possono essere utilizzate per rendere il nostro codice più pulito e facile da mantenere.
Le pipeline accettano come input degli estimators, che non sono altro che delle classi che si ereditano da sklearn.base.BaseEstimator e che contengono i metodi fit e transform. Questo ci permette di personalizzare le pipeline con funzionalità che Sklearn non offre di default.
Parleremo dei transformer, degli oggetti che applicano una trasformazione su un input. La classe dalla quale erediteremo è
TransformerMixin, ma è possibile estendere anche da
ClassifierMixin ,
RegressionMixin,
ClusterMixin e altri per creare un estimator personalizzato. Leggere qui per tutte le opzioni disponibili.
Lavoreremo con un dataset di dati testuali, sulla quale vogliamo applicare delle trasformazioni quali:
- preprocessing del testo
- vettorizzazione con TF-IDF
- creazione di feature aggiuntive a scopo di esempio come sentiment, numero di caratteri, numero di frasi
Questo verrà fatto attraverso l'utilizzo di Pipeline e FeatureUnion, una classe di Sklearn che unisce i feature set provenienti da diverse sorgenti.
- Importare un dataset da Sklearn
- Creare feature testuali e numeriche
- Usare BaseEstimator, TransformerMixin e FeatureUnion per creare una pipeline personalizzata di feature engineering
Alla fine del progetto avrai a disposizione del codice da poter riutilizzare nel tuo progetto, qualsiasi esso sia.
Iniziamo subito!
Ti piace quello che stai leggendo?
Iscriviti al blog per rimanere sempre aggiornato sui contenuti di settore. No pubblicità. No spam.
Solo contenuto utile per la tua carriera nel ML e DS.
Puoi rimuovere l'iscrizione quando vuoi seguendo il link nella email.
Il Dataset
Useremo il dataset fornito da Sklearn, 20newsgroups, per avere rapido accesso ad un corpus di dati testuali. A scopo dimostrativo, userò solo un campione di 10 testi ma l'esempio può essere esteso a qualsiasi numero di testi.
Importiamo il dataset con Python
# librerie essenziali per il nostro esempio
import numpy as np
import pandas as pd
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import FeatureUnion, Pipeline
import nltk
from nltk.corpus import stopwords
from nltk.sentiment.vader import SentimentIntensityAnalyzer
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('vader_lexicon')
import re
# categorie usate per estrarre i testi da 20newsgroups
categories = [
'comp.graphics',
'comp.os.ms-windows.misc',
'rec.sport.baseball',
'rec.sport.hockey',
'alt.atheism',
'soc.religion.christian',
]
dataset = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, remove=('headers', 'footers', 'quotes'))
df = pd.DataFrame(dataset.data, columns=[""corpus""]).sample(10) # <-- prendiamo solo 10 elementi dal corpus
Creazione di feature testuali
Costruiremo il feature set conterrà queste informazioni
- vettorizzazione con TF-IDF dopo aver applicato preprocessing
- sentiment con NLTK.Vader
- numero di caratteri nel testo
- numero di frasi nel testo
Il nostro processo, senza usare pipeline, sarebbe quello di applicare sequenzialmente tutti questi step con blocchi di codice separato. La bellezza delle pipeline è che la sequenzialità viene mantenuta in un solo blocco di codice - la pipeline stessa diventa un estimator, in grado di eseguire tutte le operazioni programmate in una sola istruzione.
Creiamo le nostre funzioni
def preprocess_text(text: str, remove_stopwords: bool) -> str:
""""""Funzione che pulisce il testo in input andando a
- rimuovere i link
- rimuovere i caratteri speciali
- rimuovere i numeri
- rimuovere le stopword
- trasformare in minuscolo
- rimuovere spazi bianchi eccessivi
Argomenti:
text (str): testo da pulire
remove_stopwords (bool): rimuovere o meno le stopword
Restituisce:
str: testo pulito
""""""
# rimuovi link
text = re.sub(r""http\S+"", """", text)
# rimuovi numeri e caratteri speciali
text = re.sub(""[^A-Za-z]+"", "" "", text)
# rimuovere le stopword
if remove_stopwords:
# 1. crea token
tokens = nltk.word_tokenize(text)
# 2. controlla se è una stopword
tokens = [w for w in tokens if not w.lower() in stopwords.words(""english"")]
# 3. unisci tutti i token
text = "" "".join(tokens)
# restituisci il testo pulito, senza spazi eccessivi, in minuscolo
text = text.lower().strip()
return text
def get_sentiment(text: str):
""""""
Funzione che usa NLTK.Vader per estrarre il sentiment.
Il sentiment è un punteggio esprime quanto un testo sia positivo o negativo.
Il valore va da -1 a 1, dove 1 è il valore più positivo.
Argomenti:
text (str): testo da analizzare
Restituisce:
sentiment (float): polarità del testo
""""""
vader = SentimentIntensityAnalyzer()
return vader.polarity_scores(text)['compound']
def get_nchars(text: str):
""""""
Funzione che restituisce il numero di caratteri in un testo.
Argomenti:
text (str): testo da analizzare
Restituisce:
n_chars (int): numero di caratteri
""""""
return len(text)
def get_nsentences(text: str):
""""""
Funzione che restituisce il numero di frasi in un testo.
Argomenti:
text (str): testo da analizzare
Restituisce:
n_sentences (int): numero di frasi
""""""
return len(text.split("".""))
Il nostro obiettivo è quello di creare un feature set unico in modo da addestrare un modello su un qualche task. Useremo le Pipeline e FeatureUnion per mettere insieme le nostre matrici.
Come unire le feature provenienti da sorgenti diverse
La vettorizzazione TF-IDF creerà una matrice sparsa che avrà dimensioni \( n\_documenti\_nel\_corpus \times n\_features \), il sentiment sarà un singolo numero, come anche l'output di n_chars e n_sentences. Andremo a prendere gli output di ognuno di questi step e a creare una matrice singola che li conterrà tutti, in modo da poter addestrare un modello su tutte le feature che abbiamo ingegnerizzato. Partiremo da una rappresentazione del genere
Fino a giungere a questo
Il feature set verrà usato come vettore di addestramento del nostro modello.
Classi che ereditano da BaseEstimator e TransformerMixin
Per poter mettere giù il nostro processo, occorre definire le classi e cosa faranno nella pipeline. Iniziamo col creare un DummyEstimator, dal quale andremo ad ereditare init, fit e transform. Il DummyEstimator è una classe comoda che ci evita la scrittura di diverso codice.
class DummyTransformer(BaseEstimator, TransformerMixin):
""""""
Classe ""fantoccio"" - ci permette di modificare solo i metodi che ci interessano,
evitando riscritture.
""""""
def __init__(self):
return None
def fit(self, X=None, y=None):
return self
def transform(self, X=None):
return self
DummyEstimator sarà ereditato da quattro classi,
Preprocessor,
SentimentAnalysis,
NChars,
NSentences e
FromSparseToArray.
class Preprocessor(DummyTransformer):
""""""
Classe che si occupa del preprocessing del testo
""""""
def __init__(self, remove_stopwords: bool):
self.remove_stopwords = remove_stopwords
return None
def transform(self, X=None):
preprocessed = X.apply(lambda x: preprocess_text(x, self.remove_stopwords)).values
return preprocessed
class SentimentAnalysis(DummyTransformer):
""""""
Classe che si occupa di generare il sentiment
""""""
def transform(self, X=None):
sentiment = X.apply(lambda x: get_sentiment(x)).values
return sentiment.reshape(-1, 1) # <-- da notare il reshape per trasformare un vettore riga in uno colonna
class NChars(DummyTransformer):
""""""
Classe che si occupa di contare i caratteri in un testo
""""""
def transform(self, X=None):
n_chars = X.apply(lambda x: get_nchars(x)).values
return n_chars.reshape(-1, 1)
class NSententences(DummyTransformer):
""""""
Classe che si occupa di contare le frasi in un testo
""""""
def transform(self, X=None):
n_sentences = X.apply(lambda x: get_nsentences(x)).values
return n_sentences.reshape(-1, 1)
class FromSparseToArray(DummyTransformer):
""""""
Classe che si occupa trasformare una matrice sparsa in un array numpy
""""""
def transform(self, X=None):
arr = X.toarray()
return arr
Com'è possibile vedere, DummyEstimator ci permette di definire solo la funzione transform, poiché ogni altra classe eredita init e fit proprio da
DummyEstimator.
Vediamo ora come implementare la pipeline di vettorizzazione, che terrà conto del preprocessing dei nostri testi.
vectorization_pipeline = Pipeline(steps=[
('preprocess', Preprocessor(remove_stopwords=True)), # il primo step della pipeline è di preprocessare il corpus
('tfidf_vectorization', TfidfVectorizer()), # il secondo step vettorizza il testo preparato dallo step 1
('arr', FromSparseToArray()), # il terzo step converte una matrice sparsa in un array numpy per poterlo mostrare in un dataframe
])
Non resta che applicare FeatureUnion per mettere insieme i pezzi
features = [
('vectorization', vectorization_pipeline),
('sentiment', SentimentAnalysis()),
('n_chars', NChars()),
('n_sentences', NSententences())
]
combined = FeatureUnion(features) # qui è dove mettiamo insieme le nostre feature
combined
Applichiamo fit_transform sul nostro corpus e vediamo l'output
L'output sembra essere corretto! Non è molto chiaro, però. Concludiamo il tutorial con l'inserimento in dataframe del feature set combinato
# qui puntiamo al secondo step del secondo oggetto nella vectorization_pipeline per reperire i termini generati dal tf-idf
# ai quali poi andiamo ad aggiungere le altre tre colonne
cols = vectorization_pipeline.steps[1][1].get_feature_names() + [""sentiment"", ""n_chars"", ""n_sentences""]
features_df = pd.DataFrame(combined.transform(df['corpus']), columns=cols)
Il risultato è il seguente (qui ho troncato il risultato per questioni di leggibilità)
Ora abbiamo un dataset pronto per essere fornito a qualsiasi modello per addestramento. Sarebbe utile sperimentare con StandardScaler o simili e normalizzare n_chars e n_sentences. Lascerò questo esercizio al lettore.
Codice
# librerie essenziali per il nostro esempio
import numpy as np
import pandas as pd
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import FeatureUnion, Pipeline
import nltk
from nltk.corpus import stopwords
from nltk.sentiment.vader import SentimentIntensityAnalyzer
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('vader_lexicon')
import re
# categorie usate per estrarre i testi da 20newsgroups
categories = [
'comp.graphics',
'comp.os.ms-windows.misc',
'rec.sport.baseball',
'rec.sport.hockey',
'alt.atheism',
'soc.religion.christian',
]
dataset = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, remove=('headers', 'footers', 'quotes'))
df = pd.DataFrame(dataset.data, columns=[""corpus""]).sample(10) # <-- prendiamo solo 10 elementi dal corpus
vectorization_pipeline = Pipeline(steps=[
('preprocess', Preprocessor(remove_stopwords=True)), # il primo step della pipeline è di preprocessare il corpus
('tfidf_vectorization', TfidfVectorizer()), # il secondo step vettorizza il testo preparato dallo step 1
('arr', FromSparseToArray()), # il terzo step converte una matrice sparsa in un array numpy per poterlo mostrare in un dataframe
])
features = [
('vectorization', vectorization_pipeline),
('sentiment', SentimentAnalysis()),
('n_chars', NChars()),
('n_sentences', NSententences())
]
combined = FeatureUnion(features) # qui è dove mettiamo insieme le nostre feature
# qui puntiamo al secondo step del secondo oggetto nella vectorization_pipeline per reperire i termini generati dal tf-idf
# ai quali poi andiamo ad aggiungere le altre tre colonne
cols = vectorization_pipeline.steps[1][1].get_feature_names() + [""sentiment"", ""n_chars"", ""n_sentences""]
features_df = pd.DataFrame(combined.transform(df['corpus']), columns=cols)
class DummyTransformer(BaseEstimator, TransformerMixin):
""""""
Classe ""fantoccio"" - ci permette di modificare solo i metodi che ci interessano,
evitando riscritture.
""""""
def __init__(self):
return None
def fit(self, X=None, y=None):
return self
def transform(self, X=None):
return self
class Preprocessor(DummyTransformer):
""""""
Classe che si occupa del preprocessing del testo
""""""
def __init__(self, remove_stopwords: bool):
self.remove_stopwords = remove_stopwords
return None
def transform(self, X=None):
preprocessed = X.apply(lambda x: preprocess_text(x, self.remove_stopwords)).values
return preprocessed
class SentimentAnalysis(DummyTransformer):
""""""
Classe che si occupa di generare il sentiment
""""""
def transform(self, X=None):
sentiment = X.apply(lambda x: get_sentiment(x)).values
return sentiment.reshape(-1, 1) # <-- da notare il reshape per trasformare un vettore riga in uno colonna
class NChars(DummyTransformer):
""""""
Classe che si occupa di contare i caratteri in un testo
""""""
def transform(self, X=None):
n_chars = X.apply(lambda x: get_nchars(x)).values
return n_chars.reshape(-1, 1)
class NSententences(DummyTransformer):
""""""
Classe che si occupa di contare le frasi in un testo
""""""
def transform(self, X=None):
n_sentences = X.apply(lambda x: get_nsentences(x)).values
return n_sentences.reshape(-1, 1)
class FromSparseToArray(DummyTransformer):
""""""
Classe che si occupa trasformare una matrice sparsa in un array numpy
""""""
def transform(self, X=None):
arr = X.toarray()
return arr
view raw
def preprocess_text(text: str, remove_stopwords: bool) -> str:
""""""Funzione che pulisce il testo in input andando a
- rimuovere i link
- rimuovere i caratteri speciali
- rimuovere i numeri
- rimuovere le stopword
- trasformare in minuscolo
- rimuovere spazi bianchi eccessivi
Argomenti:
text (str): testo da pulire
remove_stopwords (bool): rimuovere o meno le stopword
Restituisce:
str: testo pulito
""""""
# rimuovi link
text = re.sub(r""http\S+"", """", text)
# rimuovi numeri e caratteri speciali
text = re.sub(""[^A-Za-z]+"", "" "", text)
# rimuovere le stopword
if remove_stopwords:
# 1. crea token
tokens = nltk.word_tokenize(text)
# 2. controlla se è una stopword
tokens = [w for w in tokens if not w.lower() in stopwords.words(""english"")]
# 3. unisci tutti i token
text = "" "".join(tokens)
# restituisci il testo pulito, senza spazi eccessivi, in minuscolo
text = text.lower().strip()
return text
def get_sentiment(text: str):
""""""
Funzione che usa NLTK.Vader per estrarre il sentiment.
Il sentiment è un punteggio esprime quanto un testo sia positivo o negativo.
Il valore va da -1 a 1, dove 1 è il valore più positivo.
Argomenti:
text (str): testo da analizzare
Restituisce:
sentiment (float): polarità del testo
""""""
vader = SentimentIntensityAnalyzer()
return vader.polarity_scores(text)['compound']
def get_nchars(text: str):
""""""
Funzione che restituisce il numero di caratteri in un testo.
Argomenti:
text (str): testo da analizzare
Restituisce:
n_chars (int): numero di caratteri
""""""
return len(text)
def get_nsentences(text: str):
""""""
Funzione che restituisce il numero di frasi in un testo.
Argomenti:
text (str): testo da analizzare
Restituisce:
n_sentences (int): numero di frasi
""""""
return len(text.split("".""))
Commenti dalla community"
https://www.diariodiunanalista.it/posts/pipeline-scikit-learn/,"Nel data science e machine learning, una pipeline è un insieme di step sequenziali che ci permette di controllare il flusso dei dati. Sono molto utili in quanto rendono il nostro codice pulito, scalabile e leggibile.
Esse sono usate per organizzare le varie fasi di un progetto, come il preprocessing, l'addestramento di un modello e così via. Attraverso una pipeline, infatti, possiamo compattare tutte queste azioni in un singolo oggetto rendendo così il nostro codice pulito e snello.
Anche se non è necessario implementarle nel nostro progetto, usare le pipeline ha vantaggi notevoli, quali
- clean code: scriviamo meno codice in maniera più organizzata. Questo favorisce la leggibilità e l'interpretazione dei risultati
- meno spazio per l'errore - scrivendo codice organizzato riduciamo l'errore umano che spesso si presenta in contesti ""liberi""
- con Scikit-Learn, una pipeline si addestra semplicemente come un modello canonico con
.fit().
Ecco un esempio di come si usa una pipeline con un dataset sintetico di Scikit-Learn.
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
X, y = make_classification(random_state=0)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])
# la pipeline di sklearn si usa come qualsiasi altro oggetto stimatore (estimator)
pipe.fit(X_train, y_train)
Pipeline(steps=[('scaler', StandardScaler()), ('svc', SVC())])
pipe.score(X_test, y_test)
>>> 0.88
Qui stiamo creando il nostro insieme di caratteristiche \( X \) e il nostro target \( y \) attraverso
make_classification di Scikit-Learn.
Dopodiché dividiamo \( X \) e \( y \) in train e test set usando
train_test_split sempre di Scikit-Learn. Nella pipeline poi inseriremo due step: il primo è quello che si occuperà della standardizzazione del dato (scaler) e l'altro è l'applicazione del modello SVC (Support Vector Classifier).
È possibile notare questi due step all'interno della lista nell'oggetto Pipeline. Infine basta addestrare la pipeline come faremmo con un modello qualsiasi attraverso
.fit(X_train, y_train).
La pipeline si occuperà di far ""passare""
X_train e
y_train attraverso i vari step, e di restituire un modello in grado di fare predizioni attraverso
.predict().
Un esempio pratico di utilizzo di Pipeline
Qui segue un template per l'utilizzo di Pipeline in un progetto di machine learning con task di regressione.
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from xgboost import XGBRegressor
data = pd.read_csv(""./my_data.csv"")
# isoliamo feature e target
y = data.Price
X = data.drop(""Price"", axis=1)
# creiamo train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75)
# isoliamo le variabili categoriali (se esistono)
categorical_columns = [col for col in X_train.columns if X_train[col].dtype == ""object""]
# isoliamo le variabili numeriche (se esistono)
numerical_columns = [col for col in X_train.columns if X_train[col].dtype \
in [""int64"", ""float64""]]
# definiamo gli step di preprocessing
# 1) Gestiamo i valori vuoti nelle colonne numeriche
# 2) Gestiamo i valori vuoti e applichiamo one-hot encoding nelle colonne categoriali
# Useremo ColumnTransformer di Sklearn per raggruppare gli oggetti che andranno
# a trasformare le nostre colonne
# preprocessing per i dati numerici
# in questo caso solo uno step - l'imputazione dei valori vuoti
numerical_transformer = SimpleImputer()
# preprocessing per i dati categoriali
# due step: imputazione valori vuoti, one-hot encoding
categorical_transformer = Pipeline(steps=[
(""imputer"", SimpleImputer(strategy=""most_frequent"")),
(""ohe"", OneHotEncoder(handle_unknown=""ignore""))
])
# compattiamo tutto nel ColumnTransformer
preprocessor = ColumnTransformer(
transformers=[
(""numerical"", numerical_transformer, numerical_columns),
(""categorical"", categorical_transformer, categorical_columns),
]
)
# abbiamo completato il preprocessor - ora inizializziamo un modello di regressione
regressor = XGBRegressor()
Prendiamoci un secondo per analizzare il
ColumnTransformer.
È molto semplice: questo oggetto di Sklearn ci permettere di applicare delle trasformazioni a delle colonne nel nostro dataframe o array Numpy.
Nel codice che vediamo non abbiamo fatto altro che applicare dei transformers (come il One-Hot Encoder e Simple Imputer) alle colonne numeriche e categoriali. Il ColumnTransformer è poi passato direttamente nella pipeline come uno degli step. Se volete leggere di più sul suo funzionamento, consultate qui la documentazione.
Abbiamo creato la pipeline per il preprocessing in maniera pulita e interpretabile. Ora creiamo la pipeline finale che ingloba preprocessing e il modello
XGBRegressor.
final_pipeline = Pipeline(steps=[
(""preprocessor"", preprocessor),
(""xgb"", regressor)
])
# passiamo i dati di addestramento alla pipeline
final_pipeline.fit(X_train, y_train)
# creiamo predizioni
preds = final_pipeline.predict(X_test)
# e valutiamo il modello
from sklearn.metrics import mean_squared_error
# usiamo RMSE (Root Mean Squared Error) usando mean_squared_error con squared=False
eval_metric = mean_squared_error(y_test, preds, squared=False)
print(eval_metric)
Abbiamo ora un template copia-incolla della struttura e utilizzo di una pipeline con Scikit-Learn.
Commenti dalla community"
https://www.diariodiunanalista.it/posts/preparazione-dei-dati-per-nlp-con-deep-learning/,"Questo articolo mostrerà come preparare dei testi per applicazioni basate su reti neurali.
Dato un corpus di dati testuali, applicheremo il seguente processo
- mappatura della singola parola del testo (token) ad un indice
- creazione di token specifici per fine frase, padding e simboli non appartenenti al vocabolario
- conversione dei termini presenti in dizionario in tensori
Il vocabolario organizzerà i nostri dati in formato chiave: valore dove la chiave sarà il termine e il valore sarà un indice numerico intero associato a quel termine. Saranno presenti dei token speciali che avranno queste caratteristiche
- __PAD__: indica il simbolo del padding
- </e>: indica end of line (fine della frase)
- __UNK__: indica un simbolo sconosciuto, non appartenente al dizionario
Questo lavoro sarà molto utile perché fornirà un template per creare l'input al nostro generatore di dati (data generator in gergo del deep learning) in maniera facile ed efficiente.
Il Dataset
Useremo il dataset fornito da Sklearn, 20newsgroups, per avere rapido accesso ad un corpus di dati testuali. A scopo dimostrativo, userò solo un campione di 10 testi.
import numpy as np
from sklearn.datasets import fetch_20newsgroups
# categorie dalle quali prenderemo i nostri dati
categories = [
'comp.graphics',
'comp.os.ms-windows.misc',
'rec.sport.baseball',
'rec.sport.hockey',
'alt.atheism',
'soc.religion.christian',
]
# primi 10 elementi
dataset = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, remove=('headers', 'footers', 'quotes'))
corpus = [item for item in dataset['data'][:10]]
corpus
Su questi testi applicheremo un preprocessing semplice, per pulire le frasi da stop word e caratteri speciali.
Il Processo
Funzione di preprocessing
La funzione di preprocessing è la seguente
import re
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')
def preprocess_text(text: str) -> str:
""""""Funzione che pulisce il testo in input andando a
- rimuovere i link
- rimuovere i caratteri speciali
- rimuovere i numeri
- rimuovere le stopword
- trasformare in minuscolo
- rimuovere spazi bianchi eccessivi
Argomenti:
text (str): testo da pulire
Restituisce:
str: lista di token puliti
""""""
# rimuovi link
text = re.sub(r""http\S+"", """", text)
# rimuovi numeri e caratteri speciali
text = re.sub(""[^A-Za-z]+"", "" "", text)
# rimuovere le stopword
# 1. crea token
tokens = nltk.word_tokenize(text)
# 2. controlla se è una stopword
tokens = [w.lower() for w in tokens if not w in stopwords.words(""english"")]
return tokens
Questa funzione è usabile per qualsivoglia problema di elaborazione del linguaggio naturale, non solo questo presente nell'articolo.
Creazione del vocabolario
Lavorare con un vocabolario ci permette di mappare termine ad indice. L'indice sarà l'elemento che verrà convertito a tensore.
def get_vocab(training_corpus):
# includiamo caratteri speciali aggiuntivi
# padding, fine di linea, termine sconosciuto
vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2}
# costruiremo il vocabolario solo con i dati di training
for item in training_corpus: # iteriamo nel nostro corpus
processed_text = preprocess_text(item) # applichiamo preprocessing al testo
for word in processed_text: # per ogni parola contenuto nel testo (token)
if word not in vocab: # se la parola non è presente nel dizionario
vocab[word] = len(vocab) # crea una chiave che è uguale al termine, e il suo valore è uguale alla lunghezza del vocabolario
return vocab
vocab = get_vocab(corpus)
L'output sarà il seguente
Da testo a tensori
Scriveremo una funzione che utilizzerà il dizionario per creare una rappresentazione numerica dei termini presenti in quest'ultimo. Questi numeri saranno inseriti in una lista e rappresenteranno i tensori da applicare in un task di deep learning.
Non utilizzeremo TensorFlow, PyTorch o Numpy per questo esempio, così da spiegare fondamentalmente come avviene il processo.
Partiremo dalla funzione text_to_tensor
def text_to_tensor(text: str, vocab_dict: dict, unk_token='__UNK__', verbose=False):
'''
Argomenti:
text - stringa contenente il testo
vocab_dict - il dizionario di termini
unk_token - il carattere speciale usato per identificare i termini sconosciuti
verbose - stampaggio di messaggi di debug
Restituisce:
tensor_l - una lista di indici che rappresentano numericamente il nostro testo
'''
word_l = preprocess_text(text)
if verbose:
print(""Lista delle parole presenti nel testo:"")
print(word_l)
# Inizializziamo una lista vuota che conterrà i tensori
tensor_l = []
# Prendiamo il valore di __UNK__ token
unk_ID = vocab_dict[unk_token]
if verbose:
print(f""Il valore di UNK è {unk_ID}"")
# per ogni parola nella lista:
for word in word_l:
# prendiamo il suo indice
# e se la parola non è presente nel dizionario, usiamo UNK ID
word_ID = vocab_dict.get(word, unk_ID)
# inseriamo il valore nella lista finale
tensor_l.append(word_ID)
return tensor_l
Ora possiamo procedere a usare i nostri tensori per creare un batch_generator e un modello di deep learning, come una LSTM.
Commenti dalla community"
https://www.diariodiunanalista.it/posts/previsione-delle-serie-temporali-con-lstm-in-tensorflow/,"In questo articolo vedremo come fare una previsione da una serie temporale con Tensorflow e Keras in Python. Useremo una rete neurale sequenziale creata in Tensorflow basata su strati LSTM bidirezionali per catturare i pattern nelle sequenze univariate che daremo in input al modello.
In particolare vedremo come
- generate dei dati sintetici per simulare una serie temporale con diverse caratteristiche
- processare i dati in set di addestramento e validazione e creare un dataset basato su finestre temporali
- definire una architettura per la nostra rete neurale usando LSTM (long short-term memory)
- addestrare e valutare il modello
L'obiettivo di questo tutorial è quello di predire un punto nel futuro data una sequenza di dati. Non verrà coperto il caso multi-step - quello dove si fa previsione del punto precedente che è a sua volta stato generato dal modello.
Iniziamo.
Generazione dei dati
Invece di scaricare un dataset dal web, useremo delle funzioni che ci permetteranno di generare delle serie temporali plausibili e usabili per il nostro caso. Useremo anche dataclass per memorizzare i parametri della nostra serie temporale in una classe così da poterla usare a prescindere dallo scope. Il nome della dataclass sarà
G, che sta per ""globale"".
Ho deciso di usare questo approccio invece di usare un dataset vero e proprio perché questa metodica ci permette di testare più serie temporali e avere flessibilità nel processo creativo dei nostri progetti.
def plot_series(time, series, format=""-"", start=0, end=None):
""""""Funzione helper per plottare il grafico della serie temporale""""""
plt.plot(time[start:end], series[start:end], format)
plt.xlabel(""Time"")
plt.ylabel(""Value"")
plt.grid(False)
def trend(time, slope=0):
""""""Definizione di un trend attraverso pendenza e tempo""""""
return slope * time
def seasonal_pattern(season_time):
""""""Definizione arbitraria di un pattern di stagionalità""""""
return np.where(season_time < 0.1,
np.cos(season_time * 6 * np.pi),
2 / np.exp(9 * season_time))
def seasonality(time, period, amplitude=1, phase=0):
""""""Ripete un pattern a ogni periodo""""""
season_time = ((time + phase) % period) / period
return amplitude * seasonal_pattern(season_time)
def noise(time, noise_level=1, seed=None):
""""""Aggiunge rumore bianco alla serie""""""
rnd = np.random.RandomState(seed)
return rnd.randn(len(time)) * noise_level
Generiamo una serie temporale sintetica con questo codice
def generate_time_series():
# La dimensione temporale: 4 anni di dati
time = np.arange(4 * 365 + 1, dtype=""float32"")
# La serie iniziale non è altro che una linea retta che poi andremo a modificare con le altre funzioni
y_intercept = 10
slope = 0.005
series = trend(time, slope) + y_intercept
# Aggiungiamo stagionalità
amplitude = 50
series += seasonality(time, period=365, amplitude=amplitude)
# Aggiungiamo rumore
noise_level = 3
series += noise(time, noise_level, seed=51)
return time, series
# Salviamo i parametri della nostra serie temporale
@dataclass
class G:
TIME, SERIES = generate_time_series()
SPLIT_TIME = 1100 # al giorno 1100 finirà il periodo di training. Il resto sarà set di validazione
WINDOW_SIZE = 20 # quanti giorni prenderemo in considerazione per fare la nostra previsione
BATCH_SIZE = 32 # quanti item forniremo per batch
SHUFFLE_BUFFER_SIZE = 1000 # questo parametro ci serve per definire il buffer di campionatura di Tensorflow
# plottiamo la serie sintetica
plt.figure(figsize=(10, 6))
plot_series(G.TIME, G.SERIES)
plt.show()
La serie ottenuta è la seguente
Ora che abbiamo una serie temporale usabile, passiamo al preprocessing.
Preprocessing della serie temporale per il deep learning
La particolarità delle serie temporali è che devono essere divise in set di training e validazione e che a loro volta devono essere divise in sequenze di una lunghezza definita dalla nostra configurazione. Queste sequenze si chiamano finestre (windows) e il modello userà proprio queste sequenze per produrre una previsione.
Vediamo altre due funzioni helper per raggiungere questo obiettivo.
Qui sono dovute alcune spiegazioni. La funzione
train_val_split non fa altro che dividere la nostra serie in base al valore
G.SPLIT_TIME definito precedentemente nella dataclass
G. Inoltre, gli passeremo i parametri
G.TIME e
G.SERIES.
Rammentiamo la definizione della dataclass
G.
In base
generate_time_series,
TIME = range(0, 1439)
SERIES = array([ 0.81884814, 0.82252744, 0.77998762, ..., -0.44389692, -0.42693424, -0.39230758])
Entrambi di lunghezza 1439.
La divisione di
train_val_split sarà quindi uguale a
time_train = range(0, 1100)
time_val = range(1100, 1439)
Dopo aver diviso in set di training e validazione useremo delle funzioni di Tensorflow per creare un oggetto
Dataset che permetterà di creare le feature X e il target y. Ricordiamo che X sono gli n valori che il modello userà per predire il prossimo, che sarebbe la y.
Vediamo come implementare queste funzioni
def train_val_split(time, series, time_step=G.SPLIT_TIME):
""""""Dividere la serie temporale in training e validation set""""""
time_train = time[:time_step]
series_train = series[:time_step]
time_valid = time[time_step:]
series_valid = series[time_step:]
return time_train, series_train, time_valid, series_valid
def windowed_dataset(series, window_size=G.WINDOW_SIZE, batch_size=G.BATCH_SIZE, shuffle_buffer=G.SHUFFLE_BUFFER_SIZE):
""""""
Creiamo delle finestre temporali per creare delle feature X e y.
Se ad esempio scegliamo una finestra di 30, creeremo un dataset formato da 30 punti come X
""""""
dataset = tf.data.Dataset.from_tensor_slices(series)
dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)
dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))
dataset = dataset.shuffle(shuffle_buffer)
dataset = dataset.map(lambda window: (window[:-1], window[-1]))
dataset = dataset.batch(batch_size).prefetch(1)
return dataset
Useremo il metodo
.window() di Tensorflow sull’oggetto dataset per applicare uno shift di 1 ai nostri punti. Un esempio della logica applicata può essere visto qui:
Nell’esempio creiamo un range da 0 a 10 con Tensorflow, e applichiamo una window di 5. Si creeranno quindi un totale di 5 colonne. Passando
shift=1 ogni colonna avrà un valore in meno partendo dall’alto (shiftato) e
drop_remainder=True assicurerà di avere una matrice sempre della stessa dimensione.
Applichiamo queste due funzioni.
# creiamo il dataset con finestre temporali
dataset = windowed_dataset(series_train)
# dividiamo in training e validation set
time_train, series_train, time_valid, series_valid = train_val_split(G.TIME, G.SERIES)
Ora che i dati sono pronti possiamo dedicarci a strutturare la nostra rete neurale.
Architettura della rete neurale
Come menzionato all’inizio dell’articolo, la nostra rete neurale si baserà prevalentemente su strati LSTM (long-short term memory). Quello che rende una LSTM adatta a questo tipo di compito è che possiede un struttura interna in grado di propagare le informazioni attraverso lunghe sequenze. Questo le rende molto utili nell’elaborazione del linguaggio naturale (NLP) e per l’appunto nelle serie temporali, proprio perché questi due tipi di compiti potrebbero richiedere di trasferire informazioni durante l’intera sequenza.
Nel codice che segue vedremo come gli strati LSTM siano inclusi all’interno di uno strato
Bidirectional. Questo strato permette alla LSTM di considerare la sequenza di dati in entrambe le direzioni e di poter quindi avere contesto non solo del passato ma anche del futuro. Ci aiuterà a costruire una network che sarà più precisa rispetto a una unidirezionale.
Oltre alle LSTM esistono anche le GRU (Gated Recurrent Units) che possono essere impiegate per task di predizione delle serie temporali.
Useremo anche lo strato
Lambda che ci permetterà di adattare correttamente il formato dei dati in input alla nostra rete e infine uno strato denso per calcolare l’output finale.
Vediamo come implementare tutto questo in Keras e Tensorflow in maniera sequenziale.
Lo strato Lambda permette di usare delle funzioni custom all’interno di uno strato. Lo usiamo per assicurarci che la dimensionalità dell’input sia adeguata alla LSTM. Poiché il nostro dataset è composto finestre temporali bidimensionali, dove la prima è la
batch_size e l’altra è
timesteps.
Una LSTM però accetta una terza dimensione, che indica il numero di dimensioni del nostro input. Usando la funzione Lambda e input_shape = [None], stiamo di fatto dicendo a Tensorflow di accettare qualsiasi tipo di dimensione in input. Questo è comodo perché non dobbiamo scrivere codice aggiuntivo per assicurare che la dimensionalità sia corretta.
Per leggere di più su LSTM e come funziona il layer, invito il lettore a consultare la guida ufficiale di Tensorflow presente qui.
Tutti gli strati LSTM sono inclusi all’interno di uno strato Bidirezionale, e ognuna di esse passa le sequenze elaborate al prossimo strato attraverso
return_sequences = True. Se questo argomento fosse falso, Tensorflow darebbe errore, poiché lo strato LSTM successivo non troverebbe una sequenza da processare. L’unico strato che non deve restituire le sequenze è l’ultimo LSTM, poiché lo strato denso finale è quello deputato a fornire la predizione finale e non un’altra sequenza.
La callback EarlyStopping
Come è possibile leggere nell’articolo Controllare il training di una rete neurale in Tensorflow, useremo una callback per fermare il training quando la nostra metrica di performance raggiunge un livello specificato. Useremo il MAE (Mean absolute error) per misurare quanto la nostra network sia in grado di predire correttamente il prossimo punto della serie.
Di fatto, questo è un compito simile ad un task di regressione, e quindi userà delle metriche di performance analoghe.
Vediamo il codice per implementare l’early stopping
def create_uncompiled_model():
# definiamo un modello sequenziale
model = tf.keras.models.Sequential([
tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),
input_shape=[None]),
tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(1024, return_sequences=True)),
tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, return_sequences=True)),
tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True)),
tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),
tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
tf.keras.layers.Dense(1),
])
return model
Addestramento del modello
Siamo pronti ad addestrare il nostro modello LSTM. Definiamo una funzione che andrà a chiamare
create_uncompiled_model e andrà a fornire al modello una funzione di perdita e un ottimizzatore.
La funzione di perdita di Huber può essere utilizzata per bilanciare tra l'errore medio assoluto, o MAE, e l'errore quadratico medio, MSE. È quindi una buona funzione di perdita per quando si hanno dati vari o solo pochi valori anomali, come in questo caso.
L’ottimizzatore Adam è una scelta generalmente valida - settiamo un learning_rate arbitrario di 0.001.
def create_model():
tf.random.set_seed(51)
model = create_uncompiled_model()
model.compile(loss=tf.keras.losses.Huber(),
optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
metrics=[""mae""])
return model
model = create_model()
# addestriamo per 20 epochs con e assegnamo la callback
history = model.fit(dataset, epochs=20, callbacks=[early_stopping])
Lanciamo il training
Vediamo come alla 20° epoca il MAE target viene raggiunto e il training viene stoppato.
Valutazione del modello
Plottiamo le curve per la loss e il MAE.
# plottiamo MAE e loss
plt.figure(figsize=(10, 6))
plt.plot(history.history['mae'], label='mae')
plt.plot(history.history['loss'], label='loss')
plt.legend()
plt.show()
Le curve mostrano un miglioramento della net fino a stabilizzarsi già dopo la 5° epoca. È comunque un buon risultato.
Usiamo una funzione helper per un facile accesso a MAE e MSE. Inoltre, definiamo anche una funzione per creare le previsioni.
def compute_metrics(true_series, forecast):
""""""Helper per stampare MSE e MAE""""""
mse = tf.keras.metrics.mean_squared_error(true_series, forecast).numpy()
mae = tf.keras.metrics.mean_absolute_error(true_series, forecast).numpy()
return mse, mae
def model_forecast(model, series, window_size):
""""""Questa funzione converte la serie in input in un Dataset con finestre temporali per la previsione""""""
ds = tf.data.Dataset.from_tensor_slices(series)
ds = ds.window(window_size, shift=1, drop_remainder=True)
ds = ds.flat_map(lambda w: w.batch(window_size))
ds = ds.batch(32).prefetch(1)
forecast = model.predict(ds)
return forecast
Vediamo ora come si comporta il modello! Facciamo delle previsioni sulla serie intera e sul set di validazione.
# Predizione su tutta la serie temporale
all_forecast = model_forecast(model, G.SERIES, G.WINDOW_SIZE).squeeze()
# Porzione di validazione
val_forecast = all_forecast[G.SPLIT_TIME - G.WINDOW_SIZE:-1]
# Grafico
plt.figure(figsize=(10, 6))
plt.plot(series_valid, label=""validation set"")
plt.plot(val_forecast, label=""predicted"")
plt.xlabel(""Timestep"")
plt.ylabel(""Value"")
plt.legend()
plt.show()
Vediamo i risultati sul set di validazione
E sull'intera serie
I risultati sembrano essere buoni. Forse abbiamo modo di aumentare le performance del modello aumentando le epoche di addestramento oppure giocando con il learning rate.
mse, mae = compute_metrics(series_valid, val_forecast)
print(f""mse: {mse:.2f}, mae: {mae:.2f}"")
mse: 30.91, mae: 3.32 forecast.
Predire un nuovo punto nel futuro
Vediamo ora come predire un punto nel futuro data l'ultima sequenza nella serie.
new_forecast = []
new_forecast_series = G.SERIES[-G.WINDOW_SIZE:]
pred = model.predict(new_forecast_series[np.newaxis])
plt.figure(figsize=(15, 6))
plt.plot(G.TIME[-100:], G.SERIES[-100:], label=""last 100 points of time series"")
plt.scatter(max(G.TIME)+1, pred, color=""red"", marker=""x"", s=70, label=""prediction"")
plt.legend()
plt.show()
Ed ecco il risultato
Conclusione
Ricapitolando, abbiamo visto come
- generare una serie temporale sintetica
- dividere appropriatamente la serie in X e y
- strutturare una rete neurale in Keras e Tensorflow basata su LSTM bidirezionali
- addestrare con early stopping e valutare le performance
- fare previsioni sulla serie di addestramento, validazione e nel futuro
Se avete suggerimenti su come migliorare questo flusso, scrivete nei commenti e condividete il vostro metodo. Spero che l'articolo vi possa aiutare con i vostri progetti.
A presto,
Commenti dalla community"
https://www.diariodiunanalista.it/posts/quando-e-quando-non-utilizzare-il-machine-learning-nella-tua-strategia-aziendale/,"Non è una domanda scontata. Per il data scientist alle prime armi, spingere subito verso l’adozione di modelli di apprendimento automatico può essere un errore.
Di fatto, soluzioni più semplici basate su regole specifiche possono risultare efficienti e più facili da implementare, salvando grosse somme di denaro durante il processo.
A mano a mano che l’adozione del machine learning nelle varie industrie aumenta, emerge sempre di più la tendenza a voler usare questo approccio ovunque sia possibile applicarlo.
Il machine learning è sicuramente potente, ma non è un incantesimo magico in grado di risolvere ogni nostro problema. Infatti, quando lanciato, potrebbe essere un incantesimo molto oneroso da gestire da parte dell’incantatore.
In questo articolo parlerò di quando usare il machine learning, e delle domande che vale la pena porsi prima di iniziare un progetto e di vagliare i vari approcci a disposizione.
Prima di iniziare, esaminerò cosa fa in generale una soluzione basata sul machine learning
Cosa significa apprendere pattern? Inizierò parlando proprio di questa parte, per poi toccare il tema dei dati e della creazione di predizione.
Tutto questo percorso culminerà con la spiegazione dei diversi compromessi da considerare quando consideriamo di sviluppare un progetto basato sul machine learning.
Ti piace quello che stai leggendo?
Iscriviti al blog per rimanere sempre aggiornato sui contenuti di settore. No pubblicità. No spam.
Solo contenuto utile per la tua carriera nel ML e DS.
Puoi rimuovere l'iscrizione quando vuoi seguendo il link nella email.
Iniziamo.
Apprendimento di pattern
Un modello di machine learning è un sistema che attrae tecnologi e non per la sua abilità di “imparare” dai dati che osserva (da qui il termine apprendimento supervisionato).
Se ti interessa leggere di più sul machine learning in generale, leggi l’articolo qui in basso 👇
Nell’apprendimento supervisionato, il sistema impara le relazioni tra coppie di input e output e ricostruisce una funzione matematica che spiega (quanto meglio possibile) la relazione menzionata.
Ad esempio, se volessimo creare un modello di machine learning in grado di prevedere i prezzi degli immobili nella nostra regione, dovremmo addestrare il nostro modello sull’input (le caratteristiche degli immobili, come la presenza del giardino, di un camino, o del numero di metri quadri) e sull’output, il prezzo.
Il modello in questione utilizzerà le caratteristiche dei vari immobili al quale viene esposto per capire quale sia il prezzo associato a tali caratteristiche. Una volta addestrato correttamente, al data scientist basterà fornire al modello delle caratteristiche di un immobile per ricevere un prezzo.
Tutto questo è bello e interessante, ma introduce un vincolo da considerare:
Sistemi di ML sono realmente utili solo quando ci sono pattern dalla quale imparare.
Di fatto, una persona sana di mente non andrà mai a chiedere fondi e risorse per sviluppare un sistema di machine learning per predire il prossimo tiro di dado - sarebbe casuale, e il modello non apprenderebbe proprio nulla!
Questo pone di fronte a noi un’altra domanda: come facciamo a sapere che esistano davvero dei pattern nei nostri dati?
Non è una domanda ovvia, ma è spesso una domanda completamente sorvolata e si delega la responsabilità di trovare quello che non c’è al modello di machine learning. A volte il pattern potrebbe davvero esistere, ma il dataset è incompleto o non è ricco abbastanza per farlo emergere.
Se la nostra soluzione può essere espressa correttamente da una tabella pivot in Excel…non c’è nulla di sbagliato. Non ci serve un modello di machine learning.
Dunque, non ci serve un modello di machine learning se quello che vogliamo predire è il frutto di operazioni semplici, automatizzabili grazie ad uno script magari, oppure se sappiamo che i pattern sono assenti oppure casuali.
Se invece i pattern sono complessi e difficile da processare per un essere umano (come mappare centinaia di caratteristiche di un immobile ad un prezzo), allora il machine learning sarà molto probabilmente utile.
La dipendenza dai dati
Poiché il machine learning dipende dai dati, è importante che tali dati esistano in primo luogo.
Mi imbatto spesso in clienti che investono tempo e risorse nel progettare sistemi di apprendimento automatico basati su dati poco affidabili e di bassa qualità.
Questo è un tema centrale: un sistema di machine learning è da contemplare quando abbiamo dati (o siamo in grado di recuperali) di qualità che sappiamo per certo contengano le informazioni riguardo il fenomeno che vogliamo modellare.
Ci sono casi dove è possibile usare il machine learning senza dati. Questo è il caso del continual learning. In questo caso, i modelli di ML sono messi in produzione e imparano a mano a mano che i dati degli utenti reali vengono fatti passare attraverso di esso. Tali modelli avranno performance scadente all'inizio, ma poi miglioreranno gradualmente.
Quando le predizioni sono troppo ""sbagliate""
Sistemi di ML risolvono problemi di predizione. ""Predire"" si traduce con stimare il valore di qualcosa nel futuro.
...ma a volte non siamo soddisfatti di queste predizioni.
Ragioniamo sul termine ""modellare"":
- un bambino ""modella"" un castello con la sabbia
- un pasticciere usa delle ""formine"" per creare dei dolci
- un fabbro usa uno stampo per creare uno strumento, come un martello o una pinza
Come data scientist noi usiamo il passato per modellare il futuro.
I dati sono il nostro metallo, il machine learning il nostro stampino.
Ora, in base alla qualità dei nostri dati, il nostro modello riuscirà a generalizzare una ""forma"" tale per cui questa vada bene anche per altri dati sconosciuti che verranno nel futuro.
Ma una forma molto grande può ospitare bene i nostri dati....
Entra in gioco il tema dell'errore della predizione.
Quando valutiamo il modello di machine learning, lo facciamo usando metriche di business e non. Usare metriche di valutazione valide e sensate permette di gestire e di valutare correttamente sistemi di machine learning.
È quindi importante comprendere come valutare un sistema di predizione con una metrica di valutazione chiara per il business.
Ad esempio per il problema della predizione dei prezzi degli immobili, usare una metrica come il MAE (mean absolute error) può essere utile ed è una metrica facilmente comprensibile anche ai non tecnici.
Fare questo permette di implementare un sistema di ML con cognizione di causa, perché sarà facile interpretare le sue performance.
Automatizziamo tutto!
Se ogni giorno facciamo lo stesso compito ripetutamente e questo è composto da pattern, allora contemplare il machine learning è cosa giusta.
Come ho scritto più volte in questo blog, un data scientist è un abilitatore dell'automazione.
Infatti, un buon framing del problema può migliorare l'economia aziendale significativamente grazie al ML.
Se riusciamo a individuare che ci sono dati strutturati e ricorrenti che seguono una logica prevedibile, allora il machine learning può aiutarci a creare modelli che automatizzino tali compiti. Questo ci permette di risparmiare tempo e risorse preziose, oltre a ridurre gli errori umani.
L'automazione attraverso il machine learning non solo semplifica le attività quotidiane, ma può anche rivelare informazioni nascoste nei dati e fornire insight preziosi per prendere decisioni strategiche.
Inoltre: bisogna ragionare anche su quanto dobbiamo automatizzare.
Se dobbiamo automatizzare parecchie operazioni al giorno (nell'ordine del centinaio alle decine di migliaia), sicuramente il machine learning può aiutarci!
Il costo della predizione
Parliamo di nuovo di predizione, ma stavolta in termini monetari.
Mettiamo subito in chiaro anche qui: il nostro modello farà SEMPRE errori. A volte grandi a volte piccoli, ma ne farà sempre.
Quando il costo di un errore grande di predizione è basso, allora vale la pena considerare il machine learning come soluzione al nostro problema.
Un esempio con i modelli di raccomandazione:
- il modello è addestrato sul comportamento dell'umano
- il modello offre una raccomandazione
- l'utente sceglie se seguire la raccomandazione o meno
Nel caso in cui l'utente non scelga la raccomandazione, cosa succede? Niente! Il costo di una predizione sbagliata è vicino allo zero.
Se invece l'utente compra qualcosa grazie a quella raccomandazione, allora sia l'utente che il proprietario del negozio sono contenti. Bingo.
Caso opposto invece nell'ambito medico: una cattiva previsione applicata da un modello su una radiografia per la diagnosi di una malattia grave ha un costo molto elevato. In quel caso le conseguenze possono essere catastrofiche. Stesso discorso per la guida autonoma.
Queste due ultime casistiche dovrebbero rimuovere l'uso del machine learning come strumento di diagnosi o come autopilota? Certamente no. Ma va considerato quanto l'umano è incluso nel processo (come un medico che viene assistito durante la diagnosi o un guidatore che prende il controllo dell'auto, sganciando l'autopilota) e dell'impatto che possono avere questi meccanismi sul progresso generale del mondo.
Conclusione
l'adozione del machine learning richiede una valutazione attenta e consapevole. Ne vanno parecchi soldi, perché mettere su un sistema del genere è oneroso e richiede molte competenze, da parte di più professionisti.
Non dovremmo cadere nell'errore di considerarlo come la soluzione a tutti i nostri problemi, ma piuttosto come uno strumento potente da utilizzare in determinati contesti.
Posso aiutarti nel contestualizzare la tua scelta e puntarti nella direzione migliore (anche se questo articolo dovrebbe già averti chiarito parecchi dubbi!)
A presto,
Andrea
Commenti dalla community"
https://www.diariodiunanalista.it/posts/raggruppamento-testuale-con-tf-idf/,"Il TF-IDF è una tecnica di vettorizzazione molto conosciuta e documentata nel data science. La vettorizzazione è l'atto di convertire un dato in un formato numerico in modo tale che un modello statistico possa interpretarlo e fare le predizioni.
In questo articolo vedremo come convertire un corpus di testi in formato numerico e applicheremo degli algoritmi di apprendimento automatico per far emergere pattern e anomalie interessanti.
Metodologia
Utilizzeremo un dataset fornito da Sklearn per avere un corpus di testo replicabile. Dopodiché, useremo l'algoritmo KMeans per raggruppare i vettori generati dal TF-IDF.
Useremo poi la Principal Component Analysis per visualizzare i nostri gruppi e far emergere caratteristiche comuni o inusuali dei testi presenti nel nostro corpus.
Ecco una scaletta
- Importiamo il dataset
- Applichiamo preprocessing al nostro corpus, così da rimuovere parole e simboli che, se convertiti in formato numerico, non aggiungono valore al nostro modello
- Usiamo il TF-IDF come algoritmo di vettorizzazione
- Applichiamo KMeans per raggruppare i nostri dati
- Applichiamo PCA per ridurre la dimensionalità dei nostri vettori a 2 per visualizzazione
- Interpreteremo i dati e inseriremo le nostre considerazioni in un report finale
- Preprocessare un corpus di testi per prepararli alla vettorizzazione
- Applicare l'algoritmo TF-IDF sui testi in modo da convertirli in formato numerico
- Applicare il clustering K-Means per trovare gruppi di testi simili
- Applicare la PCA per visualizzare i raggruppamenti in 2 dimensioni
L'Analisi
Il Dataset
Per questo esempio useremo l'API di Scikit-Learn, sklearn.datasets che permette di accedere ad un famoso dataset per analisi linguistiche, il 20 newsgroups. Un newsgroup è un gruppo di discussione di utenti online, ad esempio un forum. Sklearn permette di accedere a diverse categorie di contenuto. Useremo i testi che hanno a che vedere con la tecnologia, la religione e lo sport.
# importiamo le librerie necessarie da sklearn
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
# importiamo le altre librerie necessarie
import pandas as pd
import numpy as np
# librerie per la manipolazione del testo
import re
import string
import nltk
from nltk.corpus import stopwords
# importiamo le librerie di visualizzazione
import matplotlib.pyplot as plt
categories = [
'comp.graphics',
'comp.os.ms-windows.misc',
'rec.sport.baseball',
'rec.sport.hockey',
'alt.atheism',
'soc.religion.christian',
]
dataset = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, remove=('headers', 'footers', 'quotes'))
Se accediamo al primo elemento con dataset['data'][0] vediamo
They tried their best not to show it, believe me. I'm surprised they couldn't find a sprint car race (mini cars through pigpens, indeed!) on short notice.
George
È possibile che il vostro dato sia diverso a causa del shuffle=True, che randomizza l'ordine degli elementi del dataset. Il numero di elementi nel nostro dataset è 3451.
Creiamo un dataframe Pandas dal nostro dataset
df = pd.DataFrame(dataset.data, columns=[""corpus""])
Notiamo come siano presenti ""\n"", ""==="" e altri simboli che andrebbero rimossi per addestrare correttamente il nostro modello.
Preprocessing
Insieme ai simboli menzionati, vogliamo anche le stopword. Quest'ultime sono una serie di parole che non aggiungono informazioni al nostro modello. Un esempio di stopword in inglese sono gli articoli, le congiunzioni e così via.
Useremo la libreria NLTK e importiamo le stopword per visualizzarle
import nltk
from nltk.corpus import stopwords
# nltk.download('stopwords')
stopwords.words(""english"")[:10] # <-- importiamo le stopword inglesi
>>> ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', ""you're""]
Ora creiamo una funzione preprocess_text che prende in input un testo e restituisce una versione pulita dello stesso.
def preprocess_text(text: str, remove_stopwords: bool) -> str:
""""""Funzione che pulisce il testo in input andando a
- rimuovere i link
- rimuovere i caratteri speciali
- rimuovere i numeri
- rimuovere le stopword
- trasformare in minuscolo
- rimuovere spazi bianchi eccessivi
Argomenti:
text (str): testo da pulire
remove_stopwords (bool): rimuovere o meno le stopword
Restituisce:
str: testo pulito
""""""
# rimuovi link
text = re.sub(r""http\S+"", """", text)
# rimuovi numeri e caratteri speciali
text = re.sub(""[^A-Za-z]+"", "" "", text)
# rimuovere le stopword
if remove_stopwords:
# 1. crea token
tokens = nltk.word_tokenize(text)
# 2. controlla se è una stopword
tokens = [w for w in tokens if not w.lower() in stopwords.words(""english"")]
# 3. unisci tutti i token
text = "" "".join(tokens)
# restituisci il testo pulito, senza spazi eccessivi, in minuscolo
text = text.lower().strip()
return text
Ecco un lo stesso documento precedente, stavolta pulito
tried best show believe im surprised couldnt find sprint car race mini cars pigpens indeed short notice george
Applichiamo la funzione a tutto il dataframe
Ora siamo pronti alla vettorizzazione.
Vettorizzazione con TF-IDF
Il TF-IDF converte in un formato numerico il nostro corpus facendo emergere termini specifici, pesando diversamente i termini molto rari o molto comuni in modo da assegnare loro un punteggio basso.
TF sta per term frequency, mentre IDF sta per inverse document frequency. Il valore TF-IDF aumenta proporzionalmente al numero di volte che una parola appare nel documento ed è compensato dal numero di documenti nel corpus che contengono quella parola.
Con questa tecnica di vettorizzazione siamo quindi in grado di raggruppare i nostri documenti considerando i termini più importanti che li costituiscono. Per leggere di più su come funziona il TF-IDF, leggere qui.
È facile applicare il TF-IDF con Sklearn:
# inizializziamo il vettorizzatore
vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.95)
# fit_transform applica il TF-IDF ai testi puliti - salviamo la matrice di vettori in X
X = vectorizer.fit_transform(df['cleaned'])
X è la matrice di vettori che verrà usata per addestrare il modello KMeans. Il comportamento predefinito di Sklearn è quello di creare una matrice sparsa. La vettorizzazione genera dei vettori simili a questo
vettore_a = [1.204, 0, 0, 0, 0, 0, 0, ..., 0]
Il vettore è composto da un singolo valore non uguale a 0. In Sklearn, una matrice sparsa non è altro che una matrice che indicizza la posizione dei valori e degli 0 invece di memorizzarla come una qualsiasi altra matrice. Questo è un meccanismo che serve a risparmiare RAM e potenza di calcolo. La comodità è che la matrice sparsa è accettata dalla maggior parte degli algoritmi di machine learning, come anche il KMeans. Infatti quest'ultimo userà i dati presenti nella matrice sparsa per trovare gruppi e pattern.
Se usiamo X.toarray() vediamo di fatto la matrice completa, non sparsa.
Implementazione del KMeans
Il KMeans è uno degli algoritmi non supervisionati più conosciuti e usati nel mondo del data science e serve a raggruppare in un numero definito di gruppi un insieme di dati. L'idea alla base è molto semplice: l'algoritmo inizializza delle posizioni a caso (chiamati centroidi, i punti rossi, blu e verdi nello screenshot in basso) nel piano vettoriale e assegna il punto al centroide più vicino.
L'algoritmo calcola la posizione media (o, se aiuta l'interpretazione, il ""centro di gravità"") dei punti e sposta il rispettivo centroide in quella posizione e aggiorna il gruppo di appartenenza di ogni punto. L'algoritmo converge quando tutti i punti sono alla distanza minima dal rispettivo centroide.
Fatta questa doverosa introduzione, continuiamo con il nostro progetto andando a usare nuovamente Sklearn
from sklearn.cluster import KMeans
# inizializziamo il kmeans con 3 centroidi
kmeans = KMeans(n_clusters=3, random_state=42)
# addestriamo il modello
kmeans.fit(X)
# salviamo i gruppi di ogni punto
clusters = kmeans.labels_
Con questo snippet di codice abbiamo addestrato il KMeans con i vettori restituiti dal TF-IDF e abbiamo assegnato i gruppi che ha trovato ad alla variabile clusters
Ora possiamo procedere alla visualizzazione dei nostri gruppi e valutare la loro segmentazione e/o presenza di anomalie.
Riduzione della dimensionalità e visualizzazione
Abbiamo la nostra X dal TF-IDF e abbiamo un modello KMeans e relativi cluster. Ora vogliamo mettere insieme questi due pezzi per visualizzare in quale gruppo va quale testo.
Come ben sappiamo, un grafico si presenta solitamente in 2 dimensioni e raramente in 3. Sicuramente non possiamo visualizzarne di più. Se andiamo a vedere la dimensionalità di X con X.shape vediamo che essa è (3451, 7390). Ci sono 3451 vettori (uno per testo), ognuno con 7390 dimensioni. Impossibile visualizzarle!
Fortunatamente per noi, esiste una tecnica che si chiama PCA (Principal Component Analysis) che riduce la dimensionalità di un set di dati ad un numero arbitrario preservando la maggior parte delle informazioni contenute in esse.
Come per il TF-IDF, non andrò nel dettaglio del suo funzionamento, e potete leggere di più qui sul suo funzionamento. Al fine di questo articolo, ci basti sapere che la PCA tende a preservare le dimensioni che meglio riassumono la variabilità totale dei nostri dati, andando a rimuovere dimensioni che contribuiscono poco a quest'ultima.
La nostra X andrà da 7390 dimensioni a 2. Sklearn.decomposition.PCA è quello che ci occorre.
from sklearn.decomposition import PCA
# inizializziamo la PCA con 2 componenti
pca = PCA(n_components=2, random_state=42)
# passiamo alla pca il nostro array X
pca_vecs = pca.fit_transform(X.toarray())
# salviamo le nostre due dimensioni in x0 e x1
x0 = pca_vecs[:, 0]
x1 = pca_vecs[:, 1]
Se ora vediamo la dimensionalità di x0 e x1 vediamo che sono rispettivamente (3451,), quindi un punto (x0,x1) per testo. Questo ci da la possibilità di creare un grafico a dispersione.
Visualizzare i gruppi
Prima di creare il nostro grafico andiamo ad organizzare meglio il nostro dataframe andando a creare una colonna cluster, x0, x1
# assegnamo cluster e vettori PCA a delle colonne nel dataframe originale
df['cluster'] = clusters
df['x0'] = x0
df['x1'] = x1
Vediamo quali sono le keyword più rilevanti per ogni centroide
def get_top_keywords(n_terms):
""""""Questa funzione restituisce le keyword per ogni centroide del KMeans""""""
df = pd.DataFrame(X.todense()).groupby(clusters).mean() # raggruppa il vettore TF-IDF per gruppo
terms = vectorizer.get_feature_names_out() # accedi ai termini del tf idf
for i,r in df.iterrows():
print('\nCluster {}'.format(i))
print(','.join([terms[t] for t in np.argsort(r)[-n_terms:]])) # per ogni riga del dataframe, trova gli n termini che hanno il punteggio più alto
get_top_keywords(10)
Bene! Vediamo come il KMeans abbia correttamente creato 3 gruppi distinti, uno per ogni categoria presente nel dataset. Il cluster 0 si riferisce allo sport, il cluster 2 al software / tech, il cluster 3 alla religione. Applichiamo questa mappatura
# mappiamo cluster con termini adatti
cluster_map = {0: ""sport"", 1: ""tecnologia"", 2: ""religione""}
# applichiamo mappatura
df['cluster'] = df['cluster'].map(cluster_map)
Procediamo con la libreria Seaborn per visualizzare in maniera molto semplice i nostri testi raggruppati.
# settiamo la grandezza dell'immagine
plt.figure(figsize=(12, 7))
# settiamo titolo
plt.title(""Raggruppamento TF-IDF + KMeans 20newsgroup"", fontdict={""fontsize"": 18})
# settiamo nome assi
plt.xlabel(""X0"", fontdict={""fontsize"": 16})
plt.ylabel(""X1"", fontdict={""fontsize"": 16})
# creiamo diagramma a dispersione con seaborn, dove hue è la classe usata per raggruppare i dati
sns.scatterplot(data=df, x='x0', y='x1', hue='cluster', palette=""viridis"")
plt.show()
Come è possibile vedere, il clustering ha funzionato bene: tre gruppi distinti come lo sono i gruppi definiti a priori dal nostro dataset. Immaginate la potenza di questo approccio nel trovare gruppi in dati non etichettati a priori!
Interpretazione
L'interpretazione è abbastanza semplice: non sono presenti anomalie particolari, tranne per il fatto che ci sono dei testi appartenenti alla categoria tecnologia che si mischiano leggermente con quelli dello sport, tra il confine blu scuro e verde acceso. Questo è dovuto alla presenza di termini comuni tra alcuni di questi testi che quando vettorizzati ottengono valori uguali per alcune dimensioni.
Come follow-up sarebbe interessante andare ad investigare come sono scritti questi testi e capire se la motivazione ipotizzata sia fondata o meno.
Il Codice
Ecco tutto il codice completo, in formato copia-incolla
def preprocess_text(text: str, remove_stopwords: bool) -> str:
""""""Funzione che pulisce il testo in input andando a
- rimuovere i link
- rimuovere i caratteri speciali
- rimuovere i numeri
- rimuovere le stopword
- trasformare in minuscolo
- rimuovere spazi bianchi eccessivi
Argomenti:
text (str): testo da pulire
remove_stopwords (bool): rimuovere o meno le stopword
Restituisce:
str: testo pulito
""""""
# rimuovi link
text = re.sub(r""http\S+"", """", text)
# rimuovi numeri e caratteri speciali
text = re.sub(""[^A-Za-z]+"", "" "", text)
# rimuovere le stopword
if remove_stopwords:
# 1. crea token
tokens = nltk.word_tokenize(text)
# 2. controlla se è una stopword
tokens = [w for w in tokens if not w.lower() in stopwords.words(""english"")]
# 3. unisci tutti i token
text = "" "".join(tokens)
# restituisci il testo pulito, senza spazi eccessivi, in minuscolo
text = text.lower().strip()
return text
def get_top_keywords(n_terms):
""""""Questa funzione restituisce le keyword per ogni centroide del KMeans""""""
df = pd.DataFrame(X.todense()).groupby(clusters).mean() # raggruppa il vettore TF-IDF per gruppo
terms = vectorizer.get_feature_names_out() # accedi ai termini del tf idf
for i,r in df.iterrows():
print('\nCluster {}'.format(i))
print(','.join([terms[t] for t in np.argsort(r)[-n_terms:]])) # per ogni riga del dataframe, trova gli n termini che hanno il punteggio più alto
# importiamo le librerie necessarie da sklearn
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
# importiamo le altre librerie necessarie
import pandas as pd
import numpy as np
# librerie per la manipolazione del testo
import re
import string
import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
# librerie di visualizzazione
import matplotlib.pyplot as plt
import seaborn as sns
categories = [
'comp.graphics',
'comp.os.ms-windows.misc',
'rec.sport.baseball',
'rec.sport.hockey',
'alt.atheism',
'soc.religion.christian',
]
dataset = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, remove=('headers', 'footers', 'quotes'))
df = pd.DataFrame(dataset.data, columns=[""corpus""])
df['cleaned'] = df['corpus'].apply(lambda x: preprocess_text(x, remove_stopwords=True))
# inizializziamo il vettorizzatore
vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.95)
# fit_transform applica il TF-IDF ai testi puliti - salviamo la matrice di vettori in X
X = vectorizer.fit_transform(df['cleaned'])
# inizializziamo il KMeans con 3 cluster
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(X)
clusters = kmeans.labels_
# inizializziamo la PCA con 2 componenti
pca = PCA(n_components=2, random_state=42)
# passiamo alla pca il nostro array X
pca_vecs = pca.fit_transform(X.toarray())
# salviamo le nostre due dimensioni in x0 e x1
x0 = pca_vecs[:, 0]
x1 = pca_vecs[:, 1]
# assegnamo cluster e vettori PCA a delle colonne nel dataframe originale
df['cluster'] = clusters
df['x0'] = x0
df['x1'] = x1
cluster_map = {0: ""sport"", 1: ""tecnologia"", 2: ""religione""} # mappatura trovata con get_top_keywords
df['cluster'] = df['cluster'].map(cluster_map)
# settiamo la grandezza dell'immagine
plt.figure(figsize=(12, 7))
# settiamo titolo
plt.title(""Raggruppamento TF-IDF + KMeans 20newsgroup"", fontdict={""fontsize"": 18})
# settiamo nome assi
plt.xlabel(""X0"", fontdict={""fontsize"": 16})
plt.ylabel(""X1"", fontdict={""fontsize"": 16})
# creiamo diagramma a dispersione con seaborn, dove hue è la classe usata per raggruppare i dati
sns.scatterplot(data=df, x='x0', y='x1', hue='cluster', palette=""viridis"")
plt.show()
Commenti dalla community"
https://www.diariodiunanalista.it/posts/rappresentazioni-vettoriali-per-il-machine-learning/,"Sviluppatori di modelli di machine learning sfruttano rappresentazioni numeriche del mondo per creare e addestrare algoritmi predittivi.
Queste rappresentazioni permettono alla macchina di imparare la relazione tra di essi e la variabile che si vuole predire.
Immaginiamo che un vettore non sia altro che una lista di numeri
$$ X = [1, 2, 3, 4, 5] $$
Questa viene messa in relazione con la variabile target \( y \)
$$ X = [1, 2, 3, 4, 5];\ y = 1 $$
Il modello di machine learning impara la relazione tra caratteristiche e target e fornisce una previsione, in questo caso una classificazione in cui una delle classi è identificata con il numero 1.
In questo post, scriverò di come i vettori possano essere utilizzati per rappresentare concetti complessi in un formato numerico.
La motivazione è che un modello di machine learning non può imparare da fenomeni che non gli siano forniti in formato numerico.
Bisogna prima trasformare un testo, una immagine, un suono o qualsiasi altro fenomeno in un formato numerico adatto all’apprendimento.
Ci sono varie tecniche per trasformare un fenomeno in vettori, e queste dipendono dal tipo di dato con il quale lavoriamo
- Inizieremo presentando il concetto di One-Hot Encoding, una tecnica utilizzata per rappresentare parole come vettori numerici.
- Successivamente, discuteremo i limiti di questa tecnica e introdurremo il concetto di embedding, una tecnica che permette di rappresentare parole, immagini, suoni e altro come vettori numerici di dimensioni inferiori rispetto alle migliaia di categorie necessarie con il One-Hot Encoding.
- Menzioneremo anche i modelli TF-IDF e bag of words, che sono fondamentali nella vettorizzazione del testo
Come si codifica un fenomeno in vettore?
Usiamo del testo per portare avanti il discorso. L’esempio è naturale perché come possiamo intuire, i modelli di machine learning non possono usare direttamente il testo per il loro apprendimento. Occorre trasformare ogni carattere o parola in un numero prima.
Facciamo un esempio. Poniamo che vogliamo creare una rappresentazione numerica delle parole
- Re
- Regina
- Principe
- Principessa
Il modo più semplice di codificare queste parole sarebbe di assegnare a ognuna di esse un numero, in maniera sequenziale.
|Re
|1
|Regina
|2
|Principe
|3
|Principessa
|4
Le parole sono state correttamente trasformate in formato numerico, seguendo la mappatura
map = {
""Re"": 1,
""Regina"": 2,
""Principe"": 3,
""Principessa"": 4
}
Ma c’è un problema. Se fornissimo questi dati a qualsiasi modello predittivo, questo andrebbe ad assegnare un valore matematico più alto a principe e principessa, rendendoli più importanti di re e regina.
Ovviamente questo andrebbe a fornire informazioni sbagliate al modello, che apprenderebbe relazioni sbagliate. Dobbiamo rendere la nostra rappresentazione numerica più precisa.
One-Hot Encoding
Per risolvere il problema di rappresentazione numerica descritto sopra, è possibile utilizzare la tecnica del One-Hot Encoding.
In questo caso, ogni parola sarebbe rappresentata da un vettore numerico di dimensione pari al numero totale di parole che si vuole rappresentare. Il vettore avrebbe tutti i valori pari a zero, tranne uno, che rappresenta la parola specifica.
Ad esempio, nel caso delle quattro parole ""Re"", ""Regina"", ""Principe"" e ""Principessa"", ogni parola sarebbe rappresentata da un vettore di quattro elementi, con il valore ""1"" nella posizione corrispondente alla parola e ""0"" in tutte le altre posizioni.
Questa tecnica risolve il problema di assegnare un valore matematico più alto a parole che non sono più importanti di altre nella rappresentazione numerica.
|Re
|[1, 0, 0, 0]
|Regina
|[0, 1, 0, 0]
|Principe
|[0, 0, 1, 0]
|Principessa
|[0, 0, 0, 1]
Molto bene. Ora il nostro modello ha una rappresentazione vettoriale “bilanciata” per ogni vocabolo appartenente al dataset (che in questo caso è formato solo da 4 vocaboli).
Ma…cosa succede se il nostro vocabolario è formato da migliaia o magari milioni di vocaboli? Considerando che nel dizionario italiano esistono circa 270.000 vocaboli, applicare il one-hot encoding sarebbe a dir poco problematico.
Le risorse computazionali per svolgere questa codifica sarebbero notevoli e la rappresentazione finale sarebbe “solo” bilanciata: non c’è alcuna informazione riguardo le relazioni tra le parole.
Word Embedding
Per superare i limiti del One-Hot Encoding, si può utilizzare la tecnica chiama embedding. Questa permette di rappresentare parole come vettori numerici di dimensioni controllabili rispetto alle migliaia di categoria necessarie con il One-Hot Encoding.
L'idea è quella di creare una rappresentazione numerica delle parole che tenga conto delle relazioni semantiche tra le parole stesse.
In pratica, ogni parola viene rappresentata come un vettore di numeri reali, dove ogni dimensione rappresenta un aspetto diverso del significato della parola.
Proviamo a creare un grafico dove catturiamo alcune delle caratteristiche delle parole menzionate prima.
Vediamo come le parole principe e principessa siano vicine tra di loro, esattamente come re e regina.
Ponendo che la variabile genere possa assumere solo due valori, M e F (usiamo 0 e 1), e che la variabile età possa assumere solo tre valori [Giovane, Mezz’età, Anziano] (usiamo 0, 1, 2), vediamo come degli embedding possano rappresentare queste relazioni
Ponendo che la variabile genere possa assumere solo due valori, M e F (usiamo 0 e 1), e che la variabile età possa assumere solo tre valori [Giovane, Mezz’età, Anziano] (usiamo 0, 1, 2), vediamo come degli embedding possano rappresentare queste relazioni
|Re
|[0, 2]
|Regina
|[1, 2]
|Principe
|[0, 1]
|Principessa
|[1, 1]
Questa rappresentazione riesce a catturare lo status nobiliare di un individuo andando a usare le dimensioni di genere e l’età.
Muovendoci sull’asse delle X possiamo osservare come i due nobili siano equidistanti da una dimensione che rappresenta la differenza di genere (0: maschio, 1: femmina). Muovendoci sull’asse delle Y, invece, possiamo osservare come l’età sia rappresentato dalla distanza dell’embedding dall’asse Y.
In questo modo, gli embedding delle parole possono essere utilizzati come input per i modelli di machine learning, permettendo di rappresentare in modo più preciso concetti complessi in un formato numerico.
In questo esempio abbiamo solo due dimensioni. Di fatto, vengono addestrate delle reti neurali con lo specifico compito di trovare queste rappresentazioni su parecchie dimensioni.
Per mettere in prospettiva, modelli come GPT-3 usano più di 12.000 dimensioni.
Una pietra miliare del settore
Gli embedding possono essere utilizzati non solo per parole, ma anche per rappresentare immagini, suoni e altro.
L'uso delle rappresentazioni vettoriali è fondamentale nel machine learning odierno. Le varie innovazioni e tecnologie nel campo del deep learning derivano a cascata dal concetto di vettorizzazione.
Modelli come GPT-3.5 nascono incrociando rappresentazioni vettoriali, algoritmi di ottimizzazione ben studiati e grosse quantità di risorse computazionali.
Teoricamente non c’è limite a questo approccio.
Più dati → Vettori di qualità sempre maggiore → Modelli che useranno tali vettori per un addestramento migliore.
Limiti degli embedding
Nonostante gli embedding siano una tecnica molto utile per la rappresentazione di concetti complessi in formato numerico, hanno anche dei limiti.
In particolare, è importante sottolineare che gli embedding sono costruiti a partire dai dati di addestramento, e quindi possono essere influenzati da eventuali bias presenti nei dati.
Come menzionato, la qualità degli embedding dipende dalla qualità dei dati di addestramento. Se i dati di addestramento non sono rappresentativi del dominio in cui il modello verrà utilizzato, gli embedding potrebbero non essere in grado di catturare tutte le relazioni semantiche tra i concetti.
Inoltre, embedding possono richiedere molta memoria per essere memorizzati, soprattutto se il numero di dimensioni è elevato. Questo può rappresentare un problema in particolare per i modelli di machine learning che devono essere eseguiti su dispositivi con risorse limitate, come ad esempio i dispositivi mobili.
Altre forme di rappresentazione per il testo
Essendo il testo il formato di dati più presente intorno a noi (basti pensare all’enorme quantità di dati testuali presenti sull’internet), alcune tecniche di vettorizzazione testuale sono comuni e ben conosciute.
Una di queste è la trasformazione TF-IDF che è una tecnica di vettorizzazione testuale che assegna un peso a ciascuna parola in base alla sua frequenza all'interno di un documento e alla sua frequenza complessiva all'interno del corpus.
In questo modo, le parole che compaiono frequentemente all'interno di un documento ma raramente all'interno del corpus avranno un peso maggiore rispetto a quelle che compaiono spesso ovunque. Questa tecnica è molto utilizzata nell'ambito del Natural Language Processing per l'analisi dei testi.
Invito il lettore interessato ad saperne di più sul modello TF-IDF leggendo il seguente articolo
Il TF-IDF si basa sul modello bag of words che rappresenta un documento come un insieme non ordinato di parole, ignorando la struttura della frase e l'ordine delle parole.
In questo modo, il bag of words può essere utilizzato per rappresentare qualsiasi documento come un vettore di valori numerici, dove ogni valore rappresenta la frequenza di una parola all'interno del documento. Ovviamente non c’è una rappresentazione adeguata della relazione tra le parole, che è fornito invece dagli embedding.
Invito il lettore interessato ad saperne di più sul modello BoW leggendo il seguente articolo
Conclusione
In questo post abbiamo visto come i vettori possano essere utilizzati per rappresentare concetti complessi in un formato numerico.
È importante per un data scientist ragionare in termini di vettorizzazione. Domande del tipo
- come posso convertire questo stimolo in un numero?
- come viene interpretato questo dato dalla rete neurale?
- come posso migliorare questa rappresentazione?
sono fondamentali, e il team che riesce a rispondere adeguatamente a queste domande creerà dei sistemi migliori.
I data scientist vedono il mondo in termini di vettori.
Commenti dalla community"
https://www.diariodiunanalista.it/posts/recensione-storytelling-with-data/,"Ho sempre pensato che la data visualization meriti uno studio approfondito per essere fatta come si deve.
Spesso e volentieri usiamo grafici che secondo noi hanno senso, senza soffermarci su quale sia la modalità migliore per trasmettere il messaggio.
Grazie a Storytelling with Data, ho scoperto che ci sono delle metodologie e ""trucchetti"" che fanno leva su principi di percezione visiva e linee guida di design che migliorano notevolmente l'efficacia delle nostre visualizzazioni.
Come data scientist, visualizzare i dati è una competenza molto importante e grazie a questo libro sono riuscito a migliorare molto nel campo.
Storytelling With Data ha un posto speciale nel mio cuore. Non perché è uno dei migliori libri sul mercato sulla data visualization, e neppure perché è scritto da Cole Knaflic, una delle esperte mondiali sul tema.
No.
Questo libro è speciale perché ti prende per mano e ti fa capire cosa sia davvero la data visualization, passando per concetti fondamentali alla base della percezione visiva, fino ai concetti di design più avanzati.
Grazie a questo libro, tra l'altro, sono riuscito a comprendere gli aspetti più subdoli della data viz (come tutta l'esperienza legata alla presentazione) che mi hanno portato enormi benefici in ambito lavorativo.
Come avete potuto leggere, questo è un gran libro e difficilmente troverete di meglio quando si parla di data visualization.
Lo consiglio assolutamente.
Storytelling With Data
A Data Visualization Guide for Business Professionals
L'Indice
Il manuale copre i seguenti macro argomenti nell'indice dei contenuti:
- Comprendere e sfruttare l'importanza del contesto
- Come scegliere la visualizzazione giusta per i nostri dati
- Come sfruttare principi psicologici per evitare distrazioni e guidare l'attenzione del pubblico
- Linee guida di design per migliorare visualizzazioni e presentazioni
- Use case ed esempi pratici dei concetti esposti
È a mio avviso un bell'indice, poiché copre infatti molti aspetti ignoti a chi si approccia per la prima volta alla visualizzazione dei dati.
Il numero totale di pagine è circa 270 - questo a mio avviso non lo rende né troppo denso, né troppo corto.
Anche i veterani dell'argomento saranno felici di leggere temi avanzati, come l'analisi dei singoli elementi, come grafici, testi e altro che compongono una scena visiva al fine di comprendere fattori che beneficiano o ostacolano la comprensione da parte del pubblico.
Una guida alla data visualization per professionisti
Scritto nel 2015 e pubblicato da Wiley, il manuale si presenta con l'affermazione di essere una guida di riferimento per qualsiasi professionista, a prescindere dal settore lavorativo (in inglese A data visualization guide for business professionals).
L'aspetto di ""guida"" è quello sicuramente più apprezzato: infatti Knaflic non da mai per scontato che il lettore sappia già cosa sua uno scatterplot oppure un diagramma a cascata.
Il target è quindi il ""professionista"". È una definizione generica ed è volontaria: infatti chiunque lavori con dati da visualizzare potrà sfruttare i concetti presenti in questo manuale.
Le spiegazioni sono sempre chiare, sviluppate con precisione e dettaglio senza però mai essere prolisse o noiose.
Un viaggio tra grafici brutti, belli e storytelling
Uno degli aspetti che mette questo manuale sul podio nella sua nicchia è che l'autrice prende per mano il lettore e lo porta ad esplorare cosa differenzia un grafico inefficace (definito brutto) da uno efficace (definito bello).
Ho imparato che grafici che io pensavo fossero efficaci sono in realtà difficilmente interpretabili senza dei riferimenti espliciti, e che grafici che consideravo banali sono da preferire poiché facili da comprendere in quasi ogni circostanza.
Ho imparato che i grafici a barre sono re incontestabili in termini di chiarezza, mentre tutte le visualizzazioni che sfruttano angoli e intersezioni tra linee (come i grafici a torta) sono molto inefficaci e traggono in inganno l'occhio umano.
La componente di storytelling entra in gioco quando Knaflic ci porta allo step successivo ai grafici belli. Per l'autrice,
Lo storytelling è l'atto di trasferire il messaggio all'audience senza mai perdere la loro attenzione e mantenendo consistente il significato che il dato ha per il presentatore e per il pubblico.
L'unione di visualizzazioni efficaci ad un linguaggio e una esposizione adattata all'audience è l'essenza della comunicazione visiva, che si esplicita nella comprensione completa dell'audience verso il materiale trattato.
Use case concreti per l'occhio attento
Il lettore appassionato troverà alquanto particolare il layout del manuale. Infatti, il testo è sempre giustificato e racchiuso all'interno di margini molto stretti, lasciando molto spazio per spazi bianchi e immagini rilevanti.
Questo, come l'autrice fa intendere, è proprio l'esecuzione dei principi che lei stessa propone nel manuale. Questi accorgimenti rendono la lettura leggera, piacevole e scorrevole.
La consistenza del layout e del formato risalta agli occhi dell'osservatore, esattamente come l'autrice insegna di fare. Molto brava.
Conclusioni
Nel 2015, Cole Knaflic mi ha fatto appassionare alla data visualization e mi ha reso un analista migliore. Il suo manuale è un must-have per il professionista interessato alla data visualization e alla comunicazione visiva.
Le sue lezioni sono applicabili a tutti i livelli e anche con tutti gli strumenti di analisi più rilevanti, come Excel e alcune librerie in Python come Matplotlib e Seaborn.
Il prezzo del libro è anche molto onesto secondo me. Al momento della scrittura di questo articolo il suo prezzo su Amazon è di circa 35€.
Non posso far altro che consigliarlo, dandogli un voto pieno.
Commenti dalla community"
https://www.diariodiunanalista.it/posts/regolarizzazione-l1-vs-l2-nel-machine-learning-differenze/,"Il Machine Learning è una disciplina che sta avendo un enorme sviluppo in ambito tecnologico e industriale.
Grazie ai suoi algoritmi e alle sue tecniche di modellazione, è possibile costruire modelli in grado di apprendere da dati passati, generalizzare e fare previsioni su nuovi dati.
Tuttavia, in alcuni casi, i modelli possono adattarsi troppo ai dati di addestramento e perdere la loro capacità di generalizzazione. Questo fenomeno si chiama overfitting.
È importante per gli addetti ai lavori comprendere cosa sia l'overfitting e perché rappresenta uno degli ostacoli principali nel Machine Learning per quando riguarda la creazione di un modello predittivo.
Quando un modello è troppo complesso o si adatta troppo bene ai dati di addestramento, può diventare molto preciso per quei dati specifici, ma generalizzerà male su dati che non ha mai visto prima. Ciò significa che il modello sarà inefficace quando si applica a nuovi dati nella realtà.
Per prevenire l'overfitting è possibile utilizzare tecniche di regolarizzazione.
Il termine regolarizzazione fa da cappello ad un insieme di tecniche che semplificano un modello predittivo. In questo articolo, ci concentreremo su due tecniche di regolarizzazione, L1 e L2, spiegandone le differenze e illustrando come applicarle in Python.
Cos'è la regolarizzazione e perché è importante?
In termini semplici, regolarizzare un modello significa cambiare il suo comportamento di apprendimento durante la fase di addestramento.
La regolarizzazione aiuta a prevenire l'overfitting aggiungendo una penalità sulla complessità del modello - se un modello è troppo complesso, verrà penalizzato durante l'addestramento, il che aiuta a mantenere un buon bilanciamento tra la complessità del modello e la sua capacità di generalizzare su dati che non ha mai visto prima.
Per aggiungere una regolarizzazione L1 o L2, andiamo ad alterare la funzione di perdita (loss function in inglese) del modello. Questa è l'espressione che l'algoritmo di apprendimento cerca di ottimizzare durante la fase di addestramento.
La regolarizzazione avviene assegnando una penalità che aumenta in base a quanto complesso diventa il modello.
Se prendiamo la regressione lineare come esempio, vediamo come la tipica loss function del MSE (mean squared error - leggi di più sulle metriche di valutazione di un modello di regressione qui) possa essere espressa così
\[ min_{w^{(i)}} \lbrack \frac{1}{N} \times{\sum_{i=1}^{N} (f(x_{i}) - y)^2} \rbrack \]
Dove l'obiettivo dell'algoritmo è quello di minimizzare la differenza tra la predizione \( f(x) \) e la \(y \) reale.
Nella equazione, \( f(x) \) è la linea di regressione, e questa avrà forma
\[ f = w^{(i)} x^{(i)} + b\]
L'algoritmo quindi dovrà trovare i valori dei parametri \( w \) e \( b \) dal set di addestramento andando a minimizzare MSE.
Un modello è considerato meno complesso se alcuni parametri \( w \) sono vicini o uguali a zero.
Regolarizzazione L1 vs L2
Vediamo ora le differenze tra regolarlizzazione L1 e L2.
Regolarizzazione L1
La regolarizzazione L1, anche conosciuta come ""Lasso"", aggiunge una penalità sulla somma degli valori assoluti dei pesi del modello.
Questo significa che i pesi che non contribuiscono molto al modello saranno azzerati, il che può portare ad una selezione delle feature automatica (in quanto i pesi corrispondenti alle feature meno importanti saranno di fatto azzerati).
Questo rende L1 particolarmente utile per problemi di feature selection e modelli sparsi.
Prendendo in esempio la formula del MSE di prima, questa apparirebbe così
\[ min_{w^{(i)}} \lbrack C \times{ (\sum_{i=1}^{N} {|w^{(i)}|})} + \frac{1}{N} \times{\sum_{i=1}^{N} (f(x_{i}) - y)^2} \rbrack \]
dove \( C \) è un iperparametro del modello che controlla l'intensità della regolarizzazione. Più alto è il valore di \( C \), più i nostri pesi tenderanno verso lo zero.
In gergo, questo viene chiamato modello sparso, dove la maggior parte dei parametri ha il valore di zero.
Il rischio qui è che un valore molto alto porterà il modello all'underfitting, che è l'opposto dell'overfitting - vale a dire che non catturerà i pattern presenti nei nostri dati.
Regolarizzazione L2
D'altra parte, la regolarizzazione L2, chiamata anche regolarizzazione Ridge, somma il quadrato dei pesi al termine di regolarizzazione.
Questo significa che i pesi più grandi vengono ridotti ma non azzerati, il che porta a modelli con meno variabili rispetto alla regolarizzazione L1 ma con pesi più distribuiti.
La regolarizzazione L2 è particolarmente utile quando si hanno molte variabili altamente correlate, poiché tende a ""distribuire"" il peso su tutte le variabili invece di concentrarsi solo su alcune di esse.
Come prima, vediamo come l'equazione iniziale cambia per integrare L2
\[ min_{w^{(i)}} \lbrack C \times{ (\sum_{i=1}^{N} {(w^{(i)}})^{2}} + \frac{1}{N} \times{\sum_{i=1}^{N} (f(x_{i}) - y)^2} \rbrack \]
La regolarizzazione L2 può migliorare la stabilità del modello quando i dati di addestramento sono rumorosi o incompleti, poiché riduce l'impatto di valori anomali o rumore sulle variabili.
Ti piace quello che stai leggendo?
Iscriviti al blog per rimanere sempre aggiornato sui contenuti di settore. No pubblicità. No spam.
Solo contenuto utile per la tua carriera nel ML e DS.
Puoi rimuovere l'iscrizione quando vuoi seguendo il link nella email.
Come applicare la regolarizzazione in Sklearn e Python
In questo esempio vedremo come applicare una regolarizzazione ad un modello di regressione logistica per un problema di classificazione. Vedremo come cambiano le performance per diversi valori di \( C \) e paragoneremo quanto il modello sia accurato nel modellare i dati in input.
Useremo il famoso breast cancer dataset da Sklearn. Iniziamo a vedere come importarlo, insieme a tutte le librerie.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
# importiamo il dataset da Sklearn
breast_cancer = load_breast_cancer()
# creiamo una variabile ""data"" che contiene il dataframe dal dataset
data = pd.DataFrame(data=breast_cancer['data'], columns=breast_cancer['feature_names'])
data['target'] = pd.Series(breast_cancer['target'], dtype='category')
Essendo un problema di classifcazione, useremo la accuracy per misurare le performance del modello. Leggi l'articolo su come misurare le performance di modelli di classificazione binaria se ti interessa saperne di più.
Ora creiamo una funzione per applicare il confronto tra regolarizzazione L1 e L2 sul dataframe.
def plot_regularization(df, reg_type='l1'):
# splittiamo i nostri dati in training e test
X = df.drop('target', axis=1)
y = df['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
# definiamo i diversi valori di C
Cs = [0.001, 0.01, 0.1, 1, 10, 100, 1000]
coefs = []
test_scores = []
train_scores = []
for C in Cs:
# addestriamo il modello per i diversi valori di C
clf = LogisticRegression(penalty=reg_type, C=C, solver='liblinear')
clf.fit(X_train, y_train)
# salviamo le performance
coefs.append(clf.coef_.ravel())
train_scores.append(clf.score(X_train, y_train))
test_scores.append(clf.score(X_test, y_test))
reg = reg_type.capitalize()
# creiamo dei grafici
fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 4))
ax1.plot(Cs, train_scores, 'b-o', label='Training Set')
ax1.plot(Cs, test_scores, 'r-o', label='Test Set')
plt.suptitle(f""Regolarizzazione {reg}"")
ax1.set_xlabel('C')
ax1.set_ylabel('Accuracy')
ax1.set_xscale('log')
ax1.set_title('Performance')
ax1.legend()
coefs = np.array(coefs)
n_params = coefs.shape[1]
for i in range(n_params):
ax2.plot(Cs, coefs[:, i], label=X.columns[i])
ax2.axhline(y=0, linestyle='--', color='black', linewidth=2)
ax2.set_xlabel('C')
ax2.set_ylabel('Valori coefficienti')
ax2.set_xscale('log')
ax2.set_title('Coefficienti')
plt.show()
Applichiamo questa logica guardando la regolarizzione L1
plot_regularization(data, 'l1')
Vediamo come la regolarizzazione L1 appiattisca i coefficienti del modello vicino allo zero per molti livelli di C. I coefficienti con i valori più alti sono, secondo il modello, le feature più importanti ai fini della predizione.
Vediamo anche l'insorgenza dell'overfitting - al valore di \( C = 100 \), la performance del training set aumenta mentre quella nel test set diminuisce.
Applichiamo ora la stessa funzione per valutare gli effetti di L2.
plot_regularization(data, 'l2')
I coefficienti sono sempre sopra lo zero, creando quindi una distribuzione di pesi gradualmente sempre più grandi per le feature più rilevanti. Si nota del leggerissimo overfitting dopo il valore di \( C = 100 \).
Altre tecniche di regolarizzazione
Oltre alle regolarizzazioni L1 e L2, esistono altre tecniche di regolarizzazione che possono essere utilizzate nei modelli di machine learning. Tra queste tecniche troviamo il dropout e l'early stopping.
Dropout
Il dropout è una tecnica utilizzata nelle reti neurali per ridurre l'overfitting. Il dropout funziona spegnendo casualmente alcuni neuroni durante la fase di addestramento, obbligando la rete neurale a trovare modi alternativi per rappresentare i dati.
Early stopping
L'early stopping è un'altra tecnica utilizzata per evitare l'overfitting nei modelli di machine learning. Questa tecnica consiste nel fermare l'addestramento del modello quando la performance sul set di validazione inizia a peggiorare. In questo modo si evita che il modello impari troppo i dati di addestramento e non generalizzi bene sui dati non visti prima.
In generale, l'overfitting può essere evitato utilizzando una combinazione di tecniche di regolarizzazione. Tuttavia, la scelta delle tecniche più appropriate dipenderà dalle caratteristiche del dataset e dal modello di machine learning utilizzato.
Conclusioni
In conclusione, la regolarizzazione è un'importante tecnica di machine learning che aiuta a migliorare le prestazioni dei modelli, evitando l'overfitting sui dati di addestramento.
Le regolarizzazioni L1 e L2 sono le tecniche più utilizzate per questo scopo. L'implementazione di queste tecniche è relativamente semplice in Python grazie alle librerie come scikit-learn e NumPy.
Nel nostro esempio abbiamo visto come la regolarizzazione influisca sulla performance del modello di regressione logistica e come il valore di C influisca sulla regolarizzazione stessa. Abbiamo anche esaminato come i coefficienti dei modelli cambiano al variare del valore di C e come la regolarizzazione L1 e L2 influisce in modo diverso sui coefficienti del modello.
Commenti dalla community"
https://www.diariodiunanalista.it/posts/rete-convolutiva-su-mnist/,"Le reti convolutive vengono utilizzate per processare dati organizzati a griglia come ad esempio immagini e video.
L'operazione di convoluzione è importante perché permette di trovare parti dell'immagine interessanti come ad esempio i contorni degli oggetti di un immagine.
Queste reti godono delle seguenti proprietà:
- Condivisione dei parametri (parameter sharing), implica l'utilizzo dello stesso parametro per più di un unità nella stessa rete
- Interazioni sparse (sparse interactions), ogni input influenza un numero limitato di output e ogni output è influenzato da un numero limitato di input. Solitamente ogni input influenza tutti gli output e tutti gli output sono influenzati da tutti gli input
- Rappresentazioni equivarianti (equivariant representations), l'operazione di convoluzione è invariante per traslazione (conseguenza del parameter sharing).
Il dataset MNIST
Il dataset MNIST è diventato un punto di riferimento standard per l'apprendimento automatico, la classificazione e i sistemi di computer vision.
Questo è stato derivato da un dataset più ampio noto come MNIST Special Database 19, che contiene cifre e lettere minuscole e maiuscole scritte a mano.
Composto da immagini di numeri da 0 a 9 scritti a mano ed è già suddiviso in training set e test set rispettivamente costituiti da 60000 e 10000 elementi.
Passiamo adesso al codice:
import tensorflow as tf
from keras.datasets import mnist
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from keras.utils import to_categorical
from sklearn.metrics import accuracy_score,f1_score,ConfusionMatrixDisplay,confusion_matrix
(x_train,y_train),(x_test,y_test)=mnist.load_data()
print(""Numero di esempi di train:"",len(x_train))
print(""Numero esempi di test:"",len(x_test))
print(""Dimensione immagini:"",x_train[0].shape)
print(""Numero di classi:"",len(np.unique(y_train)))
#visualizziamo la prima immagine per ogni classe del train test
for i in range(10):
index=np.where(y_train==i)[0][0]
img=x_train[index]
plt.figure()
plt.axis(""off"")
plt.imshow(img,cmap=""gray"")
Numero di esempi di train: 60000
Numero esempi di test: 10000
Dimensione immagini: (28, 28)
Numero di classi: 10
Prima di tutto importiamo il MNIST dai dataset presenti sulla libreria keras assegnando i nomi ai nostri train e test.
Successivamente verifichiamo il numero di elementi per il train ed il test, visualizziamo la dimensione delle immagini che dovrebbe essere 28 x 28 ed infine il numero di classi che deve corrispondere a 10 dato che abbiamo numeri da 0 a 9.
Adesso prima di passare allo sviluppo della rete neurale, dobbiamo fare si che i nostri dati siano nel corretto formato affinché tutto possa funzionare in modo appropriato.
La preparazione del dato segue questi step
- Normalizzazione
- Reshaping
- Codifica one-hot della variabile target
Si normalizza il test dividendo ogni pixel per il massimo valore dei pixel stessi, in questo caso il valore massimo sarà 255 perché ci troviamo con una scala di grigi dove lo 0 indica il nero e 255 il bianco.
A questo punto aggiungo una quarta dimensione pari a 1 la quale rappresenta il canale.
Per concludere le etichette di y_train vengono convertite alla codifica one-hot per ottenere un formato categoriale.
#normalizziamo dividendo ogni pixel per il massimo valore dei pixel in questo caso sarà 255 perchè abbiamo una scala di grigi dove 0 è il nero e 255 il bianco
x_train=x_train/x_train.max()
x_test=x_test/x_test.max()
#aggiungo una quarta dimensione pari a 1 che rappresenta il canale. Dato che sono in scala di grigio
x_train=x_train.reshape(x_train.shape[0],x_train.shape[1],x_train.shape[2],1)
x_test=x_test.reshape(x_test.shape[0],x_test.shape[1],x_test.shape[2],1)
print(x_train.shape)
print(x_test.shape)
#One-hot encoding
y_train_cat=to_categorical(y_train)
(60000, 28, 28, 1)
(10000, 28, 28, 1)
Definizione dell'architettura della rete neurale
Ora i dati sono pronti per essere processati dalla rete neurale. Passiamo alla scrittura del modello usando Keras e TensorFlow, sfruttando l'API funzionale.
Se vuoi saperne di più di Keras, TensorFlow e la sua API di leggere il seguente articolo che introduce agli argomenti menzionati
Guardiamo il codice.
# importiamo le librerie essenziali
from keras.models import Sequential
from keras.layers import Dense,Conv2D,Flatten,Dropout,MaxPooling2D
from keras.losses import CategoricalCrossentropy
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping
from tensorflow import keras
K.clear_session()
# definiamo il modello in maniera funzionale
cnn= Sequential()
cnn.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu', input_shape=(28, 28, 1)))
cnn.add(MaxPooling2D(strides=2))
cnn.add(Conv2D(filters=48, kernel_size=(5,5), padding='valid', activation='relu'))
cnn.add(MaxPooling2D(strides=2))
cnn.add(Flatten())
cnn.add(Dense(256, activation='relu'))
cnn.add(Dense(84, activation='relu'))
cnn.add(Dense(10, activation='softmax'))
# definizione dell'ottimizzatore
opt=Adam(learning_rate=1e-3) #algoritmo di ottimizzazione, learning rate indica il tasso di apprendimento
# compiliamo il modello prima dell'addestramento
cnn.compile(optimizer=opt,
loss='categorical_crossentropy', #loss function
metrics=['accuracy'])
print(cnn.summary())
keras.utils.plot_model(cnn, ""model.png"", show_shapes=True)
La parte finale del codice genera un grafico che riassume l'architettura del modello in questo modo.
Spieghiamo ora le diverse parti di codice, passo dopo passo.
Primo blocco convolutivo
Questo strato è uno dei blocchi fondamentali di una CNN. Il primo strato convoluzionale ha 32 filtri con un kernel di dimensione 5x5 (può anche essere 3x3).
Il kernel è una matrice di valori che scorre sull'immagine per eseguire l'operazione di convoluzione, ovvero moltiplica i valori dei pixel dell'immagine corrispondenti ai valori del kernel e ne calcola la somma.
Questo processo di convoluzione viene ripetuto in tutta l'immagine per produrre l'output dell'operazione.
La funzione di attivazione non lineare ReLU (Rectified Linear Unit) per l'estrazione delle caratteristiche, se il valore di input è maggiore o uguale a zero, la funzione ReLU restituisce l'input, altrimenti restituisce zero.
Quindi, la funzione ReLU ""attiva"" l'output solo se l'input è positivo o zero, altrimenti lo ""disattiva"" impostandolo a zero.
Il parametro
padding=""same"" indica che vengono aggiunti degli zero ai bordi dell'immagine in modo che l'output abbia la stessa dimensione dell'input e rendere possibile l'operazione di convoluzione su ogni posizione dell'input.
MaxPooling2D
Questo strato riduce le dimensioni dell'immagine mantenendo le informazioni più rilevanti, l'operazione viene eseguita selezionando il valore massimo in una finestra di dimensioni predefinite e spostando tale finestra in modo non sovrapposto sull'immagine.
L'argomento
strides=2 indica che il max pooling viene applicato con uno spostamento di 2 pixel in orizzontale e verticale.
Secondo blocco convolutivo
Qui viene aggiunto un altro strato convoluzionale con 48 filtri con kernel di dimensione 5x5 e funzione di attivazione ReLU. Il parametro
padding=""valid"" indica che la dimensione dell'output diminuisce ad ogni layer.
Appiattimento (flatten)
Questo strato è utilizzato per appiattire l'output del secondo blocco convolutivo in modo che possa essere collegato ai successivi strati densamente connessi.
Strati densamente connessi
Vengono aggiunti tre strati densamente connessi.
Il primo ha 256 neuroni con attivazione ReLU, seguito da un secondo strato con 84 neuroni con attivazione ReLU.
Infine, l'ultimo strato ha 10 neuroni con attivazione softmax, che è utilizzato per l'output della classificazione in 10 classi (considerando che si tratta di un problema di classificazione con 10 possibili etichette).
Softmax è una funzione di attivazione utilizzata comunemente nell'ultimo strato delle reti neurali per la classificazione multiclasse. Viene utilizzata per convertire gli output numerici di una rete neurale in probabilità, consentendo di ottenere una distribuzione di probabilità su più classi.
Opera su un vettore di numeri reali e restituisce un nuovo vettore in cui ogni elemento è una probabilità compresa tra 0 e 1, e la somma di tutti gli elementi.
Algoritmo di ottimizzazione
Adam, acronimo di ""Adaptive Moment Estimation"", è un algoritmo di ottimizzazione ampiamente utilizzato per l'addestramento di reti neurali.
È noto per la sua efficacia nel velocizzare la convergenza dell'addestramento e nel superare alcune limitazioni di altri algoritmi di ottimizzazione.
L'idea principale di Adam è combinare concetti di altri due ottimizzatori molto popolari: RMSprop e Momentum.
Questa combinazione rende Adam in grado di adattare il tasso di apprendimento per ogni parametro della rete in base ai gradienti storici e alla varianza dei gradienti.
L'algoritmo Adam è molto popolare nell'addestramento di reti neurali grazie alla sua combinazione di efficienza computazionale, adattabilità del tasso di apprendimento e robustezza rispetto a diverse configurazioni di reti neurali. È spesso il metodo di scelta per l'ottimizzazione in molti problemi di deep learning.
Loss function
La categorical cross entropy è una funzione di costo (loss function) comunemente utilizzata per problemi di classificazione multiclasse, in cui le etichette target sono rappresentate in formato one-hot encoding.
Quando si addestra una rete neurale per una classificazione multiclasse, l'obiettivo è quello di far sì che il modello produca delle probabilità predette per ciascuna classe e si avvicini il più possibile alle etichette target rappresentate in formato one-hot.
La categorical cross entropy misura la discrepanza tra queste probabilità predette e le etichette target, aiutando il modello a imparare ad adattarsi e a produrre predizioni accurate.
Addestramento del modello
Lanciamo l'addestramento con il pezzo di codice qui in basso.
import time
#definiamo callback
keras_callbacks = [
EarlyStopping(monitor='val_loss', patience=5, verbose=1, min_delta=0.001)#quantità minima di miglioramento richiesta per considerare il valore della perdita come un miglioramento
]
start_time = time.time() #registra tempo di inizio
history = cnn.fit(x_train, y_train_cat, epochs=100, batch_size=64, verbose=1,
validation_split=0.2, callbacks=keras_callbacks)
end_time = time.time() ##registra tempo di fine
Epoch 1/100
750/750 [==============================] - 87s 113ms/step - loss: 0.1675 - accuracy: 0.9498 - val_loss: 0.0583 - val_accuracy: 0.9811
Epoch 2/100
750/750 [==============================] - 82s 109ms/step - loss: 0.0479 - accuracy: 0.9851 - val_loss: 0.0426 - val_accuracy: 0.9867
Epoch 3/100
750/750 [==============================] - 84s 112ms/step - loss: 0.0325 - accuracy: 0.9901 - val_loss: 0.0410 - val_accuracy: 0.9893
Epoch 4/100
750/750 [==============================] - 81s 108ms/step - loss: 0.0248 - accuracy: 0.9921 - val_loss: 0.0445 - val_accuracy: 0.9874
Epoch 5/100
750/750 [==============================] - 80s 107ms/step - loss: 0.0200 - accuracy: 0.9940 - val_loss: 0.0395 - val_accuracy: 0.9878
Epoch 6/100
750/750 [==============================] - 82s 110ms/step - loss: 0.0151 - accuracy: 0.9951 - val_loss: 0.0576 - val_accuracy: 0.9861
Epoch 7/100
750/750 [==============================] - 80s 106ms/step - loss: 0.0139 - accuracy: 0.9952 - val_loss: 0.0432 - val_accuracy: 0.9899
Epoch 8/100
750/750 [==============================] - 80s 107ms/step - loss: 0.0099 - accuracy: 0.9970 - val_loss: 0.0507 - val_accuracy: 0.9878
Epoch 9/100
750/750 [==============================] - 82s 110ms/step - loss: 0.0106 - accuracy: 0.9965 - val_loss: 0.0470 - val_accuracy: 0.9891
Epoch 10/100
750/750 [==============================] - 81s 108ms/step - loss: 0.0107 - accuracy: 0.9964 - val_loss: 0.0463 - val_accuracy: 0.9892
Epoch 10: early stopping
Grazie alle callback inserite nel fit della CNN possiamo eseguire azioni specifiche durante l'addestramento.
L'EarlyStopping interrompe l'addestramento se il valore della funzione di perdita sulla validazione (val_loss) non migliora per un numero specificato di epoche consecutive (in questo caso ho selezionata una tolleranza di 5 epoche con patience=5), il che significa che l'addestramento si fermerà.
Infine, registriamo il tempo di inizio e fine dell'addestramento per calcolarne la durata totale. Questo può essere utile per monitorare quanto tempo impiega il modello a convergere o per confrontare i tempi di addestramento tra diverse configurazioni di modelli.
Creo un dataframe salvando i risultati migliori del modello per: accuracy, validation accuracy, loss, validation loss e tempo. Di seguito ecco i risultati
acc val_acc loss val_loss computation_time
0.994 0.9878 0.02 0.0395 13.48
Batch size
Il parametro del batch size (dimensione del batch) è un iperparametro importante da considerare durante l'addestramento di una rete neurale.
Indica il numero di campioni di addestramento che vengono passati al modello prima che i gradienti vengano calcolati e i pesi del modello vengano aggiornati.
Il batch size influisce su diversi aspetti dell'addestramento e può avere un impatto sulle prestazioni del modello.
Le dimensioni del batch più comuni sono 64, 128 e 256. Ecco alcune considerazioni generali su ciascuna di queste opzioni:
- Batch Size 64
Pro: Utilizzare un batch size più piccolo, come 64, può portare a un aggiornamento più frequente dei pesi del modello.
Ciò può essere utile quando si lavora con dataset di grandi dimensioni e risorse computazionali limitate, poiché l'aggiornamento più frequente dei pesi può consentire di utilizzare meno memoria.
Contro: Batch size più piccoli possono rallentare il processo complessivo di addestramento, poiché è necessario calcolare più volte i gradienti e aggiornare i pesi.
2. Batch Size 128
Pro: Un batch size di 128 offre un buon equilibrio tra il vantaggio di aggiornamenti più frequenti dei pesi e un addestramento più veloce rispetto a batch size più piccoli.
Contro: Potrebbe richiedere più memoria rispetto a un batch size di 64, ma solitamente è ancora gestibile con risorse computazionali moderate.
3. Batch Size 256
Pro: Un batch size di 256 può accelerare ulteriormente l'addestramento rispetto a dimensioni del batch più piccole, poiché l'aggiornamento dei pesi avviene ancora meno frequentemente, consentendo di eseguire più calcoli contemporaneamente.
Contro: Potrebbe richiedere più memoria rispetto a batch size più piccoli e potrebbe non essere adatto a tutte le architetture di rete o tipi di dataset.
La scelta della dimensione del batch dipenderà da vari fattori, tra cui la dimensione del dataset, la complessità della rete neurale, le risorse computazionali disponibili e la natura del problema di apprendimento.
Inoltre, è comune eseguire alcuni esperimenti con diverse dimensioni di batch per determinare quale funziona meglio per il tuo specifico caso d'uso.
Risultati
Avendo affrontato il tema della batch size, riportiamo qui una tabella con le varie performance divise proprio per batch size.
accuracy val accuracy loss val loss computation_time
64 0.9940 0.9878 0.0200 0.0395 13.58
128 0.9942 0.9898 0.0176 0.0364 14.50
256 0.9951 0.9904 0.0151 0.0362 16.31
Visualizziamo le curve di apprendimento, plottando la loss per le epoche di apprendimento.
val_losses=history.history[""val_loss""]
train_losses=history.history[""loss""]
epochs = range(1, len(val_losses) + 1)
plt.figure()
plt.title(""Training loss"")
plt.plot(epochs,val_losses,c=""red"",label=""Validation"")
plt.plot(epochs,train_losses,c=""orange"",label=""Training"")
plt.xlabel(""Epochs"")
plt.ylabel(""Cross entropy"")
plt.legend()
Infine, creiamo una matrice di confusione per mostrare, in formato mappa di calore, le previsioni per ogni classe.
import seaborn as sns
prob=cnn.predict(x_test)
#Contiene le probabilità di ogni classe predetta per i campioni nel set di #test
y_pred=np.argmax(prob,axis=-1)
#contiene un array unidimensionale con le etichette predette (indici delle #classi) per ogni campione nel set di test
#La funzione np.argmax() restituisce gli indici dei valori massimi lungo un #asse specificato,
# in questo caso, axis = -1 indica che l'operazione viene eseguita #sull'ultimo asse, cioè l'asse delle classi
#quindi, per ogni campione nel set di test, np.argmax(prob, axis=-1) #restituisce l'indice della classe con la probabilità massima
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
#annot = T aggiunge le etichette dei valori nella mappa di calore, fmt='d' #specifica che i valori devono essere visualizzati come interi, e #cmap='Blues' imposta la scala dei colori come sfumature di blu
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()
Vediamo come la nostra CNN prevede abbastanza bene e non emergono particolari difficoltà nell'identificare un numero piuttosto che un altro.
Conclusioni
L'utilizzo delle reti neurali convoluzionali (CNN) con il dataset MNIST ha dimostrato di essere una combinazione molto potente per il riconoscimento delle cifre scritte a mano.
Grazie alla loro capacità di apprendere automaticamente le caratteristiche rilevanti dalle immagini, le CNN hanno portato a risultati eccezionali nella classificazione di cifre da 0 a 9.
Durante il percorso di questo articolo, abbiamo esplorato le principali componenti delle reti convoluzionali, compresi i layer di convoluzione, max pooling e le funzioni di attivazione come ReLU e softmax.
Abbiamo visto come queste reti si adattino perfettamente al riconoscimento delle immagini, rivelando un'efficacia sorprendente nel catturare pattern complessi e rappresentare le immagini con elevate capacità discriminative.
Il dataset MNIST si è rivelato un eccellente punto di partenza per apprendere e sperimentare con le CNN. La sua comprensibilità e la sua dimensione relativamente ridotta hanno rappresentato un'opportunità per acquisire familiarità con la progettazione di reti neurali e l'ottimizzazione dei parametri.
Inoltre, abbiamo esplorato l'importanza delle callback nel processo di addestramento del modello, in particolare EarlyStopping che ci ha permesso di evitare l'overfitting e migliorare l'efficienza dell'addestramento.
Commenti dalla community"
https://www.diariodiunanalista.it/posts/selezione-delle-feature-con-boruta/,"La fase di selezione delle feature presenti nel set di addestramento è di fondamentale importanza in qualsiasi progetto di machine learning.
Spiegherò l'algoritmo Boruta, in grado di creare una classifica delle nostre caratteristiche, dalla più importante alla meno impattante per il nostro modello. Boruta è semplice da usare e più potente delle tradizionali tecniche usate per lo stesso compito.
Introduzione
Iniziamo scrivendo che Boruta non è un algoritmo a se stante, ma estende l'algoritmo Random Forest. Infatti, il nome Boruta deriva dal nome dello spirito della foresta nella mitologia slava. Per comprendere come funziona l'algoritmo facciamo una introduzione al Random Forest.
Il Random Forest si basa sul bagging - crea molti campioni casuali del set di addestramento e addestra un modello statistico diverso per ognuno di essi.
Per un task di classificazione il risultato è la maggioranza dei voti da parte dei modelli, mentre per un task di regressione il risultato è la media dei vari modelli. La differenza tra il bagging canonico e quello fatto nel Random Forest è che quest'ultimo utilizza sempre e solo modelli di albero di decisione (decision trees).
Per ogni campione preso in considerazione, l'albero decisionale prende in considerazione un set limitato di feature. Questo permette all'algoritmo Random Forest di poter stimare l'importanza di ogni feature, poiché traccia l'errore nelle predizioni proprio in base allo split di feature considerate.
Prendiamo in considerazione un task di classificazione. Il modo in cui Random Forest stimi l'importanza delle feature funziona in due fasi. Per prima cosa, ogni albero decisionale crea una predizione e questa viene memorizzata. Secondo, i valori di certe feature vengono permutati casualmente attraverso i vari esempi e lo step precedente viene ripetuto, andando a tracciare il risultato delle predizioni nuovamente.
L'importanza di una feature per un singolo albero decisionale è calcolato come la differenza delle performance tra il modello che utilizza le feature originali contro quello che utilizza le feature permutate diviso per il numero di esempi nel set di addestramento. L'importanza di una feature è la media delle misurazioni tra tutti gli alberi per quella feature.
Quello che non viene fatto durante questa procedura è calcolare gli z-score per ogni feature. Qui entra in gioco Boruta.
L'Algoritmo Boruta
L'idea di Boruta è abbastanza semplice: per tutte le feature presenti nel dataset originale, ne andiamo a creare delle copie casuali (chiamate shadow feature - caratteristiche fantasma) e addestrare dei classificatori che si basano su questo dataset esteso. Per comprendere l'importanza di una feature, la compariamo a tutte le shadow feature generate. Solo le feature che sono statisticamente più importanti delle feature sintetiche vengono mantenute poiché contribuiscono di più alle performance del modello.
Ecco gli step principali che effettua Boruta per la selezione delle feature
- Crea una copia delle feature del set di addestramento e le unisce alle feature originali
- Crea permutazioni casuali sulle feature sintetiche per rimuovere ogni tipo di correlazione tra queste e la variabile target y - in pratica, queste feature sintetiche sono combinazioni randomizzate della feature originale dalla quale derivano
- Si comporta come il Random Forest: le feature sintetiche sono randomizzate a ogni nuova iterazione
- Per ogni nuova iterazione, calcola lo z-score di tutte le feature originali e di quelle sintetiche. Una feature è considerata rilevante se la sua importanza è più alta della importanza massima di tutte le feature sintetiche
- Applica un test statistico per tutte le feature originali e traccia i suoi risultati. L'ipotesi nulla è che l'importanza di una feature sia uguale alla importanza massima delle feature sintetiche.
Il test statistico va a testare l'uguaglianza tra le feature originali e quelle sintetiche. L'ipotesi nulla viene respinta quando l'importanza di una feature è significativamente più alta o più bassa di una di quelle delle feature sintetiche
- Rimuove le feature che sono considerate non importanti dal dataset, sia dal dataset originale che da quello sintetico
- Ripete tutti gli step per un numero n di iterazioni finché tutte le feature sono rimosse o considerate importanti
Va notato che Boruta agisce come una euristica: vale a dire che non ci sono garanzie della sua performance. È dunque consigliabile lanciare il processo più volte e valutarne i risultati.
Esempio di utilizzo in Python
Vediamo come funziona Boruta in Python con la libreria dedicata. Useremo il dataset
load_diabetes() di Sklearn.datasets per testare Boruta su un problema di regressione.
Il feature set X è composto dalle variabili
- age (età in anni)
- sex (sesso)
- bmi (body mass index, indice di massa corporea)
- bp (pressione sanguigna media)
- s1 (tc, colesterolo totale)
- s2 (ldl, low-density lipoproteins)
- s3 (hdl, high-density lipoproteins)
- s4 (tch, colesterolo totale / HDL)
- s5 (ltg, logaritmo del livello di trigliceridi)
- s6 (glu, livello di zucchero nel sangue)
il target y è la progressione del diabete registrata nel tempo.
from sklearn.datasets import load_diabetes
from sklearn.ensemble import RandomForestRegressor
from boruta import BorutaPy
import pandas as pd
import numpy as np
# da sklearn, carichiamo il dataset del diabete
X, y = load_diabetes(return_X_y=True, as_frame=True)
# inizializziamo un modello Random Forest
model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)
# inizializziamo Boruta
feat_selector = BorutaPy(
verbose=2,
estimator=model,
n_estimators='auto',
max_iter=10 # numero di iterazioni da fare
)
# addestriamo Boruta
# N.B.: X e y devono essere numpy array
feat_selector.fit(np.array(X), np.array(y))
# stampiamo supporto e ranking per ogni feature
print(""\n------Support e Ranking per ogni feature------"")
for i in range(len(feat_selector.support_)):
if feat_selector.support_[i]:
print(""Passa il test: "", X.columns[i],
"" - Ranking: "", feat_selector.ranking_[i])
else:
print(""Non passa il test: "",
X.columns[i], "" - Ranking: "", feat_selector.ranking_[i])
Lanciando lo script vedremo in terminale come Boruta stia costruendo le sue inferenze
Il nostro report stampa questo risultato, molto comprensibile
Vediamo come bmi, bp, s5 e s6 siano le feature da usare poiché quelle più importanti. Boruta le ha identificate e ci ha aiutato nella selezione delle feature per il nostro modello.
Per filtrare il nostro dataset e selezionare solo le feature che per Boruta sono importanti basta fare
feat_selector.transform(np.array(X)) che restituirà un array Numpy.
Ora siamo pronti per fornire al nostro modello RandomForestRegressor un set di selezionato di feature X. Addestriamo il modello e stampiamo il Root Mean Squared Error (RMSE).
print(""\n------Feature selezionate------\n"")
print(X_filtered)
# set di feature selezionate da Boruta
X_filtered = feat_selector.transform(np.array(X))
# addestriamo il modello
model.fit(X_filtered, y)
# calcoliamo la predizione
predictions = model.predict(X_filtered)
# creiamo un dataframe con le predizioni e i valori reali
df = pd.DataFrame({'pred': predictions, 'observed': y})
# stampiamo il dataframe
print(""\n------Predizioni e valori reali------\n"")
print(df)
# calcoliamo RMSE
mse = ((df['pred'] - df['observed']) ** 2).mean()
rmse = np.sqrt(mse)
print(""\n------RMSE------\n"", round(rmse, 3))
Ecco i risultati dell'addestramento
Il Codice
Il codice completo qui in formato copia-incolla
from sklearn.datasets import load_diabetes
from sklearn.ensemble import RandomForestRegressor
from boruta import BorutaPy
import pandas as pd
import numpy as np
# da sklearn, carichiamo il dataset del diabete
X, y = load_diabetes(return_X_y=True, as_frame=True)
# inizializziamo un modello Random Forest
model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)
# inizializziamo Boruta
feat_selector = BorutaPy(
verbose=2,
estimator=model,
n_estimators='auto',
max_iter=10, # numero di iterazioni da fare
random_state=42,
)
# addestriamo Boruta
# N.B.: X e y devono essere numpy array
feat_selector.fit(np.array(X), np.array(y))
# stampiamo supporto e ranking per ogni feature
print(""\n------Support e Ranking per ogni feature------\n"")
for i in range(len(feat_selector.support_)):
if feat_selector.support_[i]:
print(""Passa il test: "", X.columns[i],
"" - Ranking: "", feat_selector.ranking_[i], ""✔️"")
else:
print(""Non passa il test: "",
X.columns[i], "" - Ranking: "", feat_selector.ranking_[i], ""❌"")
# set di feature selezionate da Boruta
X_filtered = feat_selector.transform(np.array(X))
print(""\n------Feature selezionate------\n"")
print(X_filtered)
# addestriamo il modello
model.fit(X_filtered, y)
# calcoliamo la predizione
predictions = model.predict(X_filtered)
# creiamo un dataframe con le predizioni e i valori reali
df = pd.DataFrame({'pred': predictions, 'observed': y})
# stampiamo il dataframe
print(""\n------Predizioni e valori reali------\n"")
print(df)
# calcoliamo RMSE
mse = ((df['pred'] - df['observed']) ** 2).mean()
rmse = np.sqrt(mse)
print(""\n------MSE------\n"", round(rmse, 3))
Commenti dalla community"
https://www.diariodiunanalista.it/posts/servire-modelli-di-machine-learning-con-fastapi/,"Hai creato un modello predittivo sul tuo dataset. Questo performa bene sia in cross-validazione che sul test set.
Molto bene.
Ma ora che ci fai?
Un modello di machine learning ha poca utilità se non viene usato da qualcuno.
Il nostro obiettivo come data scientist è quello di creare buoni modelli e condividerli con il nostro team o clienti.
Nasce quindi l’esigenza di una interfaccia al nostro software che permetta ai nostri utenti di collegarsi e sfruttare i dati o funzioni che mettiamo a disposizione.
Questa tecnologia si chiama API (application programming interface) ed è da tempo sfruttata dai programmatori in praticamente tutte le industry per creare servizi web.
In pratica, un'API fornisce un set di regole e protocolli standardizzati che consentono a un'applicazione di accedere alle funzionalità di un'altra applicazione o servizio e di utilizzarle in modo sicuro e affidabile.
In questo articolo vedremo come creare un modello di machine learning e servirlo attraverso una API.
Grazie a questa conoscenza potrai portarti avanti nella pipeline di machine learning e servire i tuoi modelli (ma anche altre funzionalità volendo) grazie a Python e a FastAPI.
FastAPI è una libreria incredibile, che ha sconvolto lo spazio della data science negli ultimi anni grazie alla sua incredibile usabilità e velocità. Inoltre, la sua documentazione è molto ricca e vasta.
- Modellare un dataset giocattolo con LightGBM
- Scrivere funzioni e classi per creare predizioni su dati non visti
- Servire il modello via REST grazie a FastAPI
Sei pronto? Iniziamo!
Il dataset
Il dataset che userò per questo esempio sarà il California housing dataset di Sklearn, su cui è possibile leggere di più qui
L'obiettivo quindi è creare un semplice modello di regressione della variabile target
MedInc cioè il valore mediano del reddito familiare annuo per i distretti della California, espresso in centinaia di migliaia di dollari ($ 100.000).
Il feature set è composto da 7 variabili:
HouseAge: età mediana di una casa nel distretto
AveRooms: numero medio di camere per un nucleo familiare
AveBedrms: numero medio di camere da letto per nucleo familiare
Population: popolazione del distretto
AveOccup: numero medio di membri di un nucleo familiare
Latitude: latitudine
Longitude: longitudine
Il dataset è composto da 20640 esempi e non sono presenti valori vuoti.
Quando si lavora con un dataset reale questo non è mai il caso e il preprocessing sarà praticamente sempre d'obbligo.
Modellazione del dato con LightGBM
Poiché il focus di questo articolo è mostrare come usare FastAPI per servire un qualsivoglia modello di apprendimento automatico, salterò direttamente alla fase di modeling del dataset.
Questo significa che non scriverò di:
- come impostare un ambiente di sviluppo per il machine learning
- come strutturare un progetto di machine learning
- come ripartire i dati per la cross-validazione
In un contesto reale si seguirebbe tutta la pipeline di machine learning, che include preprocessing, selezione del modello, cross-validazione e tuning degli iperparametri.
Se vuoi un percorso introduttivo a questi argomenti e molti altri, ti linko la pagina hub di riferimento
Userò LightGBM perché è un modello estremamente efficace e veloce per dati tabellari.
La prima cosa da fare è installarlo, poiché non è presente in Sklearn (anche se è disponibile una API dedicata)
pip install lightgbm
A questo punto creiamo uno script o notebook nella nostra cartella di progetto e iniziamo con questo codice
import pandas as pd
from sklearn.datasets import fetch_california_housing
data = fetch_california_housing() # importiamo il dataset da sklearn
df = pd.DataFrame(data.data, columns=data.feature_names) # inseriamolo in un dataframe pandas
df.head()
Ora dividiamo in set di addestramento e test
from sklearn.model_selection import train_test_split
X = df.drop(""MedInc"", axis=1)
y = df['MedInc']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(X_train.shape, X_test.shape)
>>>
(16512, 7) (4128, 7)
Ora siamo pronti ad addestrare un regressore LightGBM.
import lightgbm
from sklearn import metrics
# addestramento del modello
lgbm_model = lightgbm.LGBMRegressor()
lgbm_model.fit(X_train, y_train)
# creazione predizione
y_pred = lgbm_model.predict(X_test)
# valutazione del modello
mse = metrics.mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
print(f""Root Mean Squared Error: {rmse:.2f}"")
>>>
Root Mean Squared Error: 0.81
Salviamo il modello in formato pickle così da poterlo utilizzare nell'API.
import pickle
model_filename = 'model.pkl'
with open(model_filename, 'wb') as model_file:
pickle.dump(lgbm_model, model_file)
Con il modello pronto per fare predizioni possiamo ora creare la nostra API.
Ti piace quello che stai leggendo?
Iscriviti al blog per rimanere sempre aggiornato sui contenuti di settore. No pubblicità. No spam.
Solo contenuto utile per la tua carriera nel ML e DS.
Puoi rimuovere l'iscrizione quando vuoi seguendo il link nella email.
Creazione dell'API con FastAPI
Creiamo un file chiamato
api.py nella nostra cartella. Sottolineo che deve essere un file .py e non .ipynb perché dovremmo accendere un server che richiede un terminale attivo.
Installiamo FastAPI e Uvicorn. Uvicorn è una dipendenza di FastAPI ed è la libreria che creerà il server.
pip install fastapi uvicorn
Per lo script api.py ci serviranno diverse cose
- Una classe Pydantic che manterrà il modello dati per il feature set
- Una funzione dedicata al caricamento del modello e alla predizione
- Un contesto per il ciclo vitale del modello di machine learning
- Un endpoint da chiamare via browser
Sembra complicato, ma in realtà è molto semplice.
Vediamo step-by-step cosa fare.
Pydantic per creare il modello dati
Perché serve un modello dati? Perché FastAPI funziona molto bene con Pydantic.
Se non lo sai Pydantic è una libreria molto potente per la gestione dei modelli dati. Permette di configurarli e di fare validazione su diverse proprietà.
Ti invito a leggere l'articolo qui per ulteriori dettagli
Creare un modello dati è semplice. Sempre in api.py, iniziamo lo script così
from pydantic import BaseModel
class FeatureSet(BaseModel):
HouseAge: float
AveRooms: float
AveBedrms: float
Population: float
AveOccup: float
Latitude: float
Longitude: float
Questa classe verrà fornita a FastAPI per interpretare correttamente i dati in input e la loro struttura.
Caricamento del modello e predizione
Creiamo subito dopo il modello dati una funzione che esporrà la funzione
.predict del modello. Usiamo pickle per caricare il modello.
def medinc_regressor(x: dict) -> dict:
with open(""model.pkl"", 'rb') as model_file:
loaded_model = pickle.load(model_file)
x_df = pd.DataFrame(x, index=[0])
res = loaded_model.predict(x_df)[0]
return {""prediction"": res}
La funzione
medinc_regressor sarà responsabile di ricevere un feature set
x e di restituire una risposta del modello.
Creazione di un ciclo vitale per il modello
Qui è dove FastAPI aiuta noi programmatori di modelli di ML.
Guardiamo il codice
ml_models = {}
@asynccontextmanager
async def ml_lifespan_manager(app: FastAPI):
# caricamento della funzione dedicata alla predizione
ml_models[""medinc_regressor""] = medinc_regressor
yield
# pulizia delle risorse
ml_models.clear()
A cosa serve tutto ciò?
In uno scenario reale, uno o più modelli di machine learning sono condivisi tra le varie richieste in entrata degli utenti. Non c'è una associazione 1 modello = 1 utente.
Immaginiamo che il caricamento del modello possa richiedere del tempo perché ad esempio deve leggere molti dati dal disco...è spontaneo pensare che non vogliamo che questa cosa accada per ogni singola richiesta.
Quello che il lifespan manager fa è caricare il modello prima che le richieste vengano gestite, ma solo subito prima che l'applicazione inizi a ricevere richieste e non durante il caricamento del codice.
Se può sembrarti complicato non preoccuparti: sappi che FastAPI ci aiuta molto proprio a gestire dettagli del genere che in un contesto reale hanno un impatto importante.
In ogni caso ti linko qui la documentazione ufficiale di FastAPI che copre il lifespan manager.
Creazione di un endpoint da chiamare via browser
Ora definiamo l'oggetto principale di questo script: l'app FastAPI e un endpoint da chiamare via richiesta POST.
app = FastAPI(lifespan=ml_lifespan_manager)
@app.post(""/predict"")
async def predict(feature_set: FeatureSet):
return ml_models[""medinc_regressor""](feature_set.dict())
FastAPI accetta il lifespan manager creato sopra e inizierà a gestire correttamente il flusso di richieste.
Subito dopo aver creato l'applicazione FastAPI, c'è una funzione asincrona chiamata
predict che evoca il modello come negli snippet di codice precedenti.
La dicitura
@app.post(""/predict"") informa FastAPI che vogliamo creare una rotta chiamata
/predict che accetta richieste di tipo POST.
predict accetta l'oggetto Pydantic creato sopra. Qui è dove avviene la validazione del dato. Viene quindi passato il feature set in forma dizionario al modello di machine learning per la predizione.
Ce l'hai fatta a leggere fino a qui. Congratulazioni! Ora riassumiamo quanto fatto finora.
L'intero script sarà questo
import pickle
import pandas as pd
import numpy as np
from fastapi import FastAPI
from contextlib import asynccontextmanager
from pydantic import BaseModel
class FeatureSet(BaseModel):
""Modello dati basato su Pydantic""
HouseAge: float
AveRooms: float
AveBedrms: float
Population: float
AveOccup: float
Latitude: float
Longitude: float
def medinc_regressor(x: dict) -> dict:
""""""Funzione dedicata alla predizione""""""
with open(""model.pkl"", 'rb') as model_file:
loaded_model = pickle.load(model_file)
x_df = pd.DataFrame(x, index=[0])
res = loaded_model.predict(x_df)[0]
return {""prediction"": res}
# Creazione di un context manager per la gestione del ciclo di vita del modello
ml_models = {}
@asynccontextmanager
async def ml_lifespan_manager(app: FastAPI):
ml_models[""medinc_regressor""] = medinc_regressor
yield
ml_models.clear()
app = FastAPI(lifespan=ml_lifespan_manager)
@app.post(""/predict"")
async def predict(feature_set: FeatureSet):
return ml_models[""medinc_regressor""](feature_set.dict())
Non rimane altro che avviare l'API. Apriamo un terminale e scriviamo
> uvicorn api:app --reload --port 8010
Uvicorn creerà un server in ascolto proprio alla porta 8010, caricando il contenuto presente nel file api.py, guardando l'applicazione chiamata
app all'interno di esso.
Se ora raggiungiamo nel browser http://127.0.0.1:8010 verremo accolti da questa schermata
Questo è normale. Andiamo su /docs per vedere una delle cose più belle di FastAPI: documentazione autogenerata.
Vediamo come ci sia una rotta POST chiamata /predict, proprio come da noi programmata poco fa.
Espandendo la riga e cliccando su ""Try it out"" è possibile inviare una richiesta per testare il tutto.
Inseriamo dei valori di test e clicchiamo su ""Execute"".
Il nostro modello di machine learning viene correttamente caricato e crea una predizione. Alla grande!
Next step
Creare una API ci permette di servire modelli non di deployarli. C'è una bella differenza.
Deployare significa metterli online.
Un server del genere, al momento, risiederebbe solamente sul nostro PC in locale. Serve quindi uno step di deployment su un server virtuale (ad esempio macchine Google o Amazon) e impostare l'API con una rotta raggiungibile ad esempio a http://miosito.com/predict.
Il deployment non è oggetto di questo articolo ma verrà trattato nel futuro e questa sezione aggiornata per linkare a tale risorsa.
Conclusioni
Ecco un sommario di ciò che hai imparato
- Cosa è una API e cosa ti permette di fare
- Come creare un semplice modello predittivo e salvarlo in formato .pkl
- Come creare una API con FastAPI, passando per diversi dettagli tecnici utili per rendere il caricamento e la predizione più efficienti.
Spero che questo articolo ti abbia aiutato ad inserire un tassello in più nella tua formazione da data scientist.
Se ci sono domande o dubbi, non esitare a commentare o a scrivermi.
Alla prossima,
Andrea
Commenti dalla community"
https://www.diariodiunanalista.it/posts/tagging-di-contenuti-con-la-logica-fuzzy-in-python/,"Con il termine logica fuzzy (fuzzy logic) si intendono tutte quelle regole e funzioni che vanno ad applicare una euristica basata sulla approssimazione della verità - la parola ""fuzzy"" in inglese si traduce come poco chiaro, sfocato. Questa parola non è scelta a caso e fa proprio riferimento al modo in cui funzionano questi ragionamenti.
Nell'informatica, la fuzzy logic vuole aiutare a fornire un grado di verità invece che la verità vera e propria. È un insieme di ragionamenti che permettono di rispondere in maniera imprecisa a domande altrettanto imprecise.
Un esempio di fuzzy logic
Poniamo che ci venga chiesto quanti anni abbiamo, ma nella forma
Sei anziano?
Guardiamo questa domanda dal punto di vista logico:
- se abbiamo 5 anni, probabilmente non possiamo essere considerati anziani
- se abbiamo 70 anni, probabilmente possiamo essere considerati anziani
- ...ma cosa rispondiamo se invece abbiamo 50 anni?
La logica fuzzy risponde in questo modo:
Si e no.
Mentre la logica tradizionale si esprime solo con due stati, vero e falso, la logica fuzzy vuole riempire quello che c'è in mezzo, andando proprio a fornire dei gradi di verità invece che dell'espressione booleana.
Tornando all'esempio appena fatto, e scomponendo il quesito per tipo di logica, avremmo queste risposte alla domanda sei anziano?
Ponendo che possiamo essere considerati anziani all'età di 75 anni,
- Logica tradizionale: VERO se ETÀ > 75; altrimenti FALSO
- Logica fuzzy: ETÀ / 75 % anziano.
Quindi se abbiamo 70 anni, allora secondo la logica fuzzy noi saremmo 93% anziani.
Per noi analisti, la logica fuzzy diventa uno strumento per associare, distinguere o raggruppare elementi che sono definiti da una metrica.
Usare la logica fuzzy per il tagging / categorizzazione
Quello che mostrerò in questo articolo è il tagging o categorizzazione di contenuto testuale (articoli di questo blog) usando la logica fuzzy in Python.
Usando la distanza di Levenshtein come sistema di valutazione, useremo la logica fuzzy per associare un articolo ad una o più categorie predefinite da noi.
La distanza di Levenshtein è una misura usata spesso per valutare la differenza tra due stringhe. Essa spiega quanti caratteri occorrono alla stringa A per diventare stringa B.
Un esempio:
Stringa A:
foo | Stringa B:
fii
Distanza di Levenshtein: 2
Occorrono infatti due cambiamenti ad A per trasformarla in B (oo -> ii).
Se il lettore è interessato al tema delle manipolazioni di stringhe e alla sua applicazione principale, che è quella dell'autocorrezione, suggerisco di leggere questo articolo che spiega come funziona un modello di autocorrezione.
Il nostro sistema di logica fuzzy si baserà su una libreria chiamata The Fuzz che è stata creata proprio per il nostro intento.
Cosa faremo?
I lettori assidui di questo blog conosceranno lo scraper di articoli che ho pubblicato del tempo fa. Userò quel piccolo software per recuperare gli articoli di Diario Di Un Analista.it e applicare su di loro dei tag predefiniti.
Con questo piccolo progetto, voglio permettere al lettore di poter taggare qualsiasi contenuto testuale in base a dei tag predefiniti.
Poiché questo approccio non si basa su dei modelli predittivi, bisogna fornire al software dei tag che vogliamo applicare ai nostri documenti.
Questo approccio ha i suoi vantaggi rispetto a quelli basati su modelli predittivi. Un modello potrebbe dedurre dei tag imprecisi, che non soddisfano i criteri da noi in mente.
Definendo i tag a monte, possiamo esser sicuri che il tagging sia coerente con la nostra cernita. Certo, possiamo e incorreremo sicuramente in errori, ma saranno più controllabili da parte nostra.
I requisiti
I requisiti saranno solo due: Pandas e The Fuzz.
pip install thefuzz pandas in terminale.
Iniziamo a importare le nostre librerie come di consueto
from thefuzz import process
import pandas as pd
Creazione dei tag
La logica che andremo ad applicare ha bisogno di un riferimento per funzionare. Per questo motivo, creeremo una lista chiamata
tags con le etichette che vogliamo applicare ai documenti.
# queste sono i tag che vogliamo applicare ai nostri documenti.
# cambiate questa lista a vostra discrezione
tags = [
""machine learning"",
""clustering"",
""carriera"",
""progetto"",
""consigli"",
""analytics"",
""deep learning"",
'nlp',
]
Caricamento del dataset
Utilizzando lo scraper di blog linkato sopra, creiamo un dataset che avrà due colonne: url e article.
# carichiamo un dataset e isoliamo i post
df = pd.read_csv('dataset.csv')
posts = df[df.url.str.contains('post')]
posts.reset_index(inplace=True, drop=True)
articles = list(posts.article)
Abbiamo il nostro corpus. Al momento di scrittura di questo pezzo stiamo intorno a 30 articoli - si tratta quindi di un corpus molto piccolo. Andrà comunque bene per il nostro esempio.
Funzione di tagging
Ora scriveremo il codice per applicare il tagging usando la logica fuzzy. L'algoritmo funziona così:
- ciclare tra i tag
- per ogni tag, usare
process.extractdi The Fuzz per estrarre un gruppo di articoli rappresentativi del tag, ordinati per punteggio
- per ogni elemento dell'output precedente, strutturare il dato in un dizionario Python e creare un dataframe Pandas
- dopo aver raccolto tutti i dataframe, unirli tutti con una concatenazione
Scriviamo la funzione
def fuzzy_tagging(tags, articles):
""""""
Questa funzione riceve in input una lista di tag predefiniti e la lista di contenuto testuale da taggare.
Restituisce un dataframe Pandas con gli articoli taggati
""""""
results = []
# ciclo nei tag
for i, tag in enumerate(tags):
d = {}
ranking = process.extract(tag, articles, limit=4)
for r in ranking:
d = {""tag"": tag, ""index"": articles.index(r[0]), ""confidence"": r[1]}
results.append(d)
# organizziamo tutto in un df pandas
raw_tags = pd.DataFrame(results)
raw_tags.set_index('index', inplace=True, drop=True)
d = {}
for i, row in raw_tags.iterrows():
if d.get(i):
if row['confidence'] >= 55: # se la soglia supera il valore di 55, aggiungere il tag a quelli esistenti se è già presente un tag
d[i] += ', ' + str(row['category'])
else:
d[i] = str(row['category'])
# creiamo il dataset finale
tags = pd.Series(d, name='tag')
tagged_df = pd.concat([posts, tags], axis=1)
return tagged_df
Applicazione del tagging
Andiamo a vedere i risultati del tagging andando ad eseguire il codice scritto
tagged_df = fuzzy_tagging(tags=tags, articles=articles)
tagged_df
Come possiamo vedere, il tagging non è perfetto ma la maggior parte dei risultati sono in effetti coerenti.
Quando una pagina appartiene a più categorie, come ad esempio la riga 3 ""Clustering delle serie temporali per la previsione del mercato azionario"" che appartiene ai tag clustering e progetto.
Ci sono anche dei NaN. Questo è normale. Un articolo potrebbe non essere associato con una soglia maggiore a quella stabilita (55 nel nostro caso) ad un certo tag. In questo caso, nessun tag viene associato al contenuto e quindi viene assegnato NaN. Modificare la soglia di confidence aiuterà ad avere più o meno risultati.
Conclusioni
Abbiamo visto come fare tagging dei contenuti in Python, usando la logica fuzzy e i dataframe di Pandas.
Questo è un piccolo progetto che però può avere ripercussioni importanti sul lavoro. Un dataset già di 200 righe può richiedere delle ore per essere taggato a mano. Questo vuole essere un approccio semplice ma efficace all'automazione di questi task.
Next step
In questo esempio ho utilizzato il body dell'articolo. È stata una scelta arbitraria, ma ad esempio si può fare la stessa cosa con i title degli articoli o con le meta description. Se questi testi sono rappresentativi dell'argomento, il nostro ragionamento potrebbe funzionare anche meglio! Provate voi :)
A presto,
Template del codice
Ecco qui l'intera codebase
from thefuzz import fuzz, process
import pandas as pd
# definiamo le categorie che vogliamo applicare
tags = [
""machine learning"",
""clustering"",
""carriera"",
""progetto"",
""consigli"",
""analytics"",
""deep learning"",
'nlp',
]
# carichiamo un dataset e isoliamo i post
df = pd.read_csv('dataset.csv')
posts = df[df.url.str.contains('post')]
posts.reset_index(inplace=True, drop=True)
articles = list(posts.article)
def fuzzy_tagging(tags, articles):
""""""
Questa funzione riceve in input una lista di tag predefiniti e la lista di contenuto testuale da taggare.
Restituisce un dataframe Pandas con gli articoli taggati
""""""
results = []
# ciclo nei tag
for i, tag in enumerate(tags):
d = {}
ranking = process.extract(tag, articles, limit=4)
for r in ranking:
d = {""tag"": tag, ""index"": articles.index(r[0]), ""confidence"": r[1]}
results.append(d)
# organizziamo tutto in un df pandas
raw_tags = pd.DataFrame(results)
raw_tags.set_index('index', inplace=True, drop=True)
d = {}
for i, row in raw_tags.iterrows():
if d.get(i):
if row['confidence'] >= 55: # se la soglia supera il valore di 55, aggiungere il tag a quelli esistenti se è già presente un tag
d[i] += ', ' + str(row['category'])
else:
d[i] = str(row['category'])
# creiamo il dataset finale
tags = pd.Series(d, name='tag')
tagged_df = pd.concat([posts, tags], axis=1)
return tagged_df
tagged_df = fuzzy_tagging(tags=tags, articles=articles)
tagged_df
Commenti dalla community"
https://www.diariodiunanalista.it/posts/tecniche-di-data-visualization-dal-grafico-allo-storytelling/,"È raro trovare qualcuno bravo a comunicare efficacemente il significato dei dati.
Sui social vedo spesso grafici che sembrano belli, puliti, chiari e che evocano il famigerato effetto wow, ma che ad una ispezione più approfondita falliscono a comunicare il significato e la giusta interpretazione da dare ai dati.
L'autore della visualizzazione si concentra su dettagli che non aiutano la comprensione di quello che si osserva, come scrivere testi descrittivi, ripetere informazioni già conosciute e dare rilevanza ad elementi non rilevanti. Il risultato è un grafico pieno di elementi pittorici, con colori sgargianti che dominano la scena e poche o nessuna indicazione che aiuta il cervello dell'osservatore a dar senso di quello che c'è su schermo.
Questo accade perché non si inserisce la visualizzazione dei dati in un framework di riferimento, e non c'è consapevolezza che la data visualization è una competenza indipendente, ma strettamente collegata, a quella di analizzare i dati stessi.
L'analista incauto, quindi, salta direttamente alla fase di visualizzazione dei dati senza però riflettere sugli elementi che aumentano l'efficacia della comunicazione visiva.
Vediamo quindi visualizzazioni del genere, che illudono l'osservatore della loro efficacia.
Non stupisce che tali visualizzazioni siano anche create e diffuse da sorgenti autorevoli, che lavorano in industrie e mercati specifici.
Personalmente, trovo spesso cose del genere sui social, dove le visualizzazioni grafiche si diffondono rapidamente grazie all'utilizzo di hashtag. Infatti basta ricercare #dataviz su Twitter per trovare grafici di ogni tipo, alcuni buoni e parecchi brutti.
Proviamo ad interpretare il grafico qui sopra. Sull'asse delle X c'è il tempo, quindi stiamo guardando una serie temporale. L'asse delle Y presenta un range numerico, ma nessuna indicazione di cosa siano quei valori. Leggendo il titolo, diamo per scontato che sia la variazione percentuale.
Vediamo che sono presenti delle annotazioni. Queste mettono in risalto alcuni eventi accaduti in uno specifico punto temporale, mentre altre identificano le serie temporali. Queste ultime sono molto numerose e sono connesse alle diverse serie per permettere all'osservatore di associare un nome ad ogni serie.
Vediamo che le serie sono colorate per pendenza. Questo è possibile capirlo solo se leggono i due riquadri testuali che identificano questo scenario.
Infine, vediamo che è presente anche un raggruppamento per delta percentuale, espresso attraverso la grandezza della sfera a inizio e fine di ogni linea.
Siete riusciti a comprendere ""al volo"" quello che il grafico vuole comunicare?
Io no, come potete immaginare. Ci sono troppi fattori che confondono la mia interpretazione, ed elementi chiave, come quale etichetta associare a quale serie, sono sovrapposti e quindi incomprensibili.
Sta di fatto che grafici del genere siano creati da persone non consapevoli di quello che stanno facendo. Gettare roba su schermo non funziona, soprattutto in questo caso.
Beninteso, nessuno ha l'intenzione di creare un grafico che non funziona. Come dice Cole Knaflic, esperta mondiale di data viz e autrice di Stortelling with Data, ""noi non siamo naturalmente abili a fare buon storytelling. A scuola impariamo a comprendere parole e numeri, ma pochi hanno l'abilità di mettere insieme queste due sfere."". Alcuni elementi di questo articolo useranno il materiale di Knaflic come esempi, poiché affidabili e chiari.
Prendere consapevolezza di alcuni principi di comunicazione diventa fondamentale per il professionista che lavora con i dati. In questo articolo quindi il mio obiettivo sarà di condividere con voi alcune tecniche e principi per migliorare la comunicazione visiva e offrire degli esempi a supporto di queste tematiche. Vedremo
- Cosa rende un grafico inefficace e come correggere questi errori
- Come scegliere la visualizzazione migliore per il formato di dati disponibile
- Targeting e personalizzazione delle presentazioni per la nostra audience
- Un framework per garantire l'efficacia di una visualizzazione e di una presentazione
Sarà un articolo abbastanza corposo - vi suggerisco di mettere su il caffè nel frattempo 🙂
Prendere consapevolezza: Dove si colloca la nostra expertise?
Come in tutte le cose che implicano un cambiamento di prospettiva, prendere consapevolezza della nostra situazione attuale è il primo step per raggiungere l'obiettivo.
Diventa quindi importante stabilire dove si colloca il nostro livello di competenza nella creazione di visualizzazioni efficaci.
Useremo una scala fittizia che rappresenta il punto iniziale e il punto finale alla quale aspiriamo.
I livelli di expertise
Creiamo un benchmark di riferimento. Ponendo l'abilità di fare storytelling con i dati come punto più alto sulla scala e quello di lavorare con i dati nella loro forma più elementare, una tabella, come punto più in basso, disegnamo una scala che ci guiderà durante il percorso di apprendimento.
Diamo una spiegazione per ognuna di queste milestone.
Tabelle
Usiamo e visualizziamo i dati attraverso tabelle, la forma più basilare di visualizzazione di dati. Tipicamente le nostre tabelle non sono formattate correttamente e non mettono in risalto i dati che contengono. Usiamo formati proposti da strumenti come Excel, dando per scontato che siano efficaci.
Grafici brutti
Creiamo dei grafici che sono afflitti da molti problemi, ma non ce ne rendiamo conto. Formattiamo ogni elemento del grafico, come titolo, legenda, etichette e altre forme pensando che facendo ciò mettiamo in risalto queste caratteristiche e aiutiamo la comprensione. Il disordine regna sovrano.
Usiamo visualizzazioni non ottimali per il dato che vogliamo mostrare e usiamo lunghi testi esplicativi.
Grafici semplici
Comprendiamo che less is more. Puliamo il grafico e facciamo a meno delle compulsioni che ci portano spesso e volentieri ad esagerare. I testi sono brevi poiché il significato del dato spicca di più dalla visualizzazione.
Iniziamo ad avere comprensione dei principi percettivi che aiutano a focalizzare l'attenzione dell'osservatore e siamo consapevoli che ogni elemento aggiunge carico cognitivo.
Grafici belli
Siamo in grado di creare grafici di alta qualità, dalla facile comprensibilità. Sono chiari ed efficaci sia per l'esperto che per l'osservatore che non ha familiarità con i dati.
Siamo in grado di scegliere la visualizzazione migliore, dando priorità a elementi che sfruttano il funzionamento della attenzione umana per dirigere l'attenzione verso le informazioni più rilevanti che vogliamo trasmettere.
Storytelling
Saper fare storytelling con i dati è il culmine dell'abilità nella comunicazione visiva. Fare storytelling significa saper comunicare efficacemente il messaggio, guidando la narrazione attraverso visualizzazioni.
Per narrazione si intende spesso una presentazione digitale fatta ad esempio con Powerpoint o Keynote, ma a volte può essere fatta anche attraverso brochure o simili.
Un esperto di storytelling si nota subito: il suo racconto è subito comprensibile, diretto, snello. A volte anche divertenti.
Si fa buon storytelling quando si uniscono grafici belli, creati dinamicamente e ad-hoc per l'audience, con presentazioni verbali e interattive che catturano e affascinano gli osservatori.
È difficile mostrare lo storytelling attraverso una immagine, quindi rimando il lettore ad un video di C. Knaflic che presenta dati in una conferenza di Google.
Autovalutazione post benchmark
Dando per scontato che siamo tutti in grado di generare un grafico usando una fonte dati tabellare (ad esempio su Excel o con una libreria Python come Seaborn o Matplotlib partendo da una tabella oppure un dataframe), poniamoci una serie di domande:
- qual è la migliore visualizzazione che ho creato?
- qual è una che sicuramente aveva dei problemi ma che ho fatto finta di non vedere oppure non ho corretto per qualche motivo?
- qual è una presentazione (Powerpoint, Keynote) che ho creato di cui vado fiero?
Invito il lettore a fare mente locale e a dare delle risposte anche generiche a queste domande.
La maggior parte dei novizi di data visualization si colloca tra saper creare tabelle e grafici brutti. Questa è la ""zona"" dove non si hanno competenze in comunicazione visiva e quindi non c'è consapevolezza di cosa renda un grafico efficace.
Fare grafici semplici non è di per sé complesso, ma applicare consistentemente i principi di design e psicologici alla loro base richiede studio ed esercizio attivo.
Quindi, se non vi trovate tra tabelle e grafici brutti, congratulazioni! Siete parte di un gruppo di professionisti che ne sa molto di più di data visualization di quanto si trova generalmente online :)
In base a dove ci collochiamo sulla linea di expertise daremo priorità alle tecniche che vedremo ora per migliorare e correggere errori alle nostre visualizzazioni.
Cosa rende un grafico inefficace?
Il nemico numero uno di un grafico è il disordine. Questo disordine non riguarda necessariamente il numero di elementi su schermo, ma anche la frammentazione di stile e di ambiti. Disordine è anche quindi sinonimo di mancanza di coesione tra un elemento e l'altro.
Il nemico numero uno di un grafico è il disordine.
Un grafico disordinato è per definizione un grafico difficile da processare. Un grafico difficile da processare è innegabilmente un grafico inefficace.
Ecco una formula che utilizzo personalmente per valutare l'efficacia di un grafico.
\[ Efficacia = \frac{1}{t} \]
Quando \( t \) è grande, allora l'efficacia tende a 0.
Si pone naturalmente una domanda: cosa significa comprensibile? In termini psicologici, una cosa è comprensibile secondo due criteri:
- abbiamo la conoscenza pregressa che guida la nostra comprensione
- lo stimolo non sovraccarica il nostro sistema sensoriale
Queste due nozioni sono fondamentali per un esperto di data visualization: dobbiamo sempre pensare a quanta conoscenza abbia l'osservatore rispetto a quello che vede su schermo e quanto andiamo a sovraccaricare il suo sistema sensoriale.
Tratteremo il targeting più avanti nell'articolo. Per ora concentriamoci sul carico cognitivo.
Accenni di psicologia cognitiva: il carico cognitivo
In psicologia cognitiva, il carico cognitivo è uno spazio di memoria di lavoro che usiamo per elaborare lo stimolo in ""real time"". Tenere a mente dei numeri mentre ripetiamo una filastrocca sovraccarica notevolmente la nostra memoria di lavoro.
L'attenzione umana è selettiva ed è limitata in termini di risorse.
La teoria del carico cognitivo afferma che, poiché la memoria a breve termine è limitata, le esperienze di apprendimento dovrebbero essere progettate per ridurre il ""carico"" della memoria di lavoro al fine di promuovere l’apprendimento e la comprensione.
Come esperti di data viz vogliamo sempre e comunque considerare come la nostra rappresentazione verrà elaborata mentalmente dal nostro osservatore.
Questa è la differenza tra chi è bravo a fare data visualization e chi invece lo usa come mero strumento di comunicazione (inefficace).
Il carico cognitivo rappresenta lo sforzo mentale richiesto per processare le informazioni a cui siamo esposti.
È un po’ come quando chiediamo al nostro computer di svolgere dei calcoli. Calcoli semplici richiederanno pochissimo tempo, calcoli difficili richiederanno parecchio tempo.
La data visualization è dunque molto più di quello che sembra – bisogna essere abili a comprendere come il cervello umano processa le informazioni, e trovare il punto di equilibrio tra lo stimolare l’attenzione dell’osservatore e il sopraffarla.
Tirando le somme, quindi, impariamo una informazione importante: liberiamo il grafico di elementi che non contribuiscono alla comprensione - gli elementi che aumentano il carico cognitivo ingiustificatamente.
Alcuni esempi:
- Se l'asse delle X è una data, è inutile inserire il titolo all'asse stesso
- Se costruiamo un istogramma, è inutile applicare le etichette alle barre
- Se il titolo del grafico è informativo, è inutile inserire un testo che dice una cosa simile da qualche parte nel grafico
- Se ci sono più serie di dati, usiamo colori che sono altamente distinguibili, evitando di usare sfumature tra colori contigui
E molto altro. Impareremo col tempo a spogliare il grafico dalle informazioni non rilevanti - basterà porci la domanda ogni volta che ne costruiamo uno.
Come creare una visualizzazione efficace?
Vediamo ora come creare un grafico ""bello"". Ridurre il disordine è sicuramente una azione da fare sempre, ma ci sono anche delle tecniche che aumentano l'efficacia della visualizzazione repentinamente.
Queste tecniche fanno leva sui principi percettivi noti, come quelli della scuola di psicologia della Gestalt. La Gestalt ha avuto notevole impatto grazie alla mappatura di principi visivi che spiegano come gli esseri umani si interfacciano con il mondo.
Ancora oggi questi principi rimangono solidi e da applicare per avere successo in campi come la data viz.
Partiamo dal primo: lo spazio.
Utilizzo dello spazio vuoto
Diamo una occhiata al grafico brutto e a quello semplice dell'esempio precedente.
Spicca subito un elemento da considerare sempre durante la creazione di una visualizzazione: lo spazio vuoto.
Mentre il grafico brutto è disordinato e ha un carico cognitivo alto, il grafico semplice è più ""leggero"", più facile da digerire.
Uno degli elementi che aiuta questo processamento è proprio lo spazio vuoto tra gli elementi. Lasciar respirare gli elementi importanti è un modo per metterli in risalto, senza creare pressione percettiva.
In questo caso, è bastato rimuovere la griglia al grafico brutto. Come vedete, è molto facile ridurre il disordine di un grafico se si sa come farlo.
Lo spazio vuoto inserisce una pausa nel sistema visivo dell’osservatore e aiuta a far assimilare bene gli stimoli.
È come una pausa in un discorso o una virgola in un testo: se non ci sono vi sentite presto affaticati (eccessivo carico cognitivo) e la vostra attenzione scenderà sempre di più.
Se usato strategicamente, l’utilizzo di spazi vuoti aiuta il pubblico a metabolizzare la nostra presentazione e a seguire con un livello di attenzione più alto.
L'utilizzo del contrasto
Un utilizzo oculato del contrasto porterà un grafico semplice ad uno bello (sia a livello estetico che a livello percettivo).
L’uso di elementi contrastanti in colore aiuta il nostro pubblico a focalizzare l’attenzione.
Infatti, è facile portare il nostro sguardo sull'unica barra importante alla narrazione nel grafico bello. È altrettanto facile ignorare le barre grigie.
Dobbiamo stare attenti però nell’utilizzo di queste tecniche perché è facile spingersi oltre e ottenere l’effetto contrario.
Oltre al colore, nell'esempio, c'è anche la forma: forme contrastanti attirano l'attenzione. Usiamole con criterio.
Sistemando un po' il grafico secondo quello che abbiamo coperto, otteniamo una cosa del genere.
Molto meglio.
Ci sono stati diversi cambiamenti tra il primo e il secondo grafico:
- Usato un grafico a barre orizzontali perché è il più semplice e si adatta bene alla situazione
- Usato un contrasto marcato per guidare l’attenzione dell’osservatore velocemente sul punto chiave. Ora sappiamo subito dove siamo forti e dove invece abbiamo lacune
- Abbiamo etichettato il dato in modo da informare l'osservatore del valore del target
Prossimità degli elementi
Gli elementi del campo percettivo vengono uniti in forme distinte con tanta maggiore coesione quanto minore è la distanza tra di loro. All’interno dello stesso ambiente, elementi tra loro vicini e simili vengono considerati dal nostro cervello un tutt’uno.
Fissando i puntini neri della figura in basso, il nostro cervello non percepisce 12 punti neri uguali e distinti. Bensì 3 figure diverse - questo perché tende a collegare i puntini più vicini tra di loro.
Utilizzando questo principio, possiamo dirigere anticipare dove l'attenzione dell'osservatore andrà a cadere e quindi a manipolare il loro sguardo.
Somiglianza tra gli elementi
Oggetti che si somigliano in colore, dimensione, forma o orientamento sono percepiti come appartenenti allo stesso gruppo.
Nella immagine qui in esempio tendiamo a percepire i cerchi blu come appartenenti allo stesso gruppo e reciprocamente per i quadrati grigi a destra.
Nell’immagine inferiore invece vediamo come i nostri occhi percepiscano due gruppi diversi proprio in base al colore della ""riga"".
Questo ci aiuta a guidare lo sguardo del pubblico dove vogliamo.
Principio della chiusura
Le linee che formano delle figure chiuse tendono ad essere viste come gruppi a se stanti. La nostra mente è predisposta a fornire le informazioni mancanti per chiudere una figura, pertanto i margini chiusi o che tendono ad unirsi si impongono come unità su quelli aperti.
Anche effetti di chiusura molto leggeri attivano questo pattern visivo.
Il completamento del pattern
Gli umani (soprattutto noi analisti) hanno una forte antipatia per le questioni in sospeso.
Di fronte a stimoli visivi ambigui, oggetti che possono essere percepiti sia come forme aperte, incomplete e inusuali, sia come forme chiuse, intere e regolari, li percepiamo naturalmente come queste ultime.
Il principio di chiusura afferma che percepiamo le strutture aperte come chiuse, complete e regolari ogni volta che possiamo ragionevolmente farlo.
Continuità degli elementi
Questo effetto è simile al principio del completamento: quando guardiamo a degli oggetti, i nostri occhi cercano la via più efficiente e creano continuità tra ""percorsi"" che non esistono esplicitamente.
Nella figura in basso, vediamo come rimuovere l’asse verticale non cambia la nostra percezione del grafico. Questo perché l’allineamento a sinistra permane e siamo in grado di allineare gli elementi implicitamente.
Connettere i punti
Oggetti che sono fisicamente connessi sono percepiti come appartenenti allo stesso gruppo.
Questa proprietà è tipicamente più forte di associazioni basate su colore, forma o dimensione. Infatti la figura mostra come la connessione continui ad essere dominante anche in presenza di colore, dimensione e forma diverse.
Uno dei casi di utilizzo più comune di questo principio è quello dei grafici a linea, che si basano interamente su questo principio percettivo.
Gerarchia visiva degli elementi
Quando il design di qualcosa è fatto bene questo passa in secondo piano e il pubblico non se ne rende conto. È il cruccio del designer.
Ma quando il design è approssimativo, allora il pubblico lo percepisce negativamente. Il tema della gerarchia degli elementi visivi è rappresentativo di questo, poiché è difficile da comprendere se siamo dei novizi in quello che vogliamo esporre al pubblico.
Facciamo un piccolo esercizio. Consideriamo la prossima immagine e proviamo a stilare cosa funziona e cosa non funziona rispetto a quanto abbiamo coperto finora.
Ora guardiamo questa versione, processata e sistemata.
Il più grande cambiamento tra la prima immagine e la seconda coinvolge l’allineamento.
Nell’immagine iniziale, ogni blocco di testo era allineato al centro. Questo non crea linee pulite né a destra né a sinistra, il che ha impatto sulla nostra percezione e giudizio di qualità dell’immagine.
Abbiamo fatto delle modifiche anche alle linee diagonali che puntavano ai testi. Di fatto, sono più difficili da seguire delle linee orizzontali (e se queste possono essere evitate, come nell’esempio, ancora meglio).
Generalmente, l’idea è quella di allineare i nostri elementi in modo tale che ci sia consistenza e ordine.
Senza delle indicazioni del genere, lo sguardo del pubblico seguirà pattern simili a quello a «Z».
I grafici che funzionano e quelli da evitare sempre
Esistono dei grafici che funzionano quasi sempre, si.
Esistono anche grafici da evitare come la peste.
Partiamo con una lista dei grafici che funzionano.
Grafici che funzionano
Ecco una immagine che contiene tutti i grafici che funzionano. L'immagine seguente è stata presa da Storytelling with Data di Knaflic, traducendo le etichette in italiano.
Testo semplice
Quando abbiamo uno o due numeri da mostrare, usare del testo semplice può essere una buona scelta.
Usiamo un singolo numero, lo rendiamo prominente e gli associamo un breve testo esplicativo.
Oltre ad essere potenzialmente fraintendibile, creare una tabella o un grafico per uno o due numeri può essere una esagerazione.
Tabella
Le tabelle vanno lette. Questo è il problema principale. Il nostro occhio deve scansionare colonne e righe per trovare un valore e questo può prendere tempo e risorse mentali.
Come anticipato all'inizio di questo articolo, il suo vantaggio più grande è comunica bene un singolo valore all’intersezione di due dimensioni. Quindi quando vogliamo comunicare al nostro audience un dato del genere, nulla batte la tabella.
Il design della tabella è importante: è consigliato uno sfondo chiaro per mettere in risalto il dato al centro della tabella. Non usiamo bordi troppo spessi o altri elementi distraenti.
Mappa di calore (heatmap)
La mappa di calore va a colmare i problemi che presenta la tabella, andando quindi a rimuovere la componente di lettura del dato e utilizzando indicazioni visive per comunicare.
Essa fa uso di colori e di contrasti per mettere in risalto valori all’interno della tabella.
Diagramma a dispersione
Il diagramma a dispersione è utile per mostrare la relazione tra due variabili.
Sono molto comuni in scenari di tipo scientifico (e quindi forse ritenuti più ostici dai non addetti ai lavori), ma sono anche presenti nel campo del business e del marketing.
Essendo la correlazione una delle metriche più conosciute e applicate nei vari campi scientifici e non, usare un diagramma a dispersione si rivela efficace e affidabile.
Grafico a linee
I grafici a linee sono ideali per mostrare dati di tipo continuo (serie temporali, dati ripetutamente campionati, etc.). I punti appaiono di fatto collegati perché lo sono visivamente attraverso la linea.
Essendo uno dei grafici più comuni, risulta facile da interpretare e non genera ansia da ""comprensione"" da parte del pubblico.
Grafico a pendenza (slopegraph)
Molto utile per confrontare periodi uno con l’altro. Grazie proprio alla pendenza della linea che connette i punti, siamo in grado di trarre conclusioni velocemente sui dati.
Sono utili non solo per comprendere i valori assoluti dei dati, ma anche il cambiamento relativo tra i punti che seguono e precedono.
Grafico a barre
Il grafico a barre è probabilmente il grafico più comune di tutti. Forse proprio per questo motivo tendiamo ad evitarli il più possibile per soluzioni più fantasiose.
Piuttosto, dovremmo usarli di più proprio perché sono comuni e fare leva sulla conoscenza generale del pubblico sulla interpretazione di questo tipo di grafico.
I nostri occhi sono molto abili a confrontare le altezze di due estremità messe una di fianco all’altra. Questo ci permette di capire velocemente l’incremento o decremento del dato.
Grafico a barre impilate
L’uso per questi grafici è più limitato. Sono usati per confrontare i totali attraverso varie categorie e vedere quanto una sottocategoria contribuisce al totale.
Possono diventare disordinati e difficili da leggere velocemente, quindi vanno usati con cautela.
Un grafico a barre impilate può iniziare dallo 0 e arrivare al valore grezzo massimo oppure usare le percentuali in modo da coprire sempre tutto il totale.
Una nota: questi grafici sono complicati da interpretare quando la nostra attenzione si focalizza sulle barre che non partono dalla base - ergo, quelle che si impilano su di esse.
Grafico a cascata
Il grafico a cascata è utile per scomporre l’informazione fornita da un grafico a barre impilate. È utile per mostrare i punti di inizio e di fine, incrementi e decrementi e la tendenza del dato.
Grafico a barre orizzontale
Questo è sicuramente il grafico più semplice di tutti grazie alla sua grande leggibilità.
Se i nomi delle nostre categorie sono troppo lunghi, l’audience può comunque leggerli semplicemente perché il dato viene presentato da sinistra a destra.
Questo significa che anche l’interpretabilità del dato è avvantaggiata: infatti leggeremo sempre prima il nome della variabile e poi il suo valore.
Grafico ad area quadrata
Non è un grafico da usare spesso, perché l’occhio umano non è molto abile nel attribuire in valore numerico ad una area estesa tra due dimensioni.
Diventa utile per mostrare serie dalla magnitudine diversa, ad esempio un totale vs i suoi componenti.
La differenza tra un grafico ad area e uno ad area quadrata è la disposizione di questa area tra gli assi.
Grafici che NON funzionano (da evitare quando possibile)
Esistono grafici da evitare praticamente sempre. I motivi sono empiricamente validi: sono difficili da interpretare nella maggior parte dei casi e, nonostante ciò, vengono usati comunque.
L'utilizzo di questi grafici nonostante la loro inefficacia avvalora ancora di più la tesi della mancata consapevolezza del pubblico generale di cosa sia una comunicazione visiva efficace.
Grafico a torta
Il re dei grafici inefficaci, eppure uno di quelli più comuni. Quando pensiamo ad un grafico, di solito pensiamo ad un grafico a torta.
Perché è davvero così pessimo? Diamo una occhiata a questa immagine.
Si notano diversi problemi:
- Le aree non sono facilmente comparabili a occhio nudo, quindi ogni fetta richiede sempre l’ausilio di una etichetta numerica, il che apre strade per l’utilizzo di altri grafici
- Se due fette sono simili di dimensione, l’interpretazione è ancora più difficile.
- Sono mainstream nonostante i loro problemi – questo contribuisce a farli odiare ancora di più
Grafici a ciambella
Il grafico a ciambella (donut chart) presenta gli stessi problemi del grafico a torta, solo che anziché chiedere al pubblico di valutare angoli ed aree, qui gli chiediamo di valutare la lunghezza di un arco contro l’altro.
Il caso del 3D
Una delle regole d’oro della data viz è di non usare mai il 3D.
Mai.
L’unica eccezione è quando c'è veramente una terza dimensione, ma anche in quel caso la rappresentazione 3D potrebbe non essere la strada giusta poiché si creano distorsioni visive che sono difficili da interpretare.
Aggiungere il 3D nei nostri grafici aggiunge elementi di distrazione senza offrire empiricamente nessun vantaggio.
L'asse secondario
L’asse secondario è spesso essenziale per mostrare una serie con unità di misura diverse o indipendente rispetto alla serie sull’asse primario.
È tuttavia consigliabile evitarne l’utilizzo perché non è di facile lettura e l’osservatore meno esperto può avere difficoltà se non aiutato nella comprensione.
Ecco alcune pratiche che possiamo seguire per migliorare la leggibilità di questo grafico:
- Etichettare direttamente il dato, rimuovendo gli assi, allargando le barre e fare un po' di ordine
- Separare i grafici verticalmente
Questa soluzione è, secondo me, la migliore in termini di chiarezza.
Targeting e personalizzazione delle presentazioni
Tocchiamo ora un punto fondamentale per ogni storyteller: il suo rapporto con il pubblico.
L'abilità di comunicare efficacemente è utile solo se si comunica la cosa giusta alle persone giuste. Se queste due condizioni non sono soddisfatte, allora la comunicazione diventa inefficiente.
La presentazione dei nostri dati parte da tre domande fondamentali:
- Chi è il nostro audience?
- Cosa vogliamo che il nostro audience faccia o comprenda?
- Come possiamo usare i dati a disposizione per raggiungere i nostri obiettivi?
Meglio conosciamo il nostro audience, più informazioni avremo per comunicare al meglio il nostro messaggio.
Evitiamo di definire il pubblico come ""investitori"" oppure ""chiunque sia interessato"" - rimanendo così generici corriamo il rischio di non riuscire a comunicare efficacemente con nessuno.
Il Chi
È importante comprendere quale sia la relazione tra noi e il pubblico. Ragioniamo su come verremo percepiti:
- È la prima volta che comunichiamo con loro?
- Oppure abbiamo una relazione già definita perché conosciamo la maggior parte di loro da parecchio tempo?
- Ci considerano degli esperti oppure dobbiamo lavorare per trasmettere la nostra credibilità?
Il Cosa
Cosa deve fare il nostro audience con le informazioni che vogliamo trasmettere loro? Prendiamoci sempre del tempo per rispondere a questa domanda – vogliamo che questa sia sempre molto chiara.
Stimoliamo la conversazione attraverso delle call-to-action per il pubblico.
Se non abbiamo CTA, ne generiamo dei nuovi, ad esempio attraverso dei next step.
Il Come
La coerenza che dobbiamo avere nel nostro lavoro di data visualization deve essere apprezzabile anche nella presentazione e nel tono di voce (tone of voice).
Stiamo celebrando un successo o stiamo informando il pubblico di un pericolo imminente sulla nostra salute se non interveniamo sul problema presentato?
Evitare un tone of voice incoerente è importante perché allinea le aspettative del pubblico con il nostro messaggio.
Definire il chi, cosa e come prima di presentare i nostri dati ci permette di pianificare e adattare il nostro approccio comunicativo per massimizzare l'efficienza della nostra esposizione.
Questa pianificazione si riflette anche nei grafici e cosa andranno a mostrare - una presentazione ad un pubblico di esperti di settore sarà molto diversa dalla stessa ma per un gruppo di persone inesperte.
Framework per creare visualizzazioni efficaci
Tiriamo le somme di quello che abbiamo trattato per creare un modello mentale per la data visualization e storytelling. Questo modello dovrà aiutarci a creare visualizzazioni semplici ed efficaci e a migliorare nella nostra comunicazione visiva.
Facciamo una lista di quello che abbiamo trattato.
- La maggior parte dei grafici in circolazione non sono efficaci - è facile sbagliare ma è altrettanto facile capire perché
- Capito il perché (e quindi avendo acquisito consapevolezza di questo), fermiamoci e mettiamoci sempre nei panni dell'osservatore
- Analizziamo quello che creiamo su schermo - quanta informazione c'è nella nostra visualizzazione? È disordinata? Come impatta questo sul carico cognitivo dell'osservatore?
- Scegliamo la visualizzazione più semplice, conosciuta e empiricamente adatta ai nostri dati. Se abbiamo due variabili e vogliamo metterle in relazione, allora scegliamo uno scatterplot. Se abbiamo una serie di variabili categoriali ed una metrica, un grafico a barre orizzontali.
- Rispettare l'attenzione dell'osservatore. Ogni momento di focus da parte dell'audience è un momento guadagnato da noi che presentiamo. Se la perdiamo, la colpa è nostra, non loro.
- Fare ricerca sulla nostra audience prima della presentazione. Conoscere gli stakeholder, la loro preparazione in materia e particolari del loro background.
Conclusione
A chi ha letto tutto l'articolo fino alla fine - un grazie sentito per la vostra attenzione 🙏🏻. Un grazie sentito a chi invece ha letto poco, ma che comunque ha imparato qualcosa.
Abbiamo visto come la data visualization sia poco compresa come disciplina a se stante - le prove di questa affermazione si possono semplicemente raccogliere scandagliando l'internet in cerca di una visualizzazione qualunque.
Questo è un peccato, ma anche una opportunità, perché il mondo diventa sempre più data-driven. Un professionista che lavora con i dati ha quindi l'esigenza di imparare una skill emergente, utile e ad alto impatto come la data visualization.
Invito il lettore interessato a leggere Storytelling with Data di Knaflic, recensito in questo blog, per apprendere i dettagli che rendono efficace la comunicazione visiva basata sui dati.
Al prossimo articolo,
Andrea 👋
Commenti dalla community"
https://www.diariodiunanalista.it/posts/un-paradigma-per-la-selezione-del-modello/,"In ogni progetto di machine learning saremo posti di fronte all'esigenza di dover selezionare un modello per iniziare migliorare quella che è la nostra baseline di partenza.
Infatti, se la baseline ci da un modello di partenza utile per capire cosa possiamo aspettarci da una soluzione molto semplice, un modello selezionato attraverso una metodologia specifica ci aiuta entrare nella fase di ottimizzazione del progetto.
La metodologia
Poniamo che abbiamo un problema di regressione da risolvere. Iniziamo importando tre librerie fondamentali
from sklearn import linear_model
from sklearn import ensemble
from sklearn import tree
from sklearn import svm
from sklearn import neighbors
from lightgbm import LGBMRegressor
from xgboost import XGBRegressor
Useremo questo tipo di metodologia per selezionare il modello:
- creeremo una lista vuota e la popoleremo con la coppia (nome_modello, modello)
- definiremo i parametri per lo split dei dati attraverso il cross-validatore KFold di Scikit-Learn
- creeremo un ciclo for dove andremo a cross-validare ogni modello e a salvare la sua performance
- visualizzeremo la performance di ogni modello per poter scegliere quello che ha performato meglio
Definiamo una lista e inseriamo i modelli che vogliamo testare.
models = []
models.append(('Lasso', linear_model.Lasso()))
models.append(('Ridge', linear_model.Ridge()))
models.append(('EN', linear_model.ElasticNet()))
models.append(('RandomForest', ensemble.RandomForestRegressor()))
models.append(('KNR', neighbors.KNeighborsRegressor()))
models.append(('DT', tree.DecisionTreeRegressor()))
models.append(('ET', tree.ExtraTreeRegressor()))
models.append(('LGBM', LGBMRegressor()))
models.append(('XGB', XGBRegressor()))
models.append(('GBM', ensemble.GradientBoostingRegressor()))
models.append((""SVR"", svm.LinearSVR()))
Per ogni modello appartenente alla lista models, andremo a valutarne la performance attraverso model_selection.KFold.
Il suo funzionamento è semplice: il nostro set di dati di addestramento (X_train, y_train) verrà diviso in parti uguali (quelle che vengono chiamate folds) che verranno testate singolarmente.
Quindi la KFold cross-validation fornirà una metrica di performance media per ogni split anziché una metrica singola basata sull'intero dataset di addestramento. Questa tecnica è molto utile perché permette di misurare più accuratamente la performance di un modello.
Poiché si tratta di un problema di regressione, useremo la metrica dell'RMSE (root mean squared error).
Definiamo così i parametri per la cross-validazione inizializziamo il ciclo for.
n_folds = 5 # numero di split
results = [] # lista dove salvare le performance
names = [] # lista dove salvare i nomi dei modelli per la visualizzazione
# iniziamo il ciclo dove andremo a testare ogni modello
for name, model in models:
kfold = model_selection.KFold(n_splits=n_folds)
print(""Testing model:"", name)
cv_results = model_selection.cross_val_score(
model,
X_train,
y_train,
cv=kfold,
scoring=""neg_mean_absolute_error"",
verbose=0,
n_jobs=-1)
results.append(cv_results)
names.append(name)
msg = ""%s: %f (%f)"" % (name, cv_results.mean(), cv_results.std())
print(msg+""\n"")
Ogni modello verrà quindi sottoposto a cross-validazione, testato, e la sua performance salvata in results.
La visualizzazione è molto semplice, e verrà fatta attraverso boxplot.
# Modelli messi a confronto
fig = plt.figure(figsize=(12,7))
fig.suptitle('Algorithm Comparison')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)
plt.show()
Il risultato della selezione
Il risultato finale sarà questo:
Da qui si nota come RandomForest e GradientBoostingMachine siano i più performanti. Possiamo quindi iniziare a creare nuovi esperimenti e a testare ulteriormente questi due modelli.
Commenti dalla community"
https://www.diariodiunanalista.it/posts/valutare-il-livello-di-competenza-linguistica-col-machine-learning/,"Questo è il primo articolo che si basa su una competizione Kaggle. Nonostante non abbia raggiunto il top della ladder nella competizione, il notebook di riferimento ha ricevuto una medaglia d'oro, che è comunque un ottimo risultato.
Per chi non lo sapesse, Kaggle è una delle piattaforme di riferimento non solo per l'apprendimento e la condivisione di materiale intorno alla data science e al machine learning, ma anche per proporre competizioni basate sui dati.
Aziende importanti, del calibro di Google e Facebook, creano competizioni a premi per chiamare e riunire la community dell'IA ad aiutarli a risolvere problemi importanti che richiedono soluzioni basate sulla IA.
In questa competizione, l'obiettivo è di creare un modello in grado di valutare la competenza linguistica di studenti liceali che studiano l'inglese.
Condividerò con voi come ho modellato i dati e la strategia generale di risoluzione al problema, andando commentare il codice scritto e il suo output in modo che possiate integrare parti della pipeline in un vostro progetto simile.
Trovate il link alla competizione qui
Mentre trovate qui il link al notebook Kaggle.
Contesto
Partiamo dal problema che vogliamo risolvere. Tradotto dalla scheda descrizione della competizione
L'obiettivo di questa competizione è valutare la competenza linguistica degli studenti di lingua inglese di grado 8-12 (ELL). L'utilizzo di un set di dati di saggi scritti da ELL aiuterà a sviluppare modelli di competenza che supportino meglio tutti gli studenti.
Il tuo lavoro aiuterà gli ELL a ricevere un feedback più accurato sul loro sviluppo linguistico e ad accelerare il ciclo di valutazione per gli insegnanti. Questi risultati potrebbero consentire agli ELL di ricevere compiti di apprendimento più appropriati che li aiuteranno a migliorare la loro conoscenza della lingua inglese.
Il nostro obiettivo è chiaro: sviluppare un modello che aiuta gli enti formativi a sviluppare percorsi formativi più chiari per migliorare la competenza linguistica degli studenti.
Il dataset
Il dataset comprende saggi argomentativi scritti da studenti di lingua inglese di grado 8-12 (ELL). I saggi sono stati valutati secondo sei misure analitiche:
- coesione
- sintassi
- vocabolario
- fraseologia
- grammatica
- convenzioni
Ogni misura rappresenta una componente di competenza nella scrittura, con punteggi maggiori corrispondenti a una maggiore competenza in quella misura.
I punteggi vanno da 1 a 5 con incrementi di 0,5.
Il nostro compito è prevedere il punteggio di ciascuna delle sei misure per i saggi forniti nel set di test.
Come spesso si vede nelle competizioni Kaggle, i dataset forniti sono divisi in
train e
test.
Ecco come si presenta il dataset
- text_id: identificativo del testo scritto dallo studente
- full_text: testo scritto dallo studente
- cohesion: punteggio di valutazione della coesione del testo
- syntax: punteggio di valutazione della sintassi del testo
- vocabulary: punteggio di valutazione della del vocabolario usato nel testo
- phraseology: punteggio di valutazione della fraseologia del testo
- grammar: punteggio di valutazione della grammatica del testo
- conventions: punteggio di valutazione della convenzioni del testo
Setup del progetto
Iniziamo ad importare le dipendenze del progetto. Ci serviranno
- pandas e numpy
- Sklearn, PyTorch e SentenceTransformer
- Matplotlib e seaborn
- NLTK
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
tqdm.pandas()
from sklearn import model_selection
from sklearn import metrics
import string
from nltk.corpus import stopwords
from sentence_transformers import SentenceTransformer
Importiamo i nostri dati e stabiliamo le feature e etichette
train = pd.read_csv('./data/train.csv')
test = pd.read_csv('./data/test.csv')
sample_submission = pd.read_csv('./data/sample_submission.csv')
features = [""full_text""]
labels = [""cohesion"", ""syntax"", ""vocabulary"", ""phraseology"", ""grammar"", ""conventions""]
Analisi esplorativa del dato (EDA)
Uno degli step fondamentali in qualsiasi progetto di analisi e di modellazione predittiva è quello dell'esplorazione del dato.
Questa attività permette di comprendere meglio il dato e scoprire se esistono errori, associazioni, correlazioni e altre dinamiche nei dati.
Per chi è interessato a leggere un articolo dedicato alla analisi esplorativa, linko un articolo qui in basso.
Per questo progetto la terremo facile - andremo solo a disegnare le distribuzioni delle variabili e la heatmap di correlazioni.
# Creiamo righe e colonne per disegnare i grafici
plt.figure(figsize=(16, 6))
for i, label in enumerate(labels):
plt.subplot(2, 3, i + 1)
plt.hist(train[label], bins=20)
plt.title(label)
plt.suptitle(""Label distributions"")
plt.show()
Le distribuzioni sono abbastanza simili - questo ci informa che non ci sono anomalie e correzioni da fare.
plt.figure(figsize=(10, 5))
sns.heatmap(train[labels].corr(), annot=True, square=False, vmin=-1, vmax=1)
plt.title(""Correlation between labels"")
plt.show()
La matrice di correlazione evidenzia che le variabili sono generalmente associate tra di loro. Quando un testo è scritto bene per una variabile, allora tale testo tende a mostrare la stessa performance anche per un'altra.
Un modello predittivo potrebbe riuscire ad imparare in maniera efficiente a generalizzare su questi dati. Dobbiamo quindi lavorare sull'encoding del testo.
La motivazione è da trovarsi proprio nel compito da risolvere: per valutare la coesione, sintassi, vocabolario, etc. è importante valutare completamente il testo, senza trasformazioni.
L'utilizzo di punteggiatura e termini specifici contribuiscono alla predizione dei target, e vanno quindi ritenuti.
Approccio alla modellazione
L'idea è di creare una rete neurale in grado di imparare dagli embedding del testo scritto dagli studenti per predire le variabili target.
È utile ragionare su questo step prima di lavorare sui dati, la pipeline di preparazione richiede una idea chiara su come dev'essere fornito il dato al modello.
Il modello può essere disegnato così
L'idea è quindi che ci siano dei rami nel modello dedicati alla predizione di ogni variabile e che poi questi convergano in un singolo output concatenato.
Ora passiamo all'approccio pratico: esplorazione, split in set di addestramento e validazione, caricamento dei dati nel modello e scrittura del modello stesso.
Split in set di addestramento e validazione
Useremo
sklearn.model_selection per dividere preparare i nostri dati in set di addestramento e validazione.
x_train, x_val, y_train, y_val = model_selection.train_test_split(train[features].values, train[labels].values, test_size=0.2, random_state=42)
print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)
>> (3128, 1) (783, 1) (3128, 6) (783, 6)
Encoding del testo in embedding
Useremo il modello
all-MiniLM-L6-v2 di SentenceTransformer per convertire i testi scritti dagli studenti in tensori.
Gli embedding sono rappresentazioni vettoriali in grado di descrivere le relazioni tra parole e quindi tra concetti e contesti diversi.
Ecco il codice per fare l'encoding
embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
train_embeddings = embedding_model.encode(x_train[:, 0], batch_size=200, show_progress_bar=True)
val_embeddings = embedding_model.encode(x_val[:, 0], batch_size=200, show_progress_bar=True)
test_embeddings = embedding_model.encode(test[features].values[:, 0], batch_size=200, show_progress_bar=True)
print(train_embeddings)
>> array([[-0.09318926, 0.00423619, 0.02815985, ..., 0.00752447,
-0.06652465, -0.02501139],
[ 0.04237603, -0.02704737, 0.01021558, ..., 0.05896651,
-0.13489658, 0.02803236],
[ 0.02641385, 0.04102813, -0.08502118, ..., -0.00755973,
-0.0864008 , 0.01873737],
...,
[ 0.05361762, 0.05990427, 0.00647471, ..., 0.06769853,
-0.07111193, 0.05109518],
[ 0.03240207, -0.03758994, 0.07875723, ..., 0.01599755,
-0.12521371, 0.04028842],
[ 0.00951448, 0.08904066, 0.0581084 , ..., 0.05006377,
-0.08251362, -0.00287952]], dtype=float32)
Ora useremo
Dataset e
DataLoader di PyTorch per caricare i nostri dati all'interno del modello.
Creazione di Dataset e DataLoader
Poiché useremo PyTorch per creare il modello, useremo le classi
Dataset e
DataLoader per formattare i nostri dati correttamente.
Il dataset accetterà gli embedding e le variabili target. Invito il lettore interessato a leggere questo articolo di introduzione a PyTorch per comprendere meglio le dinamiche di
Dataset e
DataLoader.
class EnglishLanguageProficiency(Dataset):
def __init__(self, embeddings, cohesion, syntax, vocabulary, phraseology, grammar, conventions):
self.features = torch.tensor(embeddings, dtype=torch.float32)
self.cohesion = torch.tensor(cohesion, dtype=torch.float32)
self.syntax = torch.tensor(syntax, dtype=torch.float32)
self.vocabulary = torch.tensor(vocabulary, dtype=torch.float32)
self.phraseology = torch.tensor(phraseology, dtype=torch.float32)
self.grammar = torch.tensor(grammar, dtype=torch.float32)
self.conventions = torch.tensor(conventions, dtype=torch.float32)
def __len__(self):
return self.features.shape[0]
def __getitem__(self, idx):
x = self.features[idx]
y = torch.stack([
self.cohesion[idx],
self.syntax[idx],
self.vocabulary[idx],
self.phraseology[idx],
self.grammar[idx],
self.conventions[idx]
])
return {
'x': x,
'y': y
}
Creare i dataset e loader è semplice:
train_dataset = EnglishLanguageProficiency(train_embeddings, y_train[:, 0], y_train[:, 1], y_train[:, 2], y_train[:, 3], y_train[:, 4], y_train[:, 5])
val_dataset = EnglishLanguageProficiency(val_embeddings, y_val[:, 0], y_val[:, 1], y_val[:, 2], y_val[:, 3], y_val[:, 4], y_val[:, 5])
train_loader = DataLoader(
dataset=train_dataset,
batch_size=16,
shuffle=True,
drop_last=True
)
val_loader = DataLoader(
dataset=val_dataset,
batch_size=16,
shuffle=False,
drop_last=True
)
Ora scriveremo il modello che riceverà questi dati in input e produrrà delle predizioni.
Rete neurale in PyTorch
La rete neurale è scritta in PyTorch ed è di fatto un modello di regressione, cioè di predizione di un valore numerico.
class RegressionModel(nn.Module):
def __init__(self, n_features, n_outputs):
super().__init__()
self.n_features = n_features
self.n_outputs = n_outputs
self.cohesion_layer = nn.Linear(self.n_features, self.n_outputs)
self.syntax_layer = nn.Linear(self.n_features, self.n_outputs)
self.vocabulary_layer = nn.Linear(self.n_features, self.n_outputs)
self.phraseology_layer = nn.Linear(self.n_features, self.n_outputs)
self.grammar_layer = nn.Linear(self.n_features, self.n_outputs)
self.conventions_layer = nn.Linear(self.n_features, self.n_outputs)
def forward(self, x):
cohesion = self.cohesion_layer(x)
syntax = self.syntax_layer(x)
vocabulary = self.vocabulary_layer(x)
phraseology = self.phraseology_layer(x)
grammar = self.grammar_layer(x)
conventions = self.conventions_layer(x)
return torch.cat([cohesion, syntax, vocabulary, phraseology, grammar, conventions], dim=1)
Questo modello riceverà gli embedding in input, di dimensione
train_embeddings.shape[1] e
n_output 1.
Ciclo di addestramento
Ora scriveremo il codice per addestrare il modello, usando come funzione di perdita MSE (mean squared error).
model = RegressionModel(n_features=train_embeddings.shape[1], n_outputs=1)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.1, verbose=True)
criterion = nn.MSELoss()
epoch_train_losses = []
epoch_val_losses = []
epoch_train_rmse = []
epoch_val_rmse = []
num_epochs = 30
for epoch in tqdm(range(num_epochs)):
train_losses = []
val_losses = []
model = model.train()
for data in train_loader:
x = data['x']
y = data['y']
y_pred = model(x)
loss = criterion(y_pred, y)
optimizer.zero_grad()
loss.backward()
optimizer.step()
train_losses.append(loss.item())
model = model.eval()
for data in val_loader:
x = data['x']
y = data['y']
y_pred = model(x)
loss = criterion(y_pred, y)
val_losses.append(loss.item())
epoch_train_losses.append(np.mean(train_losses))
epoch_val_losses.append(np.mean(val_losses))
epoch_train_rmse.append(np.sqrt(np.mean(train_losses)))
epoch_val_rmse.append(np.sqrt(np.mean(val_losses)))
scheduler.step(np.mean(val_losses))
if (epoch+1) % 5 == 0:
print(f""Epoch: {epoch+1}/{num_epochs}, Train Loss: {np.mean(train_losses):.3f}, Val Loss: {np.mean(val_losses):.3f}, Train RMSE: {np.sqrt(np.mean(train_losses)):.3f}, Val RMSE: {np.sqrt(np.mean(val_losses)):.3f}"")
plt.title(""Train vs Val Loss"")
plt.plot(epoch_train_losses, label=""Train Loss"")
plt.plot(epoch_val_losses, label=""Val Loss"")
plt.legend()
plt.grid()
plt.show()
>>
20%|██ | 6/30 [00:00<00:02, 9.01it/s]
Epoch: 5/30, Train Loss: 0.630, Val Loss: 0.617, Train RMSE: 0.794, Val RMSE: 0.786
37%|███▋ | 11/30 [00:01<00:02, 9.16it/s]
Epoch: 10/30, Train Loss: 0.501, Val Loss: 0.492, Train RMSE: 0.708, Val RMSE: 0.701
53%|█████▎ | 16/30 [00:01<00:01, 9.18it/s]
Epoch: 15/30, Train Loss: 0.433, Val Loss: 0.431, Train RMSE: 0.658, Val RMSE: 0.657
70%|███████ | 21/30 [00:02<00:00, 9.24it/s]
Epoch: 20/30, Train Loss: 0.393, Val Loss: 0.398, Train RMSE: 0.627, Val RMSE: 0.631
87%|████████▋ | 26/30 [00:02<00:00, 9.24it/s]
Epoch: 25/30, Train Loss: 0.367, Val Loss: 0.376, Train RMSE: 0.606, Val RMSE: 0.613
100%|██████████| 30/30 [00:03<00:00, 9.11it/s]
Epoch: 30/30, Train Loss: 0.349, Val Loss: 0.362, Train RMSE: 0.591, Val RMSE: 0.601
Le due curve diminuiscono insieme ed è una indicazione di apprendimento. Non si nota overfitting.
Inferenza su testi nuovi
Ora possiamo usare il modello per creare delle predizioni su dati non visti. Kaggle fornisce un set di test di testi da valutare.
Ecco il codice Python per creare nuove predizioni con il nostro modello
with torch.inference_mode():
test_preds = model(torch.tensor(test_embeddings, dtype=torch.float32))
test_preds = test_preds.numpy()
test_preds = pd.DataFrame(test_preds, columns=labels)
E ora siamo pronti ad unire queste predizioni al dataframe di test.
test.merge(test_preds, left_index=True, right_index=True)
Conclusioni
In questo articolo ho mostrato come, in modo semplice, sia possibile creare una rete neurale in PyTorch sfruttando il transfer learning - cioè facendo leva sugli embedding, quindi l'apprendimento di grosse reti neurali, per risolvere un task di natura linguistica.
È sicuramente possibile migliorare tale approccio e architettura del modello. Ad esempio, si potrebbe fare feature engineering e aggiungere altre variabili che provengono dal testo, come lo score Flesch-Kincaid che indica il grado di leggibilità del testo.
Condivido anche il link al repository Github di questo progetto.
Alla prossima 👋
Commenti dalla community"
https://www.diariodiunanalista.it/posts/valutazione-delle-performance-di-un-modello-di-classificazione/,"La fase di valutazione del modello inizia quando creiamo un set di validazione che consiste in esempi che l'algoritmo di apprendimento non ha visto durante l'addestramento. Se il nostro modello si comporta bene su questo set allora possiamo dire che generalizza bene ed è di buona qualità.
Il modo più comune per valutare se un modello è valido o meno è calcolare una metrica di performance proprio sul set di validazione o di test.
Questo articolo si concentrerà sulle metriche di performance per i modelli di classificazione. Vale la pena specificarlo perché task di regressione hanno metriche tracciabili completamente diverse. Qui un articolo sulle metriche di valutazione di un modello di regressione.
Metriche di performance per la classificazione
Le metriche che andremo a coprire sono:
- Accuratezza
- Precisione e richiamo
- Punteggio F1
- Log loss
- ROC-AUC
- Coefficiente di correlazione di Matthews (MCC)
Accuratezza
Quando vogliamo analizzare la performance di un classificatore binario, la metrica più comune e accessibile è sicuramente la accuratezza (in inglese accuracy). Essa non fa altro che indicarci quante volte il nostro modello ha correttamente classificato un item nel nostro dataset rispetto al totale.
Infatti la formula per l'accuratezza è la divisione tra numero di risposte corrette e il totale delle risposte
\[ accuratezza = \frac{numero\_risposte\_corrette}{numero\_risposte\_totali} \]
La accuratezza tiene conto delle performance del modello in senso stretto. Vale a dire che non ci permette di comprendere il contesto nel quale stiamo operando.
Presa fuori dal contesto, la accuratezza è una metrica molto delicata da interpretare.
Ad esempio, è sconsigliato usare la accuratezza come metrica di valutazione quando operiamo con un dataset sbilanciato, dove le classi sono distribuite in maniera impari. Se la accuratezza non è altro che il rapporto tra risposte corrette sul totale, allora capirete che se una classe compone il 90% del nostro dataset e il nostro modello (erroneamente) classifica ogni esempio nel dataset con quella classe specifica, allora la sua accuratezza sarà del 90%.
Se non siamo attenti potremmo pensare che il nostro modello sia molto performante, quando in realtà è molto lontano dall'esserlo.
La accuratezza è però una metrica sensata da usare se siamo sicuri che il nostro dataset sia bilanciato e che i dati siano di alta qualità.
Precisione e richiamo
Per comprendere al meglio i concetti di precisione e richiamo useremo una matrice di confusione (confusion matrix) per introdurre l'argomento. Si chiama così proprio perché comunica all'analista il grado di errore (e quindi di confusione) del modello.
Per creare una confusion matrix non dobbiamo far altro che elencare le classi effettive presenti nel dataset su righe e e quelle che il modello deve prevedere nelle colonne. Il valore delle celle corrisponde alla risposta del modello sotto quelle condizioni.
Vediamo un esempio usando Sklearn come fonte
Questo è un esempio di una matrice di confusione per un classificatore binario applicato al famoso Iris dataset.
I valori sulla diagonale indicano i punti in cui la predizione del modello di classificazione corrisponde alla classe reale presente nel dataset. Più elementi sono presenti sulla diagonale, più le nostre predizioni sono corrette (attenzione, non ho detto che il modello sia performante! Ricordiamoci il discorso della accuratezza di prima).
Andiamo a rappresentare in maniera astratta la matrice di confusione in modo da comprendere al meglio i valori che riempiono le celle.
Vediamo una serie di etichette: true negatives (veri negativi), true positives (veri positivi), false negatives (falsi negativi) e false positives (falsi positivi). Vediamone uno ad uno.
- Veri negativi (TN): contiene gli esempi che sono stati correttamente classificati come negativi.
- Veri positivi (TP): contiene gli esempi che sono stati correttamente classificati come positivi.
- Falsi negativi (FN): contiene gli esempi che sono stati erroneamente classificati come negativi e che quindi sono in realtà positivi
- Falsi positivi (FP): contiene gli esempi che sono stati erroneamente classificati come positivi e che quindi sono in realtà negativi
Usando una matrice di confusione riusciamo quindi a capire meglio il comportamento del nostro classificatore e come migliorarlo ulteriormente.
Per continuare, vediamo come trarre la formula della accuratezza dalla matrice di confusione
\[ accuratezza = \frac{TN + TP}{TN + TP + FP + FN} \]
Questa non è altro che il numero di risposte corrette diviso il totale.
Definiamo ora la precisione con la formula
\[ precisione = \frac{TP}{TP + FP} \]
Come si interpreta? La precisione non è altro che la accuratezza calcolata solo classi positive. Essa è anche chiamata specificità poiché definisce quanto sensibile è uno strumento quando c'è il segnale da riconoscere. Di fatto, la metrica ci informa quanto spesso siamo corretti quando classifichiamo una classe come positiva.
Facciamo un'altro esempio: abbiamo installato un sistema di allarme in casa con un algoritmo di riconoscimento facciale. Questo è collegato a delle telecamere e ad una centralina che invia una notifica su una app sul nostro cellulare se in casa entra qualcuno che non riconosce come amico o familiare.
Un modello con alta precisione ci allerterà poche volte, ma quelle volte che lo farà possiamo essere abbastanza sicuri che si tratti veramente di un intruso! Quindi, il modello è abile a distinguere correttamente un intruso da un familiare quando in casa c'è effettivamente un intruso.
Il richiamo invece rappresenta l'altro ago della bilancia. Se siamo interessati a riconoscere quanti più classi positive possibili, allora il nostro modello dovrà avere un richiamo alto.
La sua formula è
\[ richiamo = \frac{TP}{TP + FN} \]
In pratica significa che qui dobbiamo tener conto dei falsi negativi invece dei falsi positivi. Il richiamo viene anche chiamato sensibilità perché all'aumentare del richiamo, il nostro modello diventa sempre meno preciso e classifica anche classi negative come positive.
Osserviamo un esempio che include il richiamo: siamo dei radiologi e abbiamo addestrato un modello che usa la computer vision per classificare la presenza di eventuali tumori ai polmoni. In questo caso vogliamo che il nostro modello abbia alto richiamo, in quanto vogliamo essere sicuri che ogni minimo esempio considerato positivo da parte del modello venga sottoposto a ispezione umana. Non vogliamo che un tumore maligno passi inosservato, e accetteremo volentieri dei falsi positivi.
Per riassumere, vediamo questa analogia
Un modello ad alta precisione è conservativo: non riconosce sempre la classe correttamente, ma quando lo fa, possiamo stare molto tranquilli che la sua risposta sia corretta.
Un modello ad alta precisione è liberale: riconosce una classe molto più spesso, ma nel farlo tende a includere anche molto rumore (falsi positivi).
Il lettore attento e curioso avrà dedotto che è impossibile avere un modello con alta precisione e alto richiamo. Infatti, queste due metriche sono complementari: se aumentiamo una, l'altra deve diminuire. Si tratta del precision/recall trade-off.
Il nostro obiettivo da analisti è quello di contestualizzare e capire quale metrica ci offre più valore.
Punteggio F1
A questo punto è chiaro che usare precisione o richiamo come metrica di valutazione è difficile perché possiamo solo usarne una a scapito dell'altra. Il punteggio F1 risolve proprio questo problema.
Infatti, il punteggio F1 combina precisione e richiamo in una sola metrica.
\[ F1 = 2 \times \frac{precisione \times richiamo}{precisione + richiamo} \]
Questa è la media armonica di precisione e richiamo, ed è probabilmente la metrica più usata per valutare modelli di classificazione binaria.
Se il nostro punteggio F1 aumenta, vuol dire che il nostro modello ha aumentato le performance per precisione, richiamo o per entrambi.
Log loss
La log loss è una metrica di valutazione comune, soprattutto su Kaggle. Conosciuta anche come cross-entropia o entropia incrociata nel contesto del deep learning, questa misura la differenza tra le probabilità delle previsioni del modello e le probabilità della realtà osservata. L'obiettivo di questa metrica è di stimare la probabilità che un esempio abbia una classe positiva.
Questa metrica è matematicamente più complessa delle precedenti e non c'è bisogno di andare in profondità per intuire la sua utilità nel valutare un sistema di classificazione binario.
Ecco la formula per completezza
\( n \) sta per il numero di esempi nel dataset, \( y_i \) sta per la realtà osservata e \( y\hat{}_i \) sta per la predizione del modello.
Non continuerò con la spiegazione di questa formula perché andrei fuori traccia. Google è il vostro migliore amico 🙂
ROC-AUC
La metrica ROC-AUC si basa su una rappresentazione grafica della curva ROC (receiving operating characteristic curve). Non proverò a spiegarla a parole mie, perché stavolta Wikipedia fa veramente un ottimo lavoro
le curve ROC […] sono degli schemi grafici per un classificatore binario. Lungo i due assi si possono rappresentare la sensibilità e (1-specificità), rispettivamente rappresentati da True Positive Rate (TPR, frazione di veri positivi) e False Positive Rate (FPR, frazione di falsi positivi). In altre parole, si studiano i rapporti fra allarmi veri (hit rate) e falsi allarmi.
La frase finale in grassetto (applicato da me) è quella che rende intuitiva la descrizione della curva ROC che abbiamo appena letto. Ovviamente vogliamo che il rapporto tra allarmi veri e falsi sia in favore di quelli veri, perché modelli più performanti faranno esattamente questo.
Vediamo come si presenta tale grafico
AUC sta per Area Under the Curve (area al di sotto della curva). Se poniamo l'attenzione sulla linea blu, vediamo che al di sotto di essa c'è di fatto una area più grande rispetto alle linee verde arancione. La linea tratteggiata indica un metrica di ROC-AUC del 50%.
Di conseguenza un buon modello avrà una ROC-AUC grande, mentre un modello scarso si posizionerà vicino alla linea tratteggiata, che non è altro che un modello che risponde in maniera casuale.
La metrica ROC-AUC è anche molto utile per confrontare diversi modelli uno contro l'altro.
Coefficiente di correlazione di Matthews (MCC)
Vediamo qui l'ultima metrica di valutazione per un modello di classificazione binaria che è concepito per valutare correttamente anche modelli addestrati su dataset non bilanciati.
Sembra uno scioglilingua, ma in realtà questa formula si comporta come un coefficiente di correlazione. Essa va quindi da +1 a -1. Un valore che tende a +1 misura la qualità delle predizioni del nostro classificatore anche in contesti con classi sbilanciate nel dataset, poiché indica una correlazione tra valori reali osservati e previsioni fatte dal nostro modello.
Conclusione
Come per le metriche valutazione dei modelli di regressione, Sklearn mette a disposizione parecchi metodi per calcolare rapidamente queste metriche. Qui un link alla documentazione.
Come nota finale, queste sono metriche per la valutazione di un classificatore binario. Per la classificazione multi-classe, ad esempio, basta applicare una di queste metriche ad ogni classe e poi applicare una strategia che vada a generalizzare su tutti gli esempi, come la media (micro/macro averaging). Ma questo è un discorso per un altro articolo :)
Commenti dalla community"
https://www.diariodiunanalista.it/posts/valutazione-delle-prestazioni-di-un-modello-di-regressione/,"La fase di valutazione del modello inizia quando creiamo un set di validazione che consiste in esempi che l'algoritmo di apprendimento non ha visto durante l'addestramento.
Se il nostro modello si comporta bene su questo set allora possiamo dire che generalizza bene ed è di buona qualità.
Il modo più comune per valutare se un modello è valido o meno è calcolare una metrica di performance proprio sul set di validazione o di test.
Questo articolo si concentrerà sulle metriche di performance per i modelli di regressione. Vale la pena specificarlo perché task di classificazione hanno metriche tracciabili completamente diverse.
Metriche di Performance per la Regressione
Parleremo delle seguenti metriche di performance per la regressione:
- errore quadratico medio (Mean Square Error, MSE)
- errore assoluto medio (Mean Absolute Error, MAE)
- errore assoluto mediano (Median Absolute Error, MdAE)
- il tasso di errore delle previsioni quasi corrette (Almost Correct Predictions Error Rate, ACPER)
- l'errore percentuale assoluto medio (Mean Absolute Percentage Error, MAPE)
- lo scarto quadratico medio (Root Mean Square Error, RMSE)
A mio avviso queste sono le metriche più utili per un modello di regressione.
Inizieremo con MSE, che è abbastanza facile da capire.
Errore quadratico medio (Mean Square Error, MSE)
La metrica più comune è la funzione di costo della regressione: l'errore quadratico medio (MSE). È definito come
\[ MSE(f) = \frac{1}{N} \sum_{i=1}^{N}(f(x_{i}) - y_{i})^2 \]
dove f è il modello che prende un vettore di feature x come input e genera una previsione y. i, che va da 1 a N, denota l'indice di un punto nel set di dati.
La somma delle previsioni meno i valori reali su \( N \) indica la media. Elevando al quadrato rimuoviamo il segno negativo e diamo più peso a differenze maggiori.
Un modello di regressione tenta di adattare i dati tracciando una linea che riduce al minimo la distanza dai punti dati reali e dal punto sulla stessa linea.
Più i valori sono vicini alla linea, migliore è il comportamento del modello per quel particolare punto. Pertanto, più basso è l'MSE, meglio è.
La metrica MSE viene solitamente confrontata con un modello di baseline che solitamente è un modello di regressione basato sulla media. Questo modello restituisce sempre la media dei valori dei dati di addestramento.
Se l'MSE del nostro modello di regressione è maggiore dell'MSE della baseline, allora c'è qualcosa che non va, come ad esempio un errore nel dataset o un bug.
L'MSE è influenzato da valori anomali (outliers), ovvero valori di dati che sono anormalmente distanti dalla vera retta di regressione.
Per definizione, l'errore al quadrato per punti così distanti sarà molto alto. In queste situazioni, è meglio applicare il MdAE che sta per Median Absolute Error (che vedremo in basso).
Ecco come implementare MSE in Python
def mean_squared_error(y_true, y_pred):
""""""
Funzione che calcola MSE.
:param y_true: lista di numeri che rappresentano i valori reali
:param y_pred: lista di numeri che rappresentano i valori predetti
:restituisce: MSE
""""""
return np.mean(np.abs(np.array(y_pred) - np.array(y_true))**2, axis=0)
Errore assoluto medio (Mean Absolute Error, MAE)
\[ MAE(f) = \frac{1}{N} \sum_{i=1}^{N}|f(x_{i}) - y_{i}| \]
MAE è simile a MSE in quanto restituisce i valori assoluti dei residui \( f(x) - y \) senza l'elevazione al quadrato.
Non considera la direzione dell'errore, il che significa che non sapremo se gli errori negativi o positivi pesano di più sulla media complessiva.
Detto questo, MAE è più robusto ai valori anomali proprio grazie all'assenza dell'elevazione al quadrato dei valori degli errori di previsioni lontane.
Ecco come implementare MAE in Python
def mean_absolute_error(y_true, y_pred):
""""""
Funzione che calcola MAE.
:param y_true: lista di numeri che rappresentano i valori reali
:param y_pred: lista di numeri che rappresentano i valori predetti
:restituisce: MAE
""""""
return np.mean(np.abs(np.array(y_pred) - np.array(y_true)), axis=0)
Ti piace quello che stai leggendo?
Iscriviti al blog per rimanere sempre aggiornato sui contenuti di settore. No pubblicità. No spam.
Solo contenuto utile per la tua carriera nel ML e DS.
Puoi rimuovere l'iscrizione quando vuoi seguendo il link nella email.
Errore assoluto mediano (Median Absolute Error, MdAE)
La mediana, come sappiamo dalle basi della statistica, non è influenzata da valori anomali come lo è la media. Il MdAE indica la distanza assoluta delle previsioni dalla retta di regressione mediana.
\[ MdAE(f) = median({|f(x_{i})- y_{i}|}) \]
dove \( |f(x_i) — y_i| \) indica l'insieme dei valori di errore assoluti per tutti i punti su cui viene eseguita la valutazione del modello.
Se sono presenti valori anomali e non trattati conviene utilizzare MdAE invece di MSE o MAE, poiché fornisce una migliore rappresentazione dell'errore per i punti che non sono così distanti dalla retta di regressione.
Ecco come implementare MdAE in Python
def median_absolute_error(y_true, y_pred):
""""""
Funzione che calcola MdAE.
:param y_true: lista di numeri che rappresentano i valori reali
:param y_pred: lista di numeri che rappresentano i valori predetti
:restituisce: MdAE
""""""
return np.median(np.abs(np.array(y_pred) - np.array(y_true)), axis=0)
Tasso di errore delle previsioni quasi corrette (Almost Correct Predictions Error Rate, ACPER)
L'ACPER è la percentuale di previsioni che è distante p valori percentuali del valore reale. Si tratta di impostare un intervallo arbitrario e calcolare quanti punti predetti ricadono in tale intervallo.
Ecco un semplice algoritmo per calcolare l'ACPER:
- definisci una soglia percentuale di errore che ritieni accettabile (diciamo 2%)
- Per ogni valore vero del target y, la previsione desiderata dovrebbe essere compresa tra \( y_i - 0.02 \times y_i \) e \(y_i+0.02 \times y_i \)
- utilizzando tutti i punti, calcolare la percentuale di valori predetti che soddisfano la regola di cui sopra. Questo ci darà l'ACPER per il nostro modello.
Ecco come implementare ACPER in Python
def acper(y_true, y_pred):
""""""
Funzione che calcola ACPER.
:param y_true: lista di numeri che rappresentano i valori reali
:param y_pred: lista di numeri che rappresentano i valori predetti
:restituisce: punteggio ACPER
""""""
threshold = 0.02
for yt, yp in zip(y_true, y_pred):
lower_bound = yt - (threshold * yt)
upper_bound = yt + (threshold * yt)
if (yp >= lower_bound) & (yp <= upper_bound):
yield True
else:
yield False
y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0.0, 2.01, 8]
list(acper(y_true, y_pred))
Risultato: [Falso, Falso, Vero, Falso]. Possiamo semplicemente contare il numero di valori True e dividerlo per la lunghezza della lista per ottenere una percentuale.
Errore percentuale assoluto medio (Mean Absolute Percentage Error, MAPE)
\[ MAPE(f) = \frac{1}{N}\sum_{i=1}^{N}\frac{f(x_{i}) - y_{i}}{y_{i}} \times 100 \]
L'errore percentuale medio assoluto (MAPE) misura l'errore in percentuale e può essere calcolato applicando una leggera modifica al MAE e moltiplicandolo per 100 per ottenere un punteggio percentuale.
È una delle metriche più utili e utilizzate per la valutazione del modello di regressione.
Supponiamo che MAPE per il nostro modello sia del 5%: ciò indicherebbe che la differenza media tra il valore previsto e il valore osservato è in media del 5%. Comodo e interpretabile, giusto?
Come MAE, soffre della presenza di valori anomali, quindi dobbiamo assicurarci di trattarli in modo appropriato.
Ecco come implementare MAPE in Python
def mean_absolute_percentage_error(y_true, y_pred):
""""""
Funzione che calcola MAPE.
:param y_true: lista di numeri che rappresentano i valori reali
:param y_pred: lista di numeri che rappresentano i valori predetti
:restituisce: punteggio MAPE
""""""
return np.mean(np.abs(np.array(y_pred) - np.array(y_true)) / np.array(y_true), axis=0)
Scarto quadratico medio (Root Mean Square Error, RMSE)
Lo scarto quadratico medio è una misura dell'errore assoluto in cui gli errori sono al quadrato per evitare che valori positivi e negativi si annullino a vicenda (proprio come MSE). Rappresenta la deviazione standard dei residui.
Il termine residuo si riferisce alla distanza tra il punto previsto e il punto osservato. Essendo la deviazione standard, indica quanto sono distribuiti i residui attorno alla nostra linea di regressione.
Citando un collega su Medium
RMSE can be thought of as some kind of (normalized) distance between the vector of predicted values and the vector of observed values.
RMSE può essere considerato come una sorta di distanza (normalizzata) tra il vettore dei valori previsti e il vettore dei valori osservati.
L'implementazione è molto semplice, poiché si tratta solo di applicare np.sqrt al nostro calcolo MSE visto sopra.
Conclusioni
Hai imparato alcune delle metriche più importanti per valutare le performance di un modello di regressione.
Queste metriche sono alcune delle più importanti per valutare un modello di regressione, ma non rappresentano una lista completa. A volte occorre definire delle metriche di valutazione personalizzate in base al problema che si vuole risolvere.
Concludo proponendoti un libro che introduce ai modelli di regressione e a come costruirli sia con Sklearn che con TensorFlow + Keras.
Si tratta i Hands-On Machine Learning with Scikit-Learn, Keras and TensorFlow di Aurelien Géron. Una pietra miliare per approcciarsi ai metodi più strutturati del machine learning.
Se vuoi invece continuare a leggere materiale introduttivo al machine learning e data science e data analytics in Python, ti linko la pagina dedicata di questo blog che contiene un indice di contenuto nella quale puoi orientarti.
Alla prossima,
Andrea
Commenti dalla community"
https://www.diariodiunanalista.it/posts/vulnerabilita-lavorativa-e-ia/,"Stiamo vivendo una rivoluzione. L'intelligenza artificiale sta (finalmente) facendosi spazio nelle nostre vite, creando tanto stupore ma anche tanta preoccupazione.
Vedere come modelli di apprendimento automatico diventino parte della quotidianità e delle nostre vite lavorative mi riempie di orgoglio - far parte di questo mondo mi fa sentire al centro dell'innovazione e delle opportunità che si stanno creando.
Tuttavia, questa emergenza di innovazione non suscita solo stupore e ammirazione, ma anche ansia e preoccupazione.
Ho notato che tali preoccupazioni originano soprattutto da quelle persone che non lavorano nel settore dell'analisi dati e del machine learning, ma che fanno lavori che modelli come ChatGPT possono ""facilmente"" rimpiazzare.
Alcuni esempi di questi lavori possono essere:
- data entry
- supporto clienti
- creazione di contenuto editoriale
e altri.
Questo è un tweet che ha ricevuto parecchio engagement il mese scorso che riporta secondo ChatGPT stesso, quali sono i lavori a rischio di sostituzione
Tale tweet può risultare sorprendente e preoccupante se si legge il proprio lavoro in quella lista.
In questo articolo limiterò le mie osservazioni proprio agli individui che si sentono minacciati dalla IA.
Leggo di colleghi che scrivono articoli e tweet spiegando che non c'è nulla da temere da parte delle IA come ChatGPT. Secondo me invece c'è bisogno di sensibilizzazione non tanto verso l'intelligenza artificiale, quanto verso il lavoro che facciamo tutti i giorni.
La realtà dei fatti è che la preoccupazione è parzialmente giustificata, soprattutto per un gruppo ristretto di persone che non hanno mai fatto uso di semplici sistemi di automazione.
La parola chiave è automazione, e cercherò di spiegare perché il problema non sia l'IA, ma la mancanza di alternative che imponiamo alla nostra routine di lavoro quotidiana.
È facile preoccuparsi...quando c'è l'IA
La domanda che vorrei porre al lettore preoccupato è questa
Ti sei mai sentito preoccupato che il tuo lavoro potesse essere rimpiazzato prima dell'avvento di ChatGPT?
So che è una domanda provocatoria, ma chiedo al lettore di portare un attimo di pazienza.
ChatGPT ha iniziato un movimento collettivo di presa di consapevolezza della propria vulnerabilità lavorativa.
Ma la realtà dei fatti è che noi (tutti, nessuno escluso) è sempre a qualche livello sostituibile da un sistema di automazione.
ChatGPT ha portato questa cosa a galla, ma in realtà ogni datore di lavoro e imprenditore sa bene di cosa parlo. La consapevolezza ora è di tutti, non solo di chi paga lo stipendio.
Chi possiede una attività lavorativa e paga fornitori e dipendenti ragiona sempre in termini di convenienza e tempo.
Domande come
- quanto mi costa?
- quanto tempo mi occupa?
- quanto posso pagare qualcuno facendo del profitto?
- quanto valore fornisce questa attività?
e molte altre, sono sempre nella mente dell'imprenditore attento e dedito.
L'IA comprende entrambe le dimensioni temporali ed economiche (e molte altre) ed è quindi la soluzione ovvia per problemi altrettanto ovvi.
È dunque facile essere preoccupati ora. ChatGPT ha risvegliato quella consapevolezza dormiente nel lavoratore che passa le sue ore su task automatizzabili, ma che per qualche motivo, non automatizza (o comunque non completamente).
Emerge lo psicologo che è in me: la presa di consapevolezza è comunque una cosa buona per il nostro cervello - rappresenta l'inizio di un percorso di rieducazione e riallineamento verso i nostri veri obiettivi.
La leggendaria ""automazione""
Programmatori e data scientist (soprattutto) pensano continuamente a come automatizzare i task di tutti i giorni.
Mentre i programmatori (so che è un termine molto generico, ma lo uso con intento) creano interi sistemi per automatizzare insiemi di task, i data scientist e esperti del machine learning si specializzano nell'automatizzare dei singoli task, mettendo insieme anch'essi sistemi, mattone su mattone.
Si creano dataset, si addestrano modelli, solo al fine di poter risolvere uno dei problemi cruciali nel grosso sistema messo su dagli sviluppatori.
Come ho scritto in passato, i data scientist sono abilitatori dell'automazione. Il nostro mestiere ci forza a voler automatizzare quello che si è sempre pensato fosse non automatizzabile secondo i paradigmi ""classici"" di sviluppo.
Basti pensare ai transformers - dei modelli estremamente potenti in grado di soddisfare una grossa moltitudine di compiti, come creare sommari da un insieme di testi oppure generare del testo partendo da una frase di input.
Dove voglio arrivare? Che c'è poca conoscenza tecnica, e questa conoscenza è alla base della paura che incute l'IA, soprattutto in Italia.
Vi faccio un esempio:
ChatGPT è molto utile per uno sviluppatore, junior o senior che sia. Questo perché aiuta a trovare errori nel codice, a riorganizzarlo e a fornire spunti utili per completare delle funzioni. Si pensi a Copilot, lo strumento di Microsoft che assiste il programmatore alla scrittore di codice. Io lo uso sempre, ed è una manna dal cielo.
Mai per una volta mi sono sentito minacciato dalla presenza di Copilot
Il motivo è perché sono uno strumento nelle mie mani che aiuta la mia produttività: mentre prima potevo metterci 3 ore a completare il mio task, ora ce ne metto 1. Per me è sempre una vittoria.
Perché questa sicurezza non si estende anche ad altri professionisti che lavorano in campi diversi?
Ecco alcune motivazioni:
- perché non sono sviluppatori, e quindi non c'è una forma mentale orientata all'automazione
- soffrono la pressione sociale: sentire e vedere altri preoccuparsi genera preoccupazione a sua volta
- bias cognitivi, come il confirmation bias, che vanno a far leva su pensieri negativi già presenti riguardo al proprio lavoro
E molte altre.
Se tu, lettore, ti rivedi in uno di questi punti, non temere. A mio avviso la situazione non è così tetra come i social la fanno passare. Tutt'altro.
Il problema non è l'IA, ma il nostro modo di lavorare
Come tutte le grosse novità che entrano nelle vite della maggioranza e che hanno impatto su più aspetti della vita, ci sarà sempre un gruppo di persone resistenti al cambiamento. È normale.
Bisogna avere senso critico e lucidità mentale per giudicare la situazione e la nostra posizione al riguardo.
Ecco il tema centrale di questo articolo
L'IA è un catalizzatore. Il problema è alla base. Se pensiamo di essere potenzialmente sostituibili su un range di compiti, allora lo siamo già da anni. La differenza è che solo ora ce ne rendiamo conto.
Dobbiamo ringraziare la IA che ci ha reso consapevoli di ciò.
Integro me stesso nel discorso, anche se lavoro nel settore: dobbiamo cambiare modo di lavorare.
Dobbiamo abbandonare il lavoro di basso valore e formarci per soddisfare lavori ad alto valore.
Mi spingo a dire ciò perché, ad oggi, il valore che offre l'IA è alto, ma perché automatizza tutta una serie di micro-compiti che fanno risparmiare tempo e denaro.
L'IA però non automatizza la creatività, la generazione di idee o i processi strategici. Questi processi sono completamente umani, e richiedono meccanismi di ragionamento e recupero di informazioni che oggi l'IA non può assolvere.
Se pensate che utilizzare Midjourney sia automazione di creatività, mi trovate in disaccordo.
La mia opinione è che dobbiamo uscire dalla zona di comfort e spingere verso obiettivi più grandi.
- Vogliamo licenziarci e aprire la nostra attività? Facciamolo.
- Vogliamo chiedere un aumento e ottenere responsabilità che pensiamo di non poter affrontare? Facciamolo
- Abbiamo desiderio di cambiare lavoro e di formarci partendo da zero? Ragioniamoci su seriamente.
Sono stato personalmente coinvolto in ragionamenti del genere di recente, e so che non è facile. Ma garantisco che iniziare è la parte più difficile - con abbastanza dedizione e costanza gli obiettivi vengono raggiunti, quasi sempre.
Che sia un obiettivo di formazione, professionale o personale, non dedichiamo il nostro tempo a fare lavori di basso valore - quello lasciamolo all'IA, com'è giusto che sia.
Trattiamo l'IA come uno strumento nelle nostre mani. Usiamola e miglioriamo la nostra vita grazie ad essa.
Una rivoluzione anche creativa e del benessere
Concludo questo flusso di pensiero con una sorta di anticipazione di quello che per me sarà il futuro.
A mano a mano che i modelli andranno a migliorare, così aumenterà il tempo che abbiamo da dedicare a quello che amiamo, alle nostre passioni.
Lavorando di meno sull'operatività saremo più in grado di concentrarci su temi rilevanti al nostro business e alla nostra persona.
Avremo più tempo per noi stessi, per attività di natura creativa e sportiva. A livello professionale invece, concentrarci su attività strategiche e di creazione.
Questo si sta avverando parzialmente già ora, con modelli ""semplici"" come ChatGPT. Dico semplici perché il campo si muove velocemente e già è possibile vedere grossi miglioramenti tra GPT-3.5 e 4, i modelli che alimentano il bot conversazionale. Tutto questo è avvenuto nel giro di pochi mesi, non anni.
Il futuro sarà sempre più automatizzato. Dobbiamo comprenderlo quanto prima. Sta a noi adattarci di conseguenza e intraprendere percorsi professionali e personali che sfruttano questa automazione.
L'innovazione è basata sul machine learning. Nessun campo più di questo influenzerà gli anni futuri.
Vedremo robot domestici con i quali converseremo come se fossero dei compagni, e interfacce che richiederanno sempre meno intervento fisico (NeuraLink, per riferimento). Tutto questo è innegabile - è solo questione di tempo.
Dobbiamo comprendere che il futuro che abbiamo sempre visto nei film è qui, e sta solo iniziando.
Se non lo facciamo, le conseguenza sono nostre. La preoccupazione è giusta a quel punto.
Commenti dalla community"
https://www.diariodiunanalista.it/privacy-policy/,"Diario Di Un Analista utilizza i seguenti dati dell'utente ai fini di migliorare l'esperienza e di fornire notifiche sulle pubblicazioni e altre informazioni pertinenti.
- utilizzo del sito web
- indirizzo email
- cookie
Utilizzo del sito web
Questo blog utilizza Google Analytics e Google Tag Manager per tracciare le informazioni sull'utilizzo dello stesso mediante script di tracciamento installato sul sito web. L'utente è anonimizzato e nessun dato personale viene trasmesso al personale di questo blog.
La piattaforma utilizzata per la pubblicazione di questo blog è Ghost.org, che utilizza dati tecnici dell'utente per migliorarne l'esperienza di utilizzo.
Indirizzo email
L'indirizzo email dell'utente viene utilizzato all'unico scopo di aggiornarlo sulle pubblicazioni di articoli e per notificarlo di eventuali informazioni inerenti ai temi trattati in questo blog. L'utente ha il diritto di inviare una email a andrea@diariodiunanalista.it per essere rimosso dalla mailing list se non vuole più ricevere queste notifiche.
Cookie
Diario Di Un Analista e le terze parti menzionate utilizza i cookie per finalità tecniche.
Leggi la privacy policy completa qui
Privacy Policy
Diario Di Un Analista utilizza i seguenti dati dell'utente ai fini di migliorare l'esperienza e di fornire notifiche sulle pubblicazioni e altre informazioni pertinenti."
https://www.diariodiunanalista.it/signin/,"Accedi
Invia link di accesso
Elaborazione della tua richiesta
Si è verificato un errore durante l’invio dell’email
Non hai ancora un account?
Registrati
Ottimo!
Controlla la tua casella di posta e clicca sul link per completare l’accesso.
Torna alla homepage"
https://www.diariodiunanalista.it/signup/,"Diventa membro della community
Continua
Elaborazione della tua richiesta
Si è verificato un errore durante l’invio dell’email
Hai già un account?
Accedi
Ottimo!
Controlla la tua casella di posta e clicca sul link per confermare la registrazione.
Torna alla homepage"
https://www.diariodiunanalista.it/tag/carriera/,"Cosa è l'IA generativa e perché sta ridefinendo i confini del possibile attraverso tecnologie che permettono alle macchine non solo di apprendere ma anche di creare
Carriera
Una lista di software utili per la gestione di progetti di data science e la raccolta di idee e contenuti
Perché prendere appunti nel modo corretto è aiuta enormemente nell'organizzazione di un progetto di data science o analytics
Le migliori risorse per imparare Python online. Queste sono state selezionate in quanto hanno avuto un grosso impatto sul mio sviluppo da programmatore e a distanza di anni si rivelano essere ancora delle pietre miliari per la mia formazione.
L'importanza di costruire il proprio dataset invece di usare una soluzione già pronta"
https://www.diariodiunanalista.it/tag/data-analytics/,"Una guida introduttiva agli algoritmi di clustering: cosa sono, quali sono, perché sono importanti e come valutarli nel contesto dell'analisi dei dati e la data science
Data Analytics
Impara come usare la PCA in Python e Sklearn per trasformare un dataset multidimensionale in un numero arbitrario di dimensioni e visualizzare i dati ridotti con Matplotlib
Una guida su come approcciare le variabili categoriali presenti all'interno di un dataset ai fini del machine learning e data science
Il campionamento statistico è fondamentale per ottenere informazioni sulla popolazione di interesse in modo efficiente. In questo articolo, esploreremo il concetto di campionamento statistico, le sue tecniche più comuni e l'utilizzo del ricampionamento per stimare la precisione delle stime.
Un articolo che esplora le tecniche per identificare le anomalie (outlier detection) nei dataset. Scopri come utilizzare la visualizzazione dei dati, score z e tecniche di clustering per individuare valori anomali
Una guida alla data visualization e ai principi che la sorreggono. Visualizzare grafici efficaci e fare storytelling per migliorare le abilità di comunicazione visiva e presentazioni.
Recensione del manuale di data visualization Storytelling with Data di Cole Nussbaumer Knaflic, esperta di comunicazione visiva. Una lettura consigliata pienamente a tutti gli interessati di data viz.
C'è davvero differenza tra data science, data engineering e data analytics? L'intero processo end-to-end di raccolta, gestione e analisi del dato viene diviso secondo questi tre termini - propongo la mia interpretazione olistica dell'intero processo.
Una attività di feature engineering può essere molto utile per migliorare le performance di un modello predittivo. Questa però può peggiorare i nostri risultati se non teniamo a mente certi principi da evitare."
https://www.diariodiunanalista.it/tag/flusso-di-coscienza/,"1 post
Pensieri riguardo la preoccupazione che affligge professionisti e non riguardo l'avvento della IA e la paura di essere sostituiti.
Parlo di come l'IA non sia una minaccia ma uno strumento da sfruttare quanto prima per migliorare le nostre vite
Ti sei iscritto con successo a Diario Di Un Analista | Data Science, Machine Learning & Analytics
Ben tornato! Hai effettuato l’accesso con successo.
Ottimo! Ti sei registrato con successo.
Successo! La tua email è stata aggiornata.
Il tuo link è scaduto
Successo! Controlla la tua casella di posta per il link magico per accedere."
https://www.diariodiunanalista.it/tag/kaggle/,"Strategia e modellazione della competizione Kaggle dove si prevede la competenza linguistica di studenti liceali che studiano l'inglese
Kaggle
1 post
Strategia e modellazione della competizione Kaggle dove si prevede la competenza linguistica di studenti liceali che studiano l'inglese"
https://www.diariodiunanalista.it/tag/llm-e-prompt-engineering/,"Cosa è l'IA generativa e perché sta ridefinendo i confini del possibile attraverso tecnologie che permettono alle macchine non solo di apprendere ma anche di creare
LLM e Prompt Engineering
2 post
Il prompt engineering consiste nello sviluppare e ottimizzare i prompt (cioè le domande che vengono poste al modello da parte dell'umano) al fine di ottenere risposte più precise da parte dei modelli linguistici"
https://www.diariodiunanalista.it/tag/machine-learning/,"Cosa è l'IA generativa e perché sta ridefinendo i confini del possibile attraverso tecnologie che permettono alle macchine non solo di apprendere ma anche di creare
Machine Learning
Una guida introduttiva ai concetti del machine learning e data science per principianti. Orientati con dei consigli diretti proprio a te che sei novizio e che vuoi diventare un esperto di machine learning
Una guida introduttiva agli algoritmi di clustering: cosa sono, quali sono, perché sono importanti e come valutarli nel contesto dell'analisi dei dati e la data science
Impara come usare la PCA in Python e Sklearn per trasformare un dataset multidimensionale in un numero arbitrario di dimensioni e visualizzare i dati ridotti con Matplotlib
Una guida a servire un modello di machine learning via API con FastAPI, Pydantic e Sklearn
L'apprendimento non supervisionato è una branca fondamentale dell'analisi dei dati che si concentra sulla scoperta di strutture nascoste nei dati senza la presenza di etichette di output.
Il data leakage rappresenta, insieme all'over/underfitting, la causa principale di fallimento di progetti di machine learning che vanno in produzione. Scopri come evitarlo in questo articolo.
Deep Learning e MNIST: Come utilizzare una rete neurale convolutiva per il riconoscimento di immagini
Come approcciare al riconoscimento delle immagini e delle cifre MNIST con una rete neurale convolutiva usando Keras e TensorFlow
Fai scelte intelligenti per la tua strategia aziendale: quando utilizzare il machine learning e quando optare per soluzioni più semplici. Esplora i vantaggi e le sfide dell'adozione dei modelli di apprendimento automatico per prendere decisioni informate"
https://www.diariodiunanalista.it/tag/progetti/,"Deep Learning e MNIST: Come utilizzare una rete neurale convolutiva per il riconoscimento di immagini
Come approcciare al riconoscimento delle immagini e delle cifre MNIST con una rete neurale convolutiva usando Keras e TensorFlow
Come approcciare al riconoscimento delle immagini e delle cifre MNIST con una rete neurale convolutiva usando Keras e TensorFlow
Strategia e modellazione della competizione Kaggle dove si prevede la competenza linguistica di studenti liceali che studiano l'inglese
PyTorch con Python: introduzione da addestramento a predizione. PyTorch è uno dei framework di deep learning più famosi e imparare questo strumento diventa fondamentale se si vuole costruire una carriera nel campo della IA applicata.
Creazione di un corpus di dati testuali in Python e taggare i testi al suo interno attraverso logica fuzzy con la libreria The Fuzz. Progetto basato sull'automazione di un task tipicamente molto lungo e noioso.
Un tentativo di identificare pattern nel mercato azionario attraverso un algoritmo custom di raggruppamento. Parte 1 di N.
Classificare immagini di cani e gatti utilizzando una rete neurale convoluzionale in Tensorflow
Un paradigma facile ed efficiente per creare un corpus da articoli di blog online
Come installare, attivare e usare un ambiente virtuale nel data science
Come ho usato conoscenze di base sulla probabilità per creare un software di generazione NFT"
https://www.diariodiunanalista.it/tag/python/,"Una guida introduttiva agli algoritmi di clustering: cosa sono, quali sono, perché sono importanti e come valutarli nel contesto dell'analisi dei dati e la data science
Python
Impara come usare la PCA in Python e Sklearn per trasformare un dataset multidimensionale in un numero arbitrario di dimensioni e visualizzare i dati ridotti con Matplotlib
Una guida a servire un modello di machine learning via API con FastAPI, Pydantic e Sklearn
Scopri come creare una classe chiamata Benchmark in Python per confrontare e valutare le prestazioni dei modelli di apprendimento automatico utilizzando la validazione incrociata e la visualizzazione dei risultati
Pydantic è una libreria Python che ci consente di strutturare e convalidare i dati in modo efficiente. Applicazioni in Python e nel contesto del Machine Learning
Le migliori risorse per imparare Python online. Queste sono state selezionate in quanto hanno avuto un grosso impatto sul mio sviluppo da programmatore e a distanza di anni si rivelano essere ancora delle pietre miliari per la mia formazione.
Come installare, attivare e usare un ambiente virtuale nel data science
Confronto di 7 algoritmi in task di estrazione di parole chiave su un corpus di 2000 documenti"
https://www.diariodiunanalista.it/tag/recensioni/,"Recensione del manuale di data visualization Storytelling with Data di Cole Nussbaumer Knaflic, esperta di comunicazione visiva. Una lettura consigliata pienamente a tutti gli interessati di data viz.
Recensioni
1 post
Recensione del manuale di data visualization Storytelling with Data di Cole Nussbaumer Knaflic, esperta di comunicazione visiva. Una lettura consigliata pienamente a tutti gli interessati di data viz."
https://www.diariodiunanalista.it/tag/statistica/,"Il campionamento statistico è fondamentale per ottenere informazioni sulla popolazione di interesse in modo efficiente. In questo articolo, esploreremo il concetto di campionamento statistico, le sue tecniche più comuni e l'utilizzo del ricampionamento per stimare la precisione delle stime.
Statistica
1 post"
